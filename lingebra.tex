\documentclass[a5paper,12pt]{amsbook}

\usepackage[czech]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{color}
\usepackage{svg}
\usepackage{amsmath}
\usepackage{blkarray}
\usepackage{bm}
\usepackage{upgreek}

\theoremstyle{definition}
\newtheorem{definition}{Definice}[chapter]
\newtheorem{example}{Příklad}[chapter]
\newtheorem{theorem}{Věta}[chapter]
\newtheorem{lemma}[theorem]{Lemma}

% Book styles
\newcommand{\myscalar}[1]{#1}
\newcommand{\myvec}[1]{\bm{#1}}
\newcommand{\mycoord}[1]{\overrightarrow{\mathbf{#1}}}
\newcommand{\mymatrix}[1]{\mathbf{#1}}
\newcommand{\myspace}[1]{\mathbb{#1}}
\newcommand{\mymap}[1]{#1}
\newcommand{\mydual}[1]{\myspace{#1^{*}}}
\newcommand{\mydouble}[1]{\myspace{#1^{**}}}
\newcommand{\mycocovec}[1]{\mathbf{\widehat{#1}}}

\begin{document}

\title{Lineární algebra pro debily}
\author{Ondřej Stárek}
\maketitle

\tableofcontents

\chapter{Úvod}

\noindent Tento text se snaží čtenáře uvést do problematiky některé pokročilejší lineární algebry.
Je vedena pocitem autora, že pokud překročíte hranici prvního kurzu algebry (za vektorové
prostory), učitelé zapomněli učit a utápí se v nadšené záplavě písmenek a symbolů, namísto
vysvětlování souvislostí.

Text si neklade žádné nároky na matematickou přesnost. Koneckonců nenapsal ho matematik,
ale programátor, který se samostudiem pokoušel naučit něco nového. Spíše se jedná o poznámky
a popsané úvahy, které musel udělat, když se prokousával spoustou matematických textů
a pokoušel se je pochopit. Vlastně tímto textem danou látku vysvětluje především sám sobě.
Přesto si autor fandí, že pro jiné samouky by text mohl být přínosem a ulehčí jim cestu
k pochopení.

Text je určený samoukům, kteří už v současné době znají základy lineární algebry: maticové
operace, tělesa a vektorové prostory.

\chapter{Lineární zobrazení}

\section{Definice}

\noindent Lineární zobrazení je všeobecně známý pojem, obvykle probíraný již v základním kurzu
algebry. Nicméně i u něho autorovi zůstala řada nejasností. Proto se mu bude věnovat.

Co vlastně takové lineární zobrazení je? Není na tom nic složitého: jedná se
o zobrazení/transformaci/převod vektorů z jednoho vektorového prostoru do jiného. Zobrazení ovšem
není libovolné - musí zachovávat strukturu vektorového prostoru. To znamená, že pokud nějaký vektor
ve zdrojovém prostoru vynásobím skalárem, obrazem jeho násobku bude stejný násobek jeho obrazu. Pokud
sečtu dva vektory, obrazem součtu bude součet jejich obrazů. 

Pokud myšlenka stále není jasná, zkusme to na obrázku. Zde je ukázka transformace, která prodlužuje
měřítko na vodorovné ose. Na levém obrázku je původní součet dvou vektorů. Vpravo jsou všechny vektory
přetransformované. Je vidět, že vůbec nezáleží, zda vektory nejdříve sečteme, a pak transformujeme,
anebo nejdříve transformujeme, a pak sečteme.

\begin{center}
\includesvg[width=200pt]{lintrans1}
\end{center}

\noindent Požadovat po lineárním zobrazení tyto podmínky je poměrně přirozené. Díky nim cokoliv, co můžeme
prohlásit o vektorech ve zdrojovém prostoru, platí zároveň i na jejich obrazech.

Nyní víme, co bychom od lineárního zobrazení očekávali, zkusme ho tedy přesněji nadefinovat.

\begin{definition}
Mějme vektorové prostory $\myspace{V}$ a $\myspace{W}$ oba nad tělesem $\myspace{T}$ a zobrazení
$\mymap{L}: \myspace{V}\rightarrow\myspace{W}$. Zobrazení $\mymap{L}$ je linerání zobrazení právě tehdy
pokud platí:

\begin{enumerate}
  \item $\forall\myvec{x}\in\myspace{V},\forall\myscalar{\alpha}\in\myspace{T}\;
    \mymap{L}(\myscalar{\alpha}\myvec{x})=\myscalar{\alpha}L(\myvec{x})$
  \item $\forall\myvec{x},\myvec{y}\in\myspace{V}\;\mymap{L}(\myvec{x}+\myvec{y})
    =\mymap{L}(\myvec{x})+\mymap{L}(\myvec{y})$
\end{enumerate}

\end{definition}

\noindent Definice je v principu jednoduchá, ale protože ďábel tkví v detailech, probereme ji podrobněji:

\begin{itemize}
  \item povšimněte si, že oba vektorové prostory jsou nad stejným tělesem. To je nutnost, jinak bychom
    nebyli schopní obraz vektoru násobit stejným skalárem. Linerní zobrazení tedy jsou omezená pouze
    na prostory nad stejným tělesem.
  \item V zápisu jsou použité stejné značky pro sčítání vektorů a pro násobení skalárem. Ale to neznamená,
    že se vždy jedná o stejné operace. V některých případech se jedná o operace prostoru $\myspace{V}$
    a v jiném o operace prostoru $\myspace{W}$. Nechávám na čtenáři, ať si odvodí, kdy se jedná o kterou
    (nehledejte v tom žádný chyták).
  \item Obě podmínky jsou poměrně přímočaré. První říká, že lineární zobrazení zachovává operaci násobení
    skalárem, druhá, že zachovává operaci sčítání vektorů.
\end{itemize}

\noindent Přímo z definice vyplývá jedna zajímavá vlastnost: obraz nulového prvku musí být opět
nulový prvek:
\begin{align*}
\myvec{v} + \myvec{0} &= \myvec{v} \\
\mymap{L}(\myvec{v} + \myvec{0}) &= \mymap{L}(\myvec{v}) \\
\mymap{L}(\myvec{v}) + \mymap{L}(\myvec{0}) &= \mymap{L}(\myvec{v}) \\
\mymap{L}(\myvec{v}) - \mymap{L}(\myvec{v}) + \mymap{L}(\myvec{0}) 
    &= \mymap{L}(\myvec{v}) - \mymap{L}(\myvec{v}) \\
\mymap{L}(\myvec{0}) &= \myvec{0}
\end{align*}
(První řádka je z definice vektorového prostoru, další dvě řádky jsou aplikace definice
lineárního zobrazení, čtvrtá přičítá opačný vektor k obrazu vektoru $\myvec{v}$.) Důsledkem
této vlastnosti je, že posunutí (přičtení nenulového vektoru) \textbf{není} lineární zobrazení
(nulu převede na neulový vektor). To je v praxi poměrně velké omezení a matematici ho řeší
pomocí tzv. affinních zobrazení a prostorů, které si podrobněji probereme v kapitole XXX.

\begin{example}\textbf{Identita} - jedno z nejjednodušších zobrazení je takové, které převádí vektor
na ten samý:
\begin{equation*}
\mymap{L}:\,\myspace{V}\rightarrow\myspace{V}:\;\mymap{L}(\myvec{x})=\myvec{x}
\end{equation*} 
Je triviální si dokázat, že obě podmínky definice jsou zachované. Aby ne, když původní vektor
a jeho obraz jsou stejné.

Identita je zároveň ukázkou typické situace. Ačkoliv definice lineárního zobrazení pracuje se dvěma
prostory, typické použití je zobrazení do stejného prostoru, tedy kdy oba prostory jsou stejné.

\end{example}

\begin{example}\label{example:rotate}\textbf{Rotace} vektoru o nějaký úhel je dalším běžným příkladem
lineárního zobrazení:
\begin{center}
\includesvg[width=200pt]{lintrans2}
\end{center}
Rotace nemění délku vektorů, pouze ho pootočí. První podmínka tedy splněna je - nezávisí zda
vektor prodloužíme před nebo až po transformaci. Druhá podmínka je graficky také splněna - oba zdrojové
vektory i jejich součet se otočí stejně. Formální důkaz si čtenář určitě dokáže udělat sám.

\end{example}

\begin{example}\label{example:polynoms}\textbf{Substituce v polynomu}. Zkusme nyní ne úplně tradiční
vektorový prostor polynomů do řádu 3. Vektory prostoru tedy jsou funkce typu
\begin{equation*}
p(x)=ax^{3}+bx^{2}+cx+d
\end{equation*}
Operace násobení skalárem a sčítání polynomů jsou definované tak, jak jsme u polynomů zvyklí.
Nyní proveďme substituci $x=\frac{t}{2}$
\begin{equation*}
p(\frac{t}{2})=a\left(\frac{t}{2}\right)^{3}+b\left(\frac{t}{2}\right)^{2}+c\left(\frac{t}{2}\right)+d 
  = \frac{a}{8}t^{3}+\frac{b}{4}t^{2}+\frac{c}{2}t+d
\end{equation*} 
\noindent Je vidět, že výsledek substituce je opět polynom řádu 3. Tedy tato substituce funguje
jako zobrazení z našeho prostoru do našeho prostoru. Jedná se o lineární zobrazení?
\begin{enumerate}
  \item Obrazem vektoru $\alpha p(x)$ je
    \begin{equation*}
      \alpha\frac{a}{8}t^{3}+\alpha\frac{b}{4}t^{2}+\alpha\frac{c}{2}t+\alpha{}d 
      = \alpha p(\frac{t}{2})
    \end{equation*}
  \item Obrazem vektoru $p_1(x) + p_2(x)$ je
    \begin{equation*}
      \frac{a_1+a_2}{8}t^{3}+\frac{b_1+b_2}{4}t^{2}+\frac{c_1+c_2}{2}t+d_1+d_2
      = p_1(\frac{t}{2})+p_2(\frac{t}{2})
    \end{equation*}
\end{enumerate}
Obě podmínky definice lineárního zobrazení jsou zřejmě splněné, takže substituce
je lineárním zobrazením.

K čemu nám něco takového je? Například tato substituce je základem velmi elegantního algoritmu
pro kreslení křivek, který si parametrickou rovnici $p(t)=0$ pomocí této substituce rozkrájí
na drobné úseky tak, aby na každém z nich byl rozdíl mezi $p(0)$ a $p(1)$ právě jeden pixel
zobrazovacího zařízení.

\end{example}

\section{Souřadnice a báze}

\noindent Ze základního kurzu lineární algebry víme, že každý vektor nějakého vektorového prostoru lze
vyjádřit jako lineární kombinaci bázových vektorů (koeficienty lineární kombinace se nazývají
\textit{souřadnice}). Ačkoliv báze mohou být různé, počet bázových vektorů je vždy stejný
a definuje \textit{dimenzi vektorového prostoru}. Omezme se nyní na prostory s konečnou dimenzí
a zkusme zapřemýšlet o vyjádření lineárního zobrazení v souřadnicích.

Vezměme si tedy zdrojový vektorový prostor $\myspace{V}$ dimenze $n$ a jeho bázi 
$\{\myvec{b_{v_1}},\myvec{b_{v_2}},\ldots,\myvec{b_{v_n}}\}$. Stejně tak cílový
vektorový prostor $\myspace{W}$ dimenze $m$ a jeho bázi $\{\myvec{b_{w_1}}, \myvec{b_{w_2}},
\ldots,\myvec{b_{w_m}}\}$. A mějme lineární zobrazení $\mymap{L}: \myspace{V}\rightarrow\myspace{W}$
mezi nimi. Víme, že každý vektor $\myvec{v}\in\myspace{V}$ lze vyjádřit jako lineární kombinaci
\begin{equation*}
\myvec{v}=\myscalar{v_{1}}\myvec{b_{v_1}}+\myscalar{v_{2}}\myvec{b_{v_2}}+\cdots+\myscalar{v_{n}}
  \myvec{b_{v_n}}
\end{equation*} 
Pokud tento vektor použijeme jako argument lineárního zobrazení, tak za použití obou podmínek
z definice:
\begin{equation*}
\begin{split}
\myvec{w}=L(\myvec{v})&=\mymap{L}(\myscalar{v_{1}}\myvec{b_{v_1}}+\myscalar{v_{2}}\myvec{b_{v_2}}
    +\cdots+\myscalar{v_{n}}\myvec{b_{v_n}})\\
  &=\myscalar{v_{1}}\mymap{L}(\myvec{b_{v_1}})+\myscalar{v_{2}}\mymap{L}(\myvec{b_{v_2}})+\cdots
    +\myscalar{v_{n}}\mymap{L}(\myvec{b_{v_n}})
\end{split}
\end{equation*} 
Tedy obraz vektoru $\myvec{v}$ je lineární kombinace obrazů bázových vektorů.
Pokud i obrazy bázových vektorů vyjádříme v souřadnicích prostoru $\myspace{W}$, lze lineární
zobrazení zapsat pomocí maticové aritmetiky (tzn. obrazy vektorů báze v předchozí rovnici
vyjádříme jako souřadnice v prostoru $\myspace{W}$ a sčítáním po složkách získáme souřadnice
vektoru $\myvec{w}$):
\begin{equation*}
\left(\begin{array}{c}
w_{1}\\
w_{2}\\
\vdots\\
w_{m}
\end{array}\right)=\left(\begin{array}{cccc}
\mymap{L}(\myvec{b_{v_1}})_1 & \mymap{L}(\myvec{b_{v_2}})_1 & \cdots & \mymap{L}(\myvec{b_{v_n}})_1\\
\mymap{L}(\myvec{b_{v_1}})_2 & \mymap{L}(\myvec{b_{v_2}})_2 & \cdots & \mymap{L}(\myvec{b_{v_n}})_2\\
\vdots & \vdots & \ddots & \vdots\\
\mymap{L}(\myvec{b_{v_1}})_m & \mymap{L}(\myvec{b_{v_2}})_m & \cdots & \mymap{L}(\myvec{b_{v_n}})_m
\end{array}\right)\left(\begin{array}{c}
v_{1}\\
v_{2}\\
\vdots\\
v_{n}
\end{array}\right)
\end{equation*}
nebo zjednodušeně:
\begin{equation*}
\mycoord{w}=\mymatrix{L}\mycoord{v}
\end{equation*}
kde sloupce matice $\mymatrix{L}$ jsou souřadnice obrazů zdrojové báze a $\mycoord{v}$
a $\mycoord{w}$ jsou souřadnice vektorů $\myvec{v}$ a $\myvec{w}$.

\begin{example}Pokračujme v příkladu \ref{example:rotate}. Pokud vektor pootočíme o úhel $\alpha$,
situace bude vypadat takto:
\begin{center}
\includesvg[width=200pt]{rotate}
\end{center}
Souřadnice původního vektoru jsou
\begin{align*} 
x &= d\cos\beta\\ 
y &= d\sin\beta
\end{align*}
kde $d$ je délka vektoru. Souřadnice transformovaného vektoru jsou
\begin{align*} 
x' &= d\cos(\beta+\alpha) = d\cos\beta\cos\alpha - d\sin\beta\sin\alpha\\ 
y' &= d\sin(\beta+\alpha) = d\sin\beta\cos\alpha + d\cos\beta\sin\alpha
\end{align*}
Když dosadíme za $d$ z původních souřadnic dostáváme
\begin{align*} 
x' &= x\cos\alpha - y\sin\alpha\\ 
y' &= x\sin\alpha + y\cos\alpha
\end{align*}
V maticovém tvaru rotaci vyjádříme takto:
\begin{equation*}
\left(\begin{array}{cc}
\cos\alpha & -\sin\alpha\\
\sin\alpha & \cos\alpha\\
\end{array}\right)\left(\begin{array}{c}
x\\
y
\end{array}\right)=\left(\begin{array}{c}
x'\\
y'
\end{array}\right)
\end{equation*}
Transformace v této podobě je používaná například v počítačové grafice. Grafik
vytvoří model, aniž by znal jeho umístění ve scéně. Ten, kdo scénu kompletuje,
model někam umístí a otočí si ho podle potřeby.

Otočení souřadnicového systému je samozřejmě také běžná činnost ve fyzice.

\end{example}

\begin{example}Vraťme se k příkladu \ref{example:polynoms} a vezměme si jednoduchou bázi prostoru
$\{x^3, x^2, x, 1\}$, polynom $p(x)$ vyjádřený jako souřadnice je $(a, b, c, d)^T$ a matice zobrazení
vypadá takto:
\begin{equation*}
\left(\begin{array}{cccc}
\frac{1}{8} & 0 & 0 & 0\\
0 & \frac{1}{4} & 0 & 0\\
0 & 0 & \frac{1}{2} & 0\\
0 & 0 & 0 & 1
\end{array}\right)\left(\begin{array}{c}
a\\
b\\
c\\
d
\end{array}\right)=\left(\begin{array}{c}
\frac{1}{8}a\\
\frac{1}{4}b\\
\frac{1}{2}c\\
d
\end{array}\right)
\end{equation*}
Ptáte se, k čemu nám to je? Už jsem zmínil, že tato substituce je základ pro jeden
algoritmus. Jenže ouha, počítače neumí jednoduše kouknout na nějaký vzoreček a do
něho substituovat. Co ale umí skvěle, je násobit matice. Tím, že jsme polynom vyjádřili
jako souřadnice a substituci jako matici, jsme získali snadný způsob, jak algoritmus
naimplementovat.
\end{example}

Nyní víme, jak lineární zobrazení vyjádřit jako matici. Protože každý vektor lze vyjádřit
jako lineární kombinaci bázových vektorů, jsme matici schopni vytvořit vždy (skládá se ze
souřadnic obrazů zdrojové báze). Tzn. \textbf{pro každé lineární zobrazení} (stále uvažujeme
prostory s konečnou dimenzí) \textbf{existuje jeho matice}.

V knížkách algebry se už obvykle příliš nezdůrazňuje, že předchozí platí i \textbf{obráceně}, tedy 
že \textbf{každá dvourozměrná matice určuje nějaké lineární zobrazení.}

Každý vektor lze jednoznačně vyjádřit jako souřadnice. I opačně každé souřadnice odpovídají nějakému
vektoru (obsahují koeficienty lineární kombinace báze a z definice vektorového prostoru je každá
lineární kombinace vektorů opět vektor). Tudíž, pokud matici $\mymatrix{A}$ vynásobíme souřadnicemi
libovolného vektoru, získáme opět souřadnice nějakého vektoru. A protože z definice maticových
operací platí
\begin{enumerate}
  \item$
      \mymatrix{A}\left(\myscalar{\alpha}\mycoord{x}\right)=
          \myscalar{\alpha}\left(\mymatrix{A}\mycoord{x}\right)$,
  \item$
      \mymatrix{A}\left(\mycoord{x_1}+\mycoord{x_2}\right)=
          \mymatrix{A}\mycoord{x_1}+\mymatrix{A}\mycoord{x_2}$,
\end{enumerate}
jsou splněné obě podmínky definice lineárního zobrazení.

Povšiměte si, že v předchozím vůbec neříkám, o jaké konkrétní prostory se jedná. Matice tedy
určuje obecně nekonečně mnoho lineárních zobrazení mezi libovolnými vektorovými prostory dimenzí
odpovídajících rozměrům matice (samozřejmě matice i prostory musí být nad stejným tělesem).

I když se jedná o poměrně triviální fakt, jde o podstatnou myšlenkovou úvahu.
Autor měl například dlouho problém pochopit termín \textit{podobnost matic}. Matice je jenom
pole čísel, proto podobnost nedává žádný smysl, obzvláště pokud podobné matice mají zcela
rozdílná čísla. Jejich podobnost spočívá v tom, že odpovídají stejnému lineárnímu zobrazení
v různých bázích (transformace bází viz. kapitola XXX). I další pojmy algebry, například vlastní
čísla matice, dávají smysl až s tímto pozorováním. Matematici často mezi oběma pojmy tiše přechází,
což před pochopením této souvislosti samoukům, jako jsem já, způsobuje bolení hlavy.

\section{Transformace bází}

\noindent Zkusme nyní zapřemýšlet, jak je možné transformovat souřadnicové systémy.  K čemu by nám
něco takového bylo? Transformace souřadnicových systémů je důležitá pro fyziku. Platnost
fyzikálních zákonů musí být nezávislá na souřadnicích. Jednoduše řečeno Newtonovo jablko bude
pořád padat zcela stejně, bez ohledu na to, jak si zvolíme osy. Pro výpočty i měření si ovšem
nějaké zvolit musíme a musíme také být schopní číselné výsledky mezi souřadnicovými systémy
převádět.

Souřadnice vektorů jsou určené bází, tedy otázku můžeme přeformulovat na transformaci bází.
Potřebujeme tedy zjistit, jaké podmínky musí lineární zobrazení splňovat, aby transformací
báze zdrojového vektorového prostoru vznikla opět báze cílového prostoru. Je zřejmé, že ne každé
lineární zobrazení toto splňuje. Například zobrazení, které každý vektor transformuje na nulový
vektor, nedokáže vytvořit bázi, protože množina nulových vektorů není lineárně nezávislá.

Mějme tedy opět lineární zobrazení $\mymap{L}: \myspace{V}\rightarrow\myspace{W}$, kde $\myspace{V}$
a $\myspace{W}$ jsou vektorové prostory konečných dimenzí $n > 0$ a $m > 0$, a bázi prostoru $\myspace{V}$
$\{\myvec{v_1}, \myvec{v_2}, \ldots, \myvec{v_n}\}$. Jaké podmníky musí lineární zobrazení
splnit, aby množina obrazů bází $\{\mymap{L}(\myvec{v_1}), \mymap{L}(\myvec{v_2}), \ldots, 
\mymap{L}(\myvec{v_n})\}$ byla bází prostoru $\myspace{W}$? Z definice báze víme, že
\begin{enumerate}
  \item musí generovat celý prostor $\myspace{W}$
  \item a musí být lineárně nezávislá.
\end{enumerate}
Je zřejmé, že dimenze obou prostorů musí být stejné. Pokud je $n < m$, pak existuje vektor, který
obrazy bází nedokáží vygenerovat. Naopak pokud $n > m$, pak množina obrazů báze není lineárně
nezávislá.

První podmínku lze splnit poměrně snadno. Každý vektor prostoru $\myspace{W}$, který je
generovaný obrazy báze, musí být obrazem nějakého vektoru prostoru $\myspace{V}$:
\begin{equation*}
\begin{split}
\myvec{w}
  &= \myscalar{\alpha{}_1}\mymap{L}(\myvec{v_1}) + \myscalar{\alpha{}_2}\mymap{L}(\myvec{v_2})
      + \cdots + \myscalar{\alpha{}_n}\mymap{L}(\myvec{v_n}) \\
  &= \mymap{L}(\myscalar{\alpha{}_1}\myvec{v_1} + \myscalar{\alpha{}_2}\myvec{v_2} + \cdots 
      + \myscalar{\alpha{}_n}\myvec{v_n}) = \mymap{L}(\myvec{v})
\end{split}
\end{equation*}
Pokud tedy obrazy báze mají generovat celý prostor $\myspace{W}$, musí být zobrazení \textbf{na}
(stručné označení, že každý prvek prostoru $\myspace{W}$ je obrazem nějakého prvku prostoru
$\myspace{V}$). Úvaha platí i obráceně: pokud je zobrazení \textbf{na}, lze každý vektor
prostoru $\myspace{W}$ vyjádřit jako lineární kombinaci obrazů báze - každý vektor má
svůj vzor a předchozí rovnici lze aplikovat pozpátku.

Druhá podmínka je složitější. Pokud je množina obrazů báze lineárně závislá, pak existuje
nenulová lineární kombinace
\begin{equation*}
\myscalar{\alpha{}_1}\mymap{L}(\myvec{v_1}) + \myscalar{\alpha{}_2}\mymap{L}(\myvec{v_2}) 
  + \cdots + \myscalar{\alpha{}_n}\mymap{L}(\myvec{v_n}) = \myvec{0}
\end{equation*}
 Z podmínky (1) víme, že množina generuje celý prostor $\myspace{W}$, proto je
možné každý vektor $\myvec{w}\neq\myvec{0}\in\myspace{W}$ vyjádřit jako nenulovou lineární kombinaci
\begin{equation*}
\begin{split}
\myvec{w} &= \myscalar{\beta{}_1}\mymap{L}(\myvec{v_1}) + \myscalar{\beta{}_2}\mymap{L}(\myvec{v_2}) 
  + \cdots + \myscalar{\beta{}_n}\mymap{L}(\myvec{v_n}) \\
&= \mymap{L}(\myscalar{\beta{}_1}\myvec{v_1} + \myscalar{\beta{}_2}\myvec{v_2} 
  + \cdots + \myscalar{\beta{}_n}\myvec{v_n})
\end{split}
\end{equation*}
K tomuto vyjádření vektoru $\myvec{w}$ je možné libovolně přičítat první kombinaci obrazů báze
(přičítání nulového vektoru nic nezmění):
\begin{equation*}
\begin{split}
\myvec{w} + \myvec{0} &= \myscalar{\beta{}_1}\mymap{L}(\myvec{v_1}) 
  + \myscalar{\beta{}_2}\mymap{L}(\myvec{v_2}) + \cdots + \myscalar{\beta{}_n}\mymap{L}(\myvec{v_n}) \\
  &+ \myscalar{\alpha{}_1}\mymap{L}(\myvec{v_1}) + \myscalar{\alpha{}_2}\mymap{L}(\myvec{v_2}) 
  + \cdots + \myscalar{\alpha{}_n}\mymap{L}(\myvec{v_n}) \\
&= \mymap{L}((\myscalar{\beta{}_1} + \myscalar{\alpha{}_1})\myvec{v_1} 
  + (\myscalar{\beta{}_2} + \myscalar{\alpha{}_2})\myvec{v_2} 
  + \cdots + (\myscalar{\beta{}_n} + \myscalar{\alpha{}_n})\myvec{v_n})
\end{split}
\end{equation*}
Protože obě kombinace jsou nenulové, je zřejmé, že vektor $\myvec{w}$ je obrazem dvou různých
vektorů z prostoru $\myspace{V}$ (jejich souřadnice jsou rozdílné). Znamená to tedy, že
linerání zobrazení \textbf{není prosté} (stručné označení, že jeden obraz může mít více vzorů).
Tedy, pokud zobrazení \textbf{je prosté}, pak je množina obrazů báze lineárně nezávislá\footnote{
  Pokud jste se v poslední větě ztratili, jedná se o tzv. \textit{nepřímý důkaz}, kdy se
  implikace $A\Rightarrow B$ dokazuje jako $\lnot B\Rightarrow\lnot A$. Já jsem dokázal, že pokud
  množina obrazů je lineárně závislá, pak zobrazení není prosté. Tedy obráceně pokud zobrazení
  je prosté, množina je lineárně nezávislá.
}.

Získali jsme tedy dvě nutné a zároveň postačující podmínky. Lineární zobrazení, které 
je \textbf{prosté} a \textbf{na} transformuje bázi na jinou bázi a tedy slouží jako transformace
souřadnicového systému.

Otázka zní, zda je možné tvrdit i opak: každé lineární zobrazení, které transformuje bázi,
je \textbf{prosté} a \textbf{na}. I toto platí. Důkaz první podmínky jsme již udělali (vztah
mezi generováním celého prostoru a \textbf{na} jsme ukázali v obou směrech). Důkaz
druhé podmínky jde udělat podobným způsobem. Zvídavý čtenář ho jistě nalezne v podstatně
elegantnějších podobách v učebnicích algebry.

Nyní tedy víme, že lineární zobrazení \textbf{prosté} a \textbf{na} funguje jako transformace
báze. Takové zobrazení se nazývá \textit{bijekce} a má jednu fajn vlastnost: existuje pro něj
inverzní zobrazení. Je opět poměrně jednoduché si ukázat, že i inverzní zobrazení je lineární:
\begin{enumerate}
  \item $\mymap{L}^{-1}(\myscalar{\alpha}\mymap{L}(\myvec{v})) 
      = \mymap{L^{-1}}(\mymap{L}(\myscalar{\alpha}\myvec{v})) = \myscalar{\alpha}\myvec{v}$
  \item $\mymap{L}^{-1}(\mymap{L}(\myvec{v_1}) + \mymap{L}(\myvec{v_2}))
      = \mymap{L}^{-1}(\mymap{L}(\myvec{v_1} + \myvec{v_2}))
      = \myvec{v_1} + \myvec{v_2}$.
\end{enumerate}
Důsledkem je tedy fakt, že pokud přetransformuji souřadnicový systém, vždy jsem schopen
se vrátit zpět k původním souřadnicím.

Doteď jsme transformovali bázi, ale nepotřebovali jsme si žádnou zvolit. Snesme se nyní
z abstraktních výšin do praktického užití a zauvažujme o transformaci v souřadnicích.
Víme, že každé lineární zobrazení lze vyjádřit jako matici. Abychom tak mohli učinit,
obecně nám v zápisu začnou figurovat 4 báze:
\begin{enumerate}
  \item báze určující vzorové souřadnice,
  \item transformovaná báze (kterou chceme transformovat),
  \item báze určující souřadnice obrazů,
  \item přetransformovaná báze (kterou jsme transformovali).
\end{enumerate}
To je poněkud matoucí, ale není na tom nic těžkého. Když tedy lineární zobrazení vyjádříme
jako matici, dostáváme:
\begin{equation*}
\mymatrix{L}\mycoord{b} = \mycoord{b'}
\end{equation*}
kde $\mycoord{b}$ je vektor báze (2) vyjádřený jako souřadnice báze (1), $\mycoord{b'}$ je
přetransformovaný vektor z báze (4) vyjádřený jako souřadnice báze (3) a sloupce matice
$\mymatrix{L}$ jsou obrazy vektorů báze (1) vyjádřené jako souřadnice báze (3) (viz. kapitola
XXX). Ztratili jste se? Zkuste si to ještě jednou, chce to jen trochu pečlivosti. Báze
(1) a (2) jsou obvykle stejné. Pak vektory $\mycoord{b}$ mají jednoduchý tvar 
$(0, \ldots, 0, 1, 0, \ldots, 0)^T$ a sloupce matice $\mymatrix{L}$ obsahují přímo obrazy
transformované báze.

Protože oba prostory musí mít stejnou dimenzi, matice $\mymatrix{L}$ musí být čtvercová.
Navíc sloupce tvoří bázi, tudíž musí být lineárně nezávislé. Podle velké věty algebry
je hodnost matice a hodnost transponované matice stejná, tudíž musí mít i lineárně
nezávislé řádky. Tedy, matice je regulární. Pro regulární matici existuje inverzní matice,
která nepříliš překvapivě odpovídá inverznímu zobrazení:
\begin{align*}
\mymatrix{L}\mycoord{v} &= \mycoord{w} \\
\mymatrix{L}^{-1}\mymatrix{L}\mycoord{v} &= \mymatrix{L}^{-1}\mycoord{w} \\
\mycoord{v} &= \mymatrix{L}^{-1}\mycoord{w} \\
\end{align*}
Matice $\mymatrix{L}^{-1}$ tedy odpovídá lineárnímu zobrazení, které každému obrazu
přiřadí jeho vzor, což je definice inverzního zobrazení.

Nyní si situaci ještě zjednodušme. Omezme se na transformaci z prostoru $\myspace{V}$
konečné dimenze do stejného prostoru $\mymap{L}: \myspace{V}\rightarrow\myspace{V}$.
Pokud báze (1), (2) a (3) vezmeme stejné, bude matice $\mymatrix{L}$ obsahovat přímo
přetransformované bázové vektory vyjádřené jako souřadnice původní báze.

Za těchto podmínek si můžeme položit otázku: jak se změní souřadnice vektoru $\myvec{v}$,
pokud přetransformujeme souřadnicový systém? Tato otázka je důležitá pro praxi, například
Newtonovo jablko padá k zemi s určitým vektorem rychlosti v daném časovém okamžiku. Jaké
bude mít vektor souřadnice, pokud například pootočíme souřadnicovou soustavu? Zkusme se
kouknout:
\begin{equation*}
\myvec{v} = \myscalar{v_1}\myvec{b_1} + \cdots + \myscalar{v_n}\myvec{b_n}
          = \myscalar{v'_1}\mymap{L}(\myvec{b_1}) + \cdots + \myscalar{v'_n}\mymap{L}(\myvec{b_n})
\end{equation*}
kde $\myscalar{v_i}$ jsou souřadnice vektoru před transformací, $\myscalar{v'_i}$ jsou souřadnice
vektoru po transformaci, $\myvec{b_i}$ jsou vektory báze před transformací a $\mymap{L}(\myvec{b_i})$
vektory báze po transformaci. Pokud toto zapíšeme jako souřadnice vůči bázi před transformací, získáme
\begin{equation*}
\mymatrix{I}\mycoord{v} = \mymatrix{L}\mycoord{v'}
\end{equation*}
Vektory báze před transformací vyjádřené jako souřadnice vůči sobě tvoří jednotkovou matici
$\mymatrix{I}$. Obrazy vektorů báze vyjádřené jako souřadnice vůči bázi před transformací
tvoří matici transformace $\mymatrix{L}$ (viz. XXX). Pokud tuto rovnici zleva vynásobíme
inverzní maticí $\mymatrix{L}^{-1}$ (matice transformace je regulární, proto inverzní
matice existuje) dostaneme
\begin{equation*}
\mymatrix{L}^{-1}\mycoord{v} = \mycoord{v'}
\end{equation*}
Souřadnice vektoru se tedy transformují \textbf{opačně}, než se transformuje báze.
Například pokud souřadnicovou soustavu otočíme o úhel $\alpha$ doprava, souřadnice vektoru
se změní tak, jakoby se vektor v původní souřadnicové soustavě otočil o stejný úhel doleva.
Pokud zvětším měřítko, souřadnice vektoru se změní, jakoby se měřítko vektoru zmenšilo (viz.
příklad XXX).

Tato vlastnost se nazývá \textit{kontravariance} a budeme se jí společně s jejím protějškem
\textit{kovariancí} zabývat v kapitole XXX.

\chapter{Duální vektorový prostor}

\section{Definice}

\noindent Pokud opustíme klidné vody vektorových prostorů, dostáváme se do bouřlivého oceánu
nepochopitlených úkazů. A hned prvním úkazem, o který si samouk nabije pusu, je \textit{duální
vektorový prostor}. Posuďte sami, něco takového najdete na Wikipedii:

\begin{definition}
Mějme vektorový prostor $\myspace{V}$ nad tělesem $\myspace{T}$. Pak \textbf{duální vektorový
prostor k prostoru $\myspace{V}$}, značený jako $\mydual{V}$, je množina všech lineárních
zobrazení $\mymap{\varphi}: \myspace{V}\rightarrow\myspace{T}$ doplněná o operace sčítání
a skalárního násobení:

\begin{enumerate}
  \item $(\mymap{\varphi} + \mymap{\psi})(\myvec{x}) = \mymap{\varphi}(\myvec{x}) + \mymap{\psi}(\myvec{x})$
  \item $(\myscalar{\alpha}\mymap{\varphi})(\myvec{x}) = \myscalar{\alpha}(\mymap{\varphi}(\myvec{x}))$
\end{enumerate}

\end{definition}

\noindent Jasné ne?

Ani se nemusíte pokoušet definici pochopit a už jste ztracení. Jaké lineární zobrazení z vektorového
prostoru do tělesa, když podle definice má jít o zobrazení z vektorového prostoru do vektorového
prostoru? Tady se totiž tiše zamlčel fakt, že těleso je možné chápat jako vektorový prostor sám
nad sebou, kdy operace vektorového sčítání a skalárního násobení přímo odpovídají sčítání a násobení
definovaného pro těleso. Je triviální si ověřit, že axiomy vektorového prostoru jsou v této situaci
vlastně shodné s odpovídajícími axiomy tělesa.

Dobře, nyní jsme si ujasnili formální stránku věci. Pojďme si zkusit vydedukovat, k čemu by něco
takového bylo. Pro tento účel nejdříve potřebujeme rozebrat význam prvků duálního prostoru.

\section{Lineární forma}

\noindent Lineární zobrazení do vlastního tělesa má svoji důležitost, a proto dostalo svůj vlastní
název \textbf{lineární forma}. Duální prostor je tedy množina všech lineárních forem nad příslušným
vektorovým prostorem. Lineární forma braná jako prvek duálního prostoru se obvykle nazývá 
\textbf{kovektor}.

Víme, že každé lineární zobrazení v konečných prostorech lze vyjádřit jako matici. Lineární forma
je zobrazení do tělesa, tedy prostoru s dimenzí 1. Její matice tedy bude mít pouze jednu řádku:
\begin{equation*}
\myscalar{K}=\mymap{L}(\myvec{x}) = \mycoord{\varphi}^T\mycoord{x} = \myscalar{\varphi_1}\myscalar{x_1} + \cdots + \myscalar{\varphi_n}\myscalar{x_n}
\end{equation*}
Pokud si zavzpomínáte na sladké časy na základní škole, možná si také vzpomenete, že rovnice, kterou
jsme dostali, se nápadně podobá rovnici plochy, kterou jsme se učili v základech geometrie. A nejedná
se o podobu náhodnou. Pomocí lineární formy můžeme zobecnit pojem \textbf{plocha} jako množinu vektorů,
pro které forma vrací stejnou hodnotu.

Všimli jste si? Nyní už pro definici plochy nepotřebujeme žádný geometrický význam. Plochu tak
lze nadefinovat i nad naprosto podivnými vektorovými prostory, nemusí to být jenom $\mathbb{R}^3$.

Ovšem tvrdit, že lineární forma určuje plochu, by bylo chybné. Forma určuje obecně nekonečně mnoho
ploch - pro různé prvky tělesa různé plochy.

\begin{example}\label{example:linforma1}Lineární formy v $\mathbb{R}^2$: pokud si vezmeme příklad
dvourozměrného prostoru, zde ``plocha`` je přímka. Linerání forma tedy určuje rovnoběžné přímky.
Příklad je na obrázku (čísla jsou hodnoty vrstevnic).
\begin{center}
\includesvg[width=120pt]{linforma}
\end{center}
Červená šipka vyjadřuje normálu přímky. Pokud souřadnice normály vypíšeme do řádkového
vektoru, získáme ono maticové vyjádření lineární formy.

Ve standardní geometrii nás příliš nezajímala velikost normálového vektoru. Pokud nebyl nulový, určoval
přímku stejně bez ohledu na velikost. V počítačové grafice je například zcela běžné, že se normála
upravuje tak, aby měla jednotkovou velikost.

Ovšem v případě duálních prostorů nás velikost zajímá. Různé velikosti normálových vektorů odpovídají
různým lineárním formám, tedy různým prvkům duálního prostoru.

Geometricky nám velikost normály určuje ``hustotu`` přímek v jejím směru. Pokud je normála malá,
odstup přímek pro stejný rozdíl $K$ bude větší. Pokud je normála velká, odstup bude menší. Můžeme
na to koukat jako na vrstevnice plochy, která je vztyčená do třetí osy. Čím větší je normála, tím
je tato plocha strmější.

Anebo jinak, délka normály je velikost, o kolik plocha v jejím směru vzroste na jednotkovou
vzdálenost.

Analogie s vrstevnicemi je všeobecně používaná. V učebnicích se kovektory na obrázcích často kreslí
jako sekvence paralelních ploch - vrstevnice jakési pro nás nepředstavitelné čtyřrozměrné plochy.

\medskip\noindent
Na linerání formu lze koukat také jako na určité pravítko. Pokud vezmu vektor a dosadím ho do linerání
formy, získám skalární hodnotu odpovídající průmětu vektoru naměřenou na imaginárním pravítku, které
umístím ve směru normály vrstevnic s nulou v počátku. Délka normály je pak nepřímo úměrná hustotě značek
na pravítku.

Tato analogie osvětluje v budoucnu vyžadovanou podmínku, aby linerání forma vracela pro stejný vektor
stejnou hodnotu bez ohledu na zvolenou bázi vektorového prostoru. Pravítko prostě musí stejný vektor
změřit pokaždé stejně.

\end{example}

\begin{example}\label{example:linforma2}Kreslení křivky. Jako další příklad si zkusme položit následující
úkol: máme dva body, mezi kterými chceme nakreslit křivku. V krajních bodech máme určený tečný vektor.
Jeho směr definuje tečnu křivky, jeho délka určuje, jak moc se křivka bude k tečně ``lepit``. Naším úkolem
je nalézt parametrické vyjádření křivky. Parametr si označíme jako $t$ a standardně ho necháme se pohybovat
v intervalu \textit{<0, 1>}. Situace je ukázaná na obrázku.

\begin{center}
\includesvg[width=200pt]{curve}
\end{center}

Už jsme si ukázali, že polynomy do určitého stupně tvoří vektorový prostor. Předpokládejme tedy, že
naše parametrické vyjádření bude polynom třetího stupně. Proč zrovna třetího? Máme 4 okrajové podmínky
(dva krajní body a dva tečné vektory), tudíž dává smysl použít prostor s dimenzí 4. Uvidíme později.

V principu vlastně chceme získat dva polynomy - jeden vyjadřující $x$ v závislosti na parametru $t$
a druhý vyjadřující $y$ v závislosti na parametru $t$. Následující odvození řeší souřadnici $x$, postup
pro druhou souřadnici je ekvivalentní.

Náš hledaný polynom tedy vypadá 
\begin{equation*}
p(t)=at^3 + bt^2 + ct + d
\end{equation*}
V bázi $\{t^3, t^2, t, 1\}$ jsou jeho souřadnice $(a, b, c, d)^T$. Abychom spočítali koeficienty
polynomu, musíme využít okrajové podmínky, které máme. Začněme prvním krajním bodem:
\begin{equation*}
x_0 = p(0) = 0a + 0b + 0c + d = \left(0, 0, 0, 1\right)\left(\begin{array}{c}
a\\
b\\
c\\
d
\end{array}\right)
\end{equation*}
Hodnotu polynomu pro $t=0$ jsme vyjádřili jako součin řádkového vektoru a sloupcového vektoru. Sloupcový
vektor jsou souřadnice vektoru v prostoru polynomů. Řádkový vektor tak určuje lineární formu. V tomto
případě nám lineární forma vrací hodnotu polynomu v $0$.\footnote{
  V XXX jsme si řekli, že každá matice určuje nějaké lineární zobrazení. Řádkový vektor je vlastně také
  matice, proto nám určuje lineární zobrazení. Jinak bychom samozřejmě mohli ověřit, zda zobrazení
  ``hodnota polynomu v bodě 0`` splňuje podmínky lineárního zobrazení. To je poměrně triviální úkol.
}

O kousek výše jsme říkali, že lineární forma zobecňuje pojem plocha. V tomto případě ``plochu`` tvoří
všechny polynomy, které v bodě $0$ mají stejnou hodnotu. A nás konkrétně zajímá ``plocha``, kdy tato
hodnota je $x_0$.

Zcela analogicky využijeme druhý krajní bod:
\begin{equation*}
x_1 = p(1) = a + b + c + d = \left(1, 1, 1, 1\right)\left(\begin{array}{c}
a\\
b\\
c\\
d
\end{array}\right)
\end{equation*}
Pro využití tečných vektorů musíme polynom nejdříve zderivovat:
\begin{equation*}
p'(t)=3at^2 + 2bt + c
\end{equation*}
a následně opět dosadit za parametr $t$ nulu a jedničku:
\begin{equation*}
\Delta x_0 = p'(0) = c = \left(0, 0, 1, 0\right)\left(\begin{array}{c}
a\\
b\\
c\\
d
\end{array}\right)
\end{equation*}
\begin{equation*}
\Delta x_1 = p'(1) = 3a + 2b + c = \left(3, 2, 1, 0\right)\left(\begin{array}{c}
a\\
b\\
c\\
d
\end{array}\right)
\end{equation*}
Opět jsme získali dvě lineární formy, které vrací hodnotu derivace polynomu v bodě $0$ a $1$.

Celkově nám všechny čtyři okrajové podmínky určují 4 ``plochy``. A my potřebujeme najít jejich
průsečík. Tím získáváme jednoduchou soustavu rovnic\footnote{
  Zde je vidět, proč jsme zvolili polynom třetího stupně. Máme 4 rovnice a abychom měli
  jednoznačné řešení, potřebujeme 4-rozměrný prostor. Pokud by měl méně rozměrů, okrajové
  podmínky by nemohly být nezávislé. Pokud by byl větší, polynomů by bylo nekonečně mnoho.
}
\begin{equation*}
\left(\begin{array}{cccc}
0 & 0 & 0 & 1\\
1 & 1 & 1 & 1\\
0 & 0 & 1 & 0\\
3 & 2 & 1 & 0
\end{array}\right)\left(\begin{array}{c}
a\\
b\\
c\\
d
\end{array}\right)=\left(\begin{array}{c}
x_0\\
x_1\\
\Delta x_0\\
\Delta x_1
\end{array}\right)
\end{equation*}
jejíž řešení je
\begin{equation*}
\begin{split}
a &= 2x_0 - 2x_1 + \Delta x_0 + \Delta x_1\\
b &= -3x_0 + 3x_1 - 2\Delta x_0 - \Delta x_1\\
c &= \Delta x_0\\
d &= x_0
\end{split}
\end{equation*}
Hurá, nyní známe koeficienty polynomu a můžeme si celkem snadno křivku mezi body nakreslit.

Zvídavý čtenář si může za domácí úkol vyzkoušet, že slavné Bezierovy kubiky jsou shodné s námi
nalezeným polynomem, pouze třetí a čtvrtá krajní podmínka je $3\Delta x_0$ a $3\Delta x_1$ (tzn.
křivky se k tečným vektorům ``lepí`` o něco agresivněji).

\end{example}

\noindent Pojďme se nyní zamyslet nad případem, kdy do lineární formy dosadíme nulový vektor. Již víme
(viz. XXX), že obrazem nulového vektoru je opět nulový vektor. V případě lineární formy nulový
prvek tělesa. Pokud tedy přijmeme geometrickou představu lineární formy jako ``vrstevnice``
nadřazené plochy z prostoru o jeden řád větší, pak tato plocha vždy \textbf{prochází počátkem
souřadnicového systému}: $\mymap{L}(\myvec{0}) = \myscalar{0}$, tedy vrstevnice pro $K = 0$ prochází
počátkem.

Ve fyzice se mluví o \textit{volných vektorech} - tzn. vektor má velikost a směr, ale není vázaný
ke konkrétnímu bodu v prostoru. Kovektory se chovají stejně - zajímá nás pouze prudkost a směr
sklonu plochy, ale ne, kde je nadřazená plocha v prostoru umístěná.

Tento význam je ve fyzice, často používaný. Typický kovektor je např. gradient, tzn. tečná plocha
funkce (hodnota funkce přidává rozměr navíc). Gradient se sice počítá pro konkrétní bod funkce,
jeho složky (parciální derivace v daném bodě) už ovšem tuto informaci dále nenesou.

Na závěr kapitoly proberme krajní případ: $\mymap{L}(\myvec{x}) = \myscalar{0}$. V geometrickém významu je
zřejmé, že tato rovnice neurčuje žádnou plochu ve vektorovém prostoru, nad kterým je forma definovaná,
protože normálový vektor je nulový. Nicméně jedná se o platnou lineární formu a její geometrický
význam jsou ``vrstevnice`` nadřazené plochy, která je rovnoběžná se ``základnou`` - například gradient
konstantní funkce nebo gradient funkce v lokálním extrému.

\subsection{Shrnutí}

Lineární forma tedy zobecňuje pojem plocha (rovina): rovnice $L(\mycoord{x}) = \myscalar{K}$ definuje
plochu i nad prostory, kde tento pojem nemá geometrickou představu.

Protože nás ale zajímá i délka normály, lineární forma určuje plochu v prostoru, který je o jeden
rozměr větší. Geometricky lineární forma definuje ``vrstevnice`` této plochy do prostoru, nad kterým
je forma definovaná. Tato plocha je určená jednoznačně, kromě jejího umístění.

\vfill
\section{Vektorový prostor?}

\noindent Pozorný čtenář si možná všiml, že jsme v definici duálního vektorového prostoru poněkud
podváděli. Nadefinovali jsme duální prostor, ale nijak jsme neprokázali, že se jedná o vektorový
prostor. Pojďme to nyní rychle napravit.

Ve skutečnosti je to dost snadné. Sčítání a násobení skalárem je definované pomocí operací nad tělesem,
takže zcela přirozeně splňuje podmínky pro komutativitu, asociativitu, distribuci a násobení jednotkovým
prvkem:
\begin{equation*}
\begin{split}
\varphi(x) + \psi(x) &= \psi(x) + \varphi(x)\\
(\varphi(x) + \psi(x)) + \theta(x) &= \varphi(x) + (\psi(x) + \theta(x))\\
\alpha(\beta \varphi(x)) &= (\alpha\beta)\varphi(x)\\
\alpha(\varphi(x) + \psi(x)) &= \alpha\varphi(x) + \alpha\psi(x)\\
(\alpha + \beta)\varphi(x) &= \alpha\varphi(x) + \beta\varphi(x)\\
\myscalar{1}\varphi(x) &= \varphi(x)
\end{split}
\end{equation*}
Nyní potřebujeme nalézt nulový prvek tak, aby:
\begin{equation*}
\begin{split}
\myvec{0}(x) + \varphi(x) &= \varphi(x)
\end{split}
\end{equation*}
Takovou podmínku přirozeně splňuje lineární forma
\begin{equation*}
\begin{split}
\myvec{0}(x) &= \myscalar{0}
\end{split}
\end{equation*}
která, jak jsme si ukázali v předchozí kapitole, je krajním případem, a tudíž i snadno očekávatelná.
A s existujícím nulovým prvkem už není problém nalézt opačný prvek, jako lineární formu danou předpisem
\begin{equation*}
\begin{split}
-\varphi(x) &= \myscalar{-1}\varphi(x)
\end{split}
\end{equation*}

Nyní věříme, že duální prostor je vektorový prostor. Zkusme nyní zkonstruovat jeho bázi.

\label{def:dual_basis}
Mějme vektorový prostor $\myspace{V}$ s bází $\{\myvec{b_1}, \ldots, \myvec{b_n}\}$ a k němu duální
vektorový prostor $\mydual{V}$. Pro libovolný kovektor $\mymap{\varphi} \in \mydual{V}$ a libovolný
vektor $\myvec{x} = \myscalar{\alpha_1}\myvec{b_1} + \cdots + \myscalar{\alpha_n}\myvec{b_n} \in 
\myspace{V}$ můžeme (z~definice lineárního zobrazení) napsat:
\begin{equation*}
\mymap{\varphi}(\myvec{x}) = \mymap{\varphi}(\myscalar{\alpha_1}\myvec{b_1} + \cdots 
  + \myscalar{\alpha_n}\myvec{b_n}) = \myscalar{\alpha_1}\mymap{\varphi}(\myvec{b_1}) + \cdots
  + \myscalar{\alpha_n}\mymap{\varphi}(\myvec{b_n})
\end{equation*}
Zvolme nyní kovektory $\{\mymap{\epsilon_1}, \ldots, \mymap{\epsilon_n}\}$ takové, aby platilo
\begin{equation*}
\mymap{\epsilon_i}(\myvec{b_j}) = 
\begin{cases}
1 & i = j\\
0 & i \neq j
\end{cases}
\end{equation*}
a vyjádřeme (opět z definice lineárního zobrazení) jejich obrazy vektoru $\myvec{x}$
\begin{equation*}
\mymap{\epsilon_i}(\myvec{x}) = \myscalar{\alpha_1}\mymap{\epsilon_i}(\myvec{b_1}) + \cdots
  + \myscalar{\alpha_n}\mymap{\epsilon_i}(\myvec{b_n}) = \myscalar{\alpha_i}\mymap{\epsilon_i}(\myvec{b_i})
\end{equation*}
Nyní můžeme kovektory $\mymap{\epsilon_i}$ dosadit do vyjádření kovektoru $\mymap{\varphi}$ 
(nejdříve ke každému prvku součtu přinásobíme 1, následně otočíme pořadí součinu):
\begin{equation*}
\begin{split}
\mymap{\varphi}(\myvec{x}) &= \myscalar{\alpha_1}\mymap{\varphi}(\myvec{b_1}) + \cdots
  + \myscalar{\alpha_n}\mymap{\varphi}(\myvec{b_n}) \\
&= \myscalar{\alpha_1}\mymap{\varphi}(\myvec{b_1})\mymap{\epsilon_1}(\myvec{b_1}) + \cdots
  + \myscalar{\alpha_n}\mymap{\varphi}(\myvec{b_n})\mymap{\epsilon_n}(\myvec{b_n}) \\
&= \mymap{\varphi}(\myvec{b_1})\mymap{\epsilon_1}(\myvec{x}) + \cdots
  + \mymap{\varphi}(\myvec{b_n})\mymap{\epsilon_n}(\myvec{x})
\end{split}
\end{equation*}
Hodnoty $\mymap{\varphi}(\myvec{b_i})$ (dále značeno jako $\myscalar{\mu_i}$) jsou skaláry,
tedy libovolný kovektor můžeme vyjádřit jako lineární kombinaci kovektorů $\mymap{\epsilon_i}$.
Abychom je mohli považovat za bázi, zbývá dokázat, že jsou lineárně nezávislé. Předpokládejme tedy,
že existuje nenulová kombinace taková
\begin{equation*}
\myvec{0}(\myvec{x}) = \myscalar{\mu_1}\mymap{\epsilon_1}(\myvec{x}) + \cdots 
  + \myscalar{\mu_n}\mymap{\epsilon_n}(\myvec{x})
\end{equation*}
tedy alespoň jedno $\myscalar{\mu_i} \neq 0$. Pokud za vektor $\myvec{x}$ dosadíme vektor báze
$\myvec{b_i}$, dostaneme
\begin{equation*}
\myvec{0}(\myvec{b_i}) = \myscalar{\mu_i}\mymap{\epsilon_i}(\myvec{b_i}) = \myscalar{\mu_i} = 0
\end{equation*}
což je spor s původním předpokladem. Nenulová kombinace tedy neexistuje a množina kovektorů
$\{\mymap{\epsilon_1}, \ldots, \mymap{\epsilon_n}\}$ je lineárně nezávislá.

Zkonstruovali jsme tedy bázi $\{\mymap{\epsilon_1}, \ldots, \mymap{\epsilon_n}\}$ duálního prostoru.
Její velikost je $n$, tedy duální vektorový prostor \textbf{má stejnou dimenzi}, jako prostor, nad
kterým je definovaný.

Bázi, kterou jsme zkonstruovali, nazýváme \textbf{duální báze}. Aby nedošlo k mýlce, neplést se
spojením \textit{báze duálního prostoru}! Zatímco druhé označuje libovolnou bázi duálního
prostoru včetně té první, duální báze je jedna konkrétní báze duálního prostoru svázaná s
jednou konkrétní bází vektorového prostoru. Jinými slovy, pokud si zvolím bázi vektorového
prostoru, existuje k němu jedna konkrétní duální báze.

O kousek výše jsme libovolný kovektor vyjádřili jako
\begin{equation*}
\begin{split}
\mymap{\varphi}(\myvec{x}) &= \myscalar{\alpha_1}\mymap{\varphi}(\myvec{b_1}) + \cdots
  + \myscalar{\alpha_n}\mymap{\varphi}(\myvec{b_n}) \\
&= \myscalar{\alpha_1}\myscalar{\mu_1} + \cdots + \myscalar{\alpha_n}\myscalar{\mu_n}
\end{split}
\end{equation*}
Pokud vektor $\myvec{x}$ vyjádříme jako sloupcový vektor, získáme následující maticový součin
\begin{equation*}
\mymap{\varphi}(\myvec{x}) = \left(\begin{array}{ccc}
\myscalar{\mu_1} & \cdots & \myscalar{\mu_n}
\end{array}\right)\left(\begin{array}{c}
\myscalar{\alpha_1}\\
\vdots\\
\myscalar{\alpha_n}
\end{array}\right)
\end{equation*}
Kovektor zde působí jako \textbf{řádkový vektor}. Odtud také plyne, že v mnoha textech a učebnicích
se kovektor s řádkovým vektorem ztotožňuje.

\begin{example}\label{example:covector1}Ortonormální báze. Ortonormální báze je taková báze,
kdy pro skalární součin\footnote{
  O skalárním součinu jsme v tomto textu dosud nemluvili. Tak nějak se očekává, že ho čtenář
  zná.
} každé dvojice bázových vektorů platí:
\begin{equation*}
\myvec{b_i}\cdot\myvec{b_j} = 
\begin{cases}
1 & i = j\\
0 & i \neq j
\end{cases}
\end{equation*}
Z definice duální báze platí
\begin{equation*}
\mymap{\epsilon_i}(\myvec{b_i}) = 1 = \left(\epsilon_{i_1}, \ldots, \epsilon_{i_n}\right)
\left(\begin{array}{c}
b_{i_1} \\
\vdots \\
b_{i_n}\end{array}\right)
\end{equation*}
a z definice skalárního součinu platí:
\begin{equation*}
\left(b_{i_1}, \ldots, b_{i_n}\right)
\left(\begin{array}{c}
b_{i_1} \\
\vdots \\
b_{i_n}\end{array}\right) = 1
\end{equation*}
Je tedy zřejmé, že $\epsilon_{i_j} = b_{i_j}$. Jinými slovy, kovektory duální báze budou mít
zcela totožné souřadnice\footnote{
  Pozor! Nejedná se o stejné vektory. Vektory a kovektory ``žijí`` každý v jiném prostoru.
  Shodné budou pouze jejich souřadnice.
}.

Podívejme se na tuto situaci geometricky. Na následujícím obrázku je zobrazena ortonormální
báze v prostoru $\mathbb{R}^2$ (červeně) a naznačené ``vrstevnice`` kovektoru $\mymap{\epsilon_1}$:
\begin{center}
\includesvg[width=200pt]{covector_orthogonal}
\end{center}
Vrstevnice kovektoru $\mymap{\epsilon_1}$ o hodnotě 1 musí procházet vektorem $\myvec{b_1}$
a vrstevnice o hodnotě 0 musí procházet vektorem $\myvec{b_2}$. Zároveň z vlastností lineární
formy víme, že nultá vrstevnice musí procházet počátkem. Vrstevnice bázového kovektoru jsou
tedy rovnoběžné s druhou osou.

\medskip\noindent
Pozorování, že souřadnice ortonormální báze jsou shodné se souřadnicemi jejich duální báze,
je podstatně důležitější, než se na první pohled zdá. A zároveň vysvětluje další
z nevysvětlitelných problémů, na které samouk při studiu narazí.

Pokud se kouknete do libovolné učebnice, s klidným svědomím vám kreslí vektory i kovektory
jako šipečky nebo jako vrstevnice do jednoho obrázku. Což je dost podstatný nesoulad,
protože vektory a kovektory jsou prvky dvou zcela rozdílných prostorů.

Pozorování ortonormální báze tento problém vysvětluje. Plocha papíru obvykle symbolizuje
aritmetický prostor s ortonormální bází a geometrické vyjádření vektorů (ty ``šipečky``)
jsou jejich souřadnice vůči této bázi. Protože souřadnice ortonormální báze a její duální
báze jsou shodné, lze i kovektory kreslit jako ``šipečky`` na stejném papíru ve stejné
souřadnicové síti.

Jako bychom na papíru měli dvě průhledné vrstvy - jednu pro vektory, druhou pro kovektory.
Každá vrstva je oddělená (jiný prostor), ale my je vidíme dohromady jako jeden obrázek.

\end{example}

\begin{example}\label{example:covector2}Reciproční báze. Po všech útrapách s definováním
lineární formy a duálního prostoru jsme v předchozím příkladu dostali úplně stejný
řádkový vektor jako byl bázový sloupcový. Což je poněkud frustrující.

Naštěstí skutečná zábava s duální bází totiž začíná až v okamžiku, kdy báze vektorového prostoru
není ortonormální. Vezměme si případ báze $\myvec{b_1} = (2, 0)^T$ a $\myvec{b_2} = (1, 2)^T$
zobrazeném na následujícím obrázku
\begin{center}
\includesvg[width=200pt]{covector_general}
\end{center}
Číselně kovektor $\mymap{\epsilon_1}$ vychází takto\footnote{
  Povšimněte si, že bázové vektory zde vyjadřujeme jako souřadnice ortonormální báze. To je záměr,
  aby bylo něco vidět. Pokud bychom souřadnice psali vůči nim samotným, dostali bychom stejná čísla
  jako v předchozím příkladu a neměli bychom viditelný geometrický význam.
}
\begin{equation*}
\begin{split}
\left(\begin{array}{cc}\frac{1}{2} & -\frac{1}{4}\end{array}\right)\left(\begin{array}{c}2\\0\end{array}\right) &= 1\\
\left(\begin{array}{cc}\frac{1}{2} & -\frac{1}{4}\end{array}\right)\left(\begin{array}{c}1\\2\end{array}\right) &= 0
\end{split}
\end{equation*}
a kovektor $\mymap{\epsilon_2}$ takto
\begin{equation*}
\begin{split}
\left(\begin{array}{cc}0 & \frac{1}{2}\end{array}\right)\left(\begin{array}{c}2\\0\end{array}\right) &= 0\\
\left(\begin{array}{cc}0 & \frac{1}{2}\end{array}\right)\left(\begin{array}{c}1\\2\end{array}\right) &= 1
\end{split}
\end{equation*}
Pokud mluvíme o duální bázi v geometrickém významu, označuje se také někdy jako \textbf{reciproční báze}.
Vektor reciproční báze $\mymap{\epsilon_i}$ je vždy kolmý na všechny bázové vektory kromě $\myvec{b_i}$.

\end{example}

\section{Geometrické souvislosti}

\noindent Také vás na střední škole štvalo, že vám předhodili informace, ale už neřekli, proč platí?
Osobně jsem vždy bádal, proč skalární součin je násobek velikosti vektoru a velikosti průmětu druhého
vektoru. Stejně jsem nechápal, proč koeficienty obecné rovnice přímky/plochy jsou údajně její normála.

\medskip\noindent
Pokud se bavíme o geometrickém významu, obvykle pracujeme v prostoru $\mathbb{R}^n$ s ortonormální
bází. Plocha papíru, na kterou kreslíme čáry, simuluje dvourozměrný prostor a pozice bodů jsou
souřadnice vůči ortonormální bázi. Tedy i v této kapitole se omezíme na ortonormální bázi.

Začněme velikostí vektoru. Pokud udělám skalární součin vektoru se sebou samým, dostanu:
\begin{equation*}
\myvec{a}\cdot\myvec{a} = {a_1}^2 + \cdots + {a_n}^2
\end{equation*}
získávám známou Pythagorovskou větu, takže skalární součin vektoru se sebou samým můžeme označit
jako kvadrát velikosti:
\begin{equation*}
\|\myvec{a}\|^2 = \myvec{a}\cdot\myvec{a}
\end{equation*}
Správně? No, bohužel úplně ne. Pythagorova věta je definovaná a dokázaná pro trojúhelník. Tady ale máme
obecně více stran ($n$). Pokud zagooglíte a hledáte důkazy Pythagorovy věty ve více rozměrech,
většinou jsou postavené na skalárním součinu. Čímž vzniká poněkud cyklický důkaz: obecná Pythagorova
věta se dokazuje pomocí skalárního součinu, zatímco pro důkaz vlastností skalárního součinu budeme
potřebovat obecnou Pythagorovu větu.

Naštěstí samozřejmě existuje důkaz i bez skalárních součinů. Jednoduše nejdříve spočítáte velikost
průmětu vektoru do dvou souřadnic. Pak přidáte třetí, čímž opět vznikne pravoúhlý trojúhelník, pak
další a další až máte celkovou délku. (Důkaz si může čtenář udělat za domácí úkol.)

Tedy nakonec je skutečně možné skalární součin se sebou samým považovat za velikost vektoru. Podívejme
se nyní, jak je to se skalárním součinem, pokud jsou vektory rozdílné:
\begin{center}
\includesvg[width=200pt]{dotproduct}
\end{center}
Zkusme nyní vyjádřit velikost vektoru rozdílu (na obrázku červeně):
\begin{equation*}
\begin{split}
\|\myvec{a}-\myvec{b}\|^2 &= (\myvec{a}-\myvec{b})\cdot(\myvec{a}-\myvec{b}) \\
  &= \myvec{a}\cdot\myvec{a} - 2(\myvec{a}\cdot\myvec{b}) + \myvec{b}\cdot\myvec{b} \\
  &= \|\myvec{a}\|^2 + \|\myvec{b}\|^2 - 2(\myvec{a}\cdot\myvec{b})
\end{split}
\end{equation*}
Nyní zkusme vyjádřit velikost červené hrany pomocí známé cosinové věty\footnote{
  Cosinovou větu zde dokazovat nebudeme, čtenář důkaz může snadno nalézt na Internetu. Pro naše účely
  je potřeba vědět, že se dá dokázat geometricky bez potřeby skalárního součinu.
}:
\begin{equation*}
\|\myvec{a}-\myvec{b}\|^2 = \|\myvec{a}\|^2 + \|\myvec{b}\|^2 - 2\|\myvec{a}\|\|\myvec{b}\|cos\theta
\end{equation*}
Porovnáme-li obě rovnice, snadno získáváme očekávanou závislost:
\begin{equation*}
\myvec{a}\cdot\myvec{b} = \|\myvec{a}\|\|\myvec{b}\|cos\theta
\end{equation*}
Odtud už pak snadno vyplývá geometrický význam ``násobek délky vektoru krát násobek průmětu druhého
vektoru do prvního`` - $\|\myvec{a}\|cos\theta$ je průmět vektoru $\myvec{a}$ do vektoru $\myvec{b}$.
Skalární součin je symetrický, takže samozřejmě funguje i průmět $\myvec{b}$ do $\myvec{a}$.

Pokud jste si právě udělali pašáka, jak jsme to hezky dokázali, je potřeba podotknout, že ještě chybí
část důkazu, kdy $\theta$ je 0 nebo 180 stupňů - pak nelze sestrojit trojúhelník a nelze použít
cosinovou větu. Zkuste si to za domácí úkol. Nápověda: v tomto případě jde jeden vektor vyjádřit
jako skalární násobek druhého.

\medskip\noindent
Přesuňme se nyní k obecné rovnici přímky. Na střední škole nám řekli, že tato rovnice určuje
přímku/plochu:
\begin{equation*}
a_1x_1 + \cdots + a_nx_n + C = 0
\end{equation*}
s tím, že koeficienty $a_1, \ldots, a_n$ definují její normálový vektor. Je tomu skutečně tak?

Přepišme nyní rovnici do této podoby:
\begin{equation*}
a_1x_1 + \cdots + a_nx_n = -C
\end{equation*}
z předchozích kapitol víme, že levá strana rovnice je lineární forma s dosazeným bodem:
\begin{equation*}
\left(a_1, \ldots, a_n\right)\left(\begin{array}{c}x_1 \\ \vdots \\ x_n\end{array}\right)
\end{equation*}
Zvolme si dva body $\mycoord{x_1} \neq \mycoord{x_2}$, pro které rovnice platí:
\begin{equation*}
\begin{split}
a_1x_{1_1} + \cdots + a_nx_{1_n} &= -C \\
a_1x_{2_1} + \cdots + a_nx_{2_n} &= -C
\end{split}
\end{equation*}
Pokud rovnice odečteme, získáváme
\begin{equation*}
a_1(x_{1_1} - x_{2_1}) + \cdots + a_n(x_{1_n} - x_{2_n}) = 0
\end{equation*}
což je jinak zapsaný skálární součin
\begin{equation*}
\left(\begin{array}{c}a_1 \\ \vdots \\ a_n\end{array}\right)\cdot\left(\begin{array}{c}x_{1_1}-x_{2_1} \\
  \vdots \\ x_{1_n} - x_{2_n}\end{array}\right) = \mycoord{a} \cdot (\mycoord{x_1} - \mycoord{x_2}) = 0
\end{equation*}
Tedy, skalární součin vektoru $\mycoord{a}$ s jakýmkoliv směrovým vektorem v ploše je 0, tedy vektory
musí být kolmé.

Vektor $\mycoord{a}$ jsou souřadnice kovektoru v ortonormální bázi, proto platí geometrická analogie
``vrstevnic`` a jejich normály popisovaná v kapitole XXX.

\section{Duální prostor duálního prostoru}\label{section:dual-dual}

\noindent V matematice je normální, že ze šíleností můžete skládat další šílenosti. Protože duální
prostor je vektorový prostor, pak je možné vytvořit duální prostor duálního prostoru jako
$\mydual{(\mydual{V})}$ obvykle značený jako $\mydouble{V}$.

V principu by duální prostor duálního prostoru ani nebyl příliš zajímavý. Chová se úplně stejně, jako
každý jiný duální prostor. Ale přece jenom jednu význačnou vlastnost má - existuje pro něj
\textbf{přirozený isomorfismus} z jeho prapůvodního vektorového prostoru. Důsledkem tedy je, že duální
prostor duálního je v principu stejný jako původní vektorový prostor. Dvojnásobnou aplikací duality
se tak dostáváme zpět na začátek.

Isomorfismus je v principu zobrazení z jednoho prostoru do jiného, takové aby \textit{zachovalo strukturu}.
Jinými slovy pokud mám isomorfismus, mohu problém v jednom prostoru převést do druhého, vyřešit ho tam,
a následně převést zpět. Výsledky, které takto dostanu, budou zcela shodné, jako kdybychom problém počítali
celou dobu v prvním prostoru. Transformace souřadnic popsané v kapitole XXX jsou příkladem isomorfismu.

Dva prostory jsou isomorfní, pokud existuje nějaký isomorfismus mezi nimi.

Obecně musí být isomorfismus zobrazení \textbf{prosté} a \textbf{na}, aby bylo možné provést transformaci
jednoznačně tam i zpět. V případě vektorových prostorů musí být navíc \textbf{lineární}, aby zachovával
strukturu vektorových prostorů.

\medskip\noindent
Každé dva vektorové prostory stejné konečné dimense jsou isomorfní. To lze velmi snadno dokázat konstrukcí
zobrazení:
\begin{equation*}
\mymap{\Phi(\myvec{x})} = \mymap{\Phi(\alpha_1\myvec{v_1} + \cdots + \alpha_2\myvec{v_n})} = \alpha_1\myvec{w_1}
 + \cdots + \alpha_n\myvec{w_n} = \myvec{y}
\end{equation*}
Jinými slovy jsme vektoru $\myvec{x} \in \myspace{V}$ přiřadili vektor $\myvec{y} \in \myspace{W}$, který má
v cílovém prostoru stejné souřadnice. Je jednoduché dokázat, že se jedná o isomorfismus: souřadnice jednoznačně
identifikují vektor, tudíž každý vektor má pouze jeden obraz a každý obraz má pouze jeden vzor. Zachování
operací vektorového prostoru také jednoduše vyplývá z chování souřadnic.

Takový isomorfismus je sice zcela platný, ale člověk ho tak nějak necítí přirozeně. Není vidět žádný důvod,
proč byl pro nějaký vektor zvolený zrovna tento obraz. Pokud změníme některou z bází, změní se i zobrazení
a obrazy vektorů budou jiné. My bychom však chtěli vytvořit isomorfismus, kde přiřazení obrazu bude mít
nějaký smysl. A to bez ohledu na zvolenou bázi.

\medskip\noindent
Konstrukci přirozeného isomorfismu mezi vektorovým prostorem a jeho dvojitým duálním zkusíme vysvětlit opět
na geometrické analogii v $\mathbb{R}^2$. Kovektor jsme geometricky kreslili jako vrstevnice. A hodnota kovektoru
(kovektor je lineární forma) pro daný vektor říká, kolik vrstevnic vektor "protnul". Dá se na to také koukat,
jakobychom umístili pravítko ve směru souřadnic kovektoru a změřili průmět vektoru na něj. Velikost
kovektoru definuje hustotu značek na pravítku.

Pokud pro jeden vektor a jeden kovektor znám jeho hodnotu $\myvec{f(\myvec{x})} = K$, nedokážu zrekonstruovat
zpět vektor $\myvec{x}$, protože možných vektorů na stejné vrstevnici je nekonečně mnoho. Ze dvou (jsme stále
v $\mathbb{R}^2$) už to možné je - pokud nalezneme průsečík obou vrstevnic, získáme hledaný vektor:

\begin{center}
\includesvg[width=200pt]{rulers}
\end{center}

\noindent Takto získávám dvě rovnice
\begin{equation*}
\begin{split}
\myvec{f_1}(\myvec{x}) = K_1 \\
\myvec{f_2}(\myvec{x}) = K_2
\end{split}
\end{equation*}
které jednoznačně určují vektor $\myvec{x}$.

V případě duálního-duálního prostoru bychom chtěli najít isomorfismus, který by každému prvku prostoru
$\myspace{V}$ přiřadil prvek prostoru $\mydouble{V}$, který by na obrázku vypadal stejně jako vektor
$\myvec{x}$. Tím bychom ukázali, že dvojnásobnou aplikací duality se dostáváme zpět k prostoru,
ze kterého jsme vyšli. Přesněji k prostoru, který je přirozeně isomorfní, takže isomorfní prvky prostorů
můžeme chováním považovat za shodné.

Pokud tedy předchozí dvě rovnice zapíšu jako souřadnice v ortonormální bázi:
\begin{equation*}
\begin{split}
\left(f_{1_1} f_{1_2}\right)\left(\begin{array}{c}
x_1 \\
x_2\end{array}\right) = K_1 \\
\left(f_{2_1} f_{2_2}\right)\left(\begin{array}{c}
x_1 \\
x_2\end{array}\right) = K_2 \\
\end{split}
\end{equation*}
a součin otočím
\begin{equation*}
\begin{split}
\left(x_1 x_2\right)\left(\begin{array}{c}
f_{1_1} \\
f_{1_2}\end{array}\right) = K_1 \\
\left(x_1 x_2\right)\left(\begin{array}{c}
f_{2_1} \\
f_{2_2}\end{array}\right) = K_2
\end{split}
\end{equation*}
získám dvakrát maticový zápis stejné lineární formy s dosazenými kovektory $\myvec{f_1}$ a $\myvec{f_2}$:
\begin{equation*}
\begin{split}
\mycocovec{x}(\myvec{f_1}) = K_1 \\
\mycocovec{x}(\myvec{f_2}) = K_2
\end{split}
\end{equation*}
Je vidět, že souřadnice ko-ko-vektoru $\mycocovec{x}$ v duální-duální bázi jsou stejné, jako souřadnice
vektoru $\myvec{x}$ v ortonormálních souřadnicích prostoru $\myspace{V}$, tedy ``graficky`` by na obrázku
vypadaly stejně. Stejně tak je vidět, že nezávisí na volbě kovektorů $\myvec{f}$.

Tímto způsobem můžeme vytvořit odpovídající ko-ko-vektor pro každý vektor prostoru $\myspace{V}$, a tedy
můžeme nadefinovat zobrazení $\myspace{V}\rightarrow\mydouble{V}: \Psi(\myvec{x}) = \mycocovec{x}$.
A pokud toto zobrazení je isomorfismus, podařilo se nám najít, co jsme hledali.

\medskip\noindent
Přechozí postup ovšem trochu podváděl. Celou akci jsme dělali v rámci jedné (ortonormální) báze. My jsme
ale chtěli zkonstruovat přirozený isomorfismus, jehož konstrukce na bázi nezáleží. Proto začneme znovu,
tentokrát bez geometrické interpretace.

Mějme vektorový prostor $\myspace{V}$ nad tělesem $\myspace{T}$ a k němu duální vektorový prostor $\mydual{V}$
a duální-duální prostor $\mydouble{V}$. Pro každý prvek $\myvec{x}\in\myspace{V}$ zkonstruujeme zobrazení
$\mydual{V}\rightarrow\myspace{T}: \mycocovec{x}(\myvec{f}) = \myvec{f}(\myvec{x})$.
Toto zobrazení je lineární forma, protože
\begin{equation*}
\begin{split}
\mycocovec{x}(\myvec{f} + \alpha\myvec{g}) = (\myvec{f} + \alpha\myvec{g})(\myvec{x}) 
     = \myvec{f}(\myvec{x}) + \alpha\myvec{g}(\myvec{x})
     = \mycocovec{x}(\myvec{f}) + \alpha\mycocovec{x}(\myvec{g})
\end{split}
\end{equation*}
(nejprve se aplikuje definice zobrazení, následně definice operací sčítání a násobení skalárem duálního
prostoru) a tedy je prvkem duálního-duálního prostoru $\mydouble{V}$. Nadefinujme nyní zobrazení
$\myspace{V}\rightarrow\mydouble{V}: \Psi(\myvec{x}) = \mycocovec{x}$. Zbývá
dokázat, že toto zobrazení je isomorfismus.

\medskip\noindent
Že je zobrazení lineární, se dá dokázat poměrně snadno:
\begin{equation*}
\begin{split}
\Psi(\myvec{x} + \alpha\myvec{y})(\myvec{f}) &= \mycocovec{\myvec{x} + \alpha\myvec{y}}(\myvec{f})
     = \myvec{f}(\myvec{x} + \alpha\myvec{y}) = \myvec{f}(\myvec{x}) + \alpha\myvec{f}(\myvec{y}) \\
     &= \mycocovec{x}(\myvec{f}) + \alpha\mycocovec{y}(\myvec{f})
     = \Psi(\myvec{x})(\myvec{f}) + \alpha\Psi(\myvec{y})(\myvec{f}) \\
     &= (\Psi(\myvec{x}) + \alpha\Psi(\myvec{y}))(\myvec{f})
\end{split}
\end{equation*}
(Nejprve se aplikuje definice zobrzení. Pak se aplikuje definice zobrazení přiřazeného vektoru.
Tím získáme lineární formu z duálního prostoru. Pak můžeme aplikovat vlastnosti lineárního
zobrazení. A nakonec se z definic obou zobrazení a definice duálního prostoru dostáváme k linerání
kombinaci obrazů.)

\medskip\noindent
Důkaz, že je zobrazení prosté je poněkud složitější. Předpokládejme, že máme dva rozdílné vektory
$\myvec{x_1}\neq\myvec{x_2}\in\myspace{V}$, pro které zobrazení vrací stejné hodnoty
$\Psi(\myvec{x_1}) = \Psi(\myvec{x_2})$. Jestliže oba obrazy jsou stejné, pak musí platit:
\begin{equation*}
\begin{split}
\myvec{0} = \Psi(\myvec{x_1}) - \Psi(\myvec{x_2}) = \Psi(\myvec{x_1} - \myvec{x_2}) = \Psi(\myvec{y})
\end{split}
\end{equation*}
kde vektor zřejmě $\myvec{y}\neq\myvec{0}$. Z definicí musí dále platit:
\begin{equation*}
\begin{split}
\forall \myvec{f}\in\mydual{V}: \Psi(\myvec{y})(\myvec{f}) = \mycocovec{y}(\myvec{f}) 
    = \myvec{f}(\myvec{y}) = 0
\end{split}
\end{equation*}
Jestliže musí platit pro každý prvek duálního prostoru, můžeme dosadit libovolný kovektor
duální báze:
\begin{equation*}
\begin{split}
\myvec{\epsilon_i}(\myvec{y}) &= \myvec{\epsilon_i}(y_{1}\myvec{v_1} + \cdots + y_{n}\myvec{v_n})
    = y_{1}\myvec{\epsilon_i}(\myvec{v_1}) + \cdots + y_{n}\myvec{\epsilon_i}(\myvec{v_n}) \\
    &= y_{i}\myvec{\epsilon_i}(\myvec{v_i}) = 0
\end{split}
\end{equation*}
Je zřejmé, že aby rovnice platila, musí být souřadnice vektoru $y_i = 0$. Protože předchozí rovnice
musí platit pro každý vektor duální báze, musí být všechny souřadnice vektoru $\myvec{y}$ nulové,
což je v rozporu s předpokladem. Zobrazení tedy je prosté.

\medskip\noindent
Zaobrazení je \textbf{na} poměrně jednoduchou úvahou: pokud je zobrazení prosté a oba prostory
mají stejnou konečnou dimenzi, pak zobrazení musí být i \textbf{na}. (Důkaz si zkuste udělat
na dobrou noc doma. V principu převedu bázi prostoru $\myspace{V}$. Tak získám vektory, které
jsou také lineárně nezávislé. Pokud by nebyly, zobrazení by nebylo prosté, protože obraz
nenulové lineární kombinace v prostoru $\myspace{V}$ by dal obraz nulového vektoru v prostoru
$\mydouble{V}$. Takže obrazy báze jsou opět báze a musí být schopné vygenerovat všechny prvky.)

\section{Kovektor v souřadnicích}

\noindent
Již jsme několikrát označovali kovektor za řádkový vektor, několikrát jsme ho vyjadřovali jako
souřadnice v ortonormální bázi. Pojďme si nyní chování kovektoru v souřadnicích probrat pořádně.

Mějme tedy vektorový prostor $\myspace{V}$ s konečnou bází ${\myvec{v_1}, \hdots , \myvec{v_n}}$.
K němu mějme příslušný duální prostor $\mydual{V}$ s bází
${\myvec{\delta_1}, \hdots , \myvec{\delta_n}}$ (míněna obecně báze, nikoliv jen duální báze).
Nyní vezměme libovolný vektor
$\myvec{x}\in\myspace{V}: \myvec{x} = x_1\myvec{v_1} + \cdots + x_n\myvec{v_n}$
a libovolný kovektor
$\myvec{\varphi}\in\mydual{V}: \myvec{\varphi} = \varphi_1\myvec{\delta_1} + \cdots + \varphi_n\myvec{\delta_n}$
Pokud vektor dosadíme do kovektoru, oba vyjádříme jako lineární kombinaci báze a využijeme
vlastnosti lineárního zobrazení a definicí obou vektorových prostorů, získáme:
\begin{equation*}
\begin{split}
\myvec{\varphi}(\myvec{x}) &= \myvec{\varphi}(x_1\myvec{v_1} + \cdots + x_n\myvec{v_n})
     = x_1\myvec{\varphi}(\myvec{v_1}) + \cdots + x_n\myvec{\varphi}(\myvec{v_n}) \\
    &= x_1(\varphi_1\myvec{\delta_1} + \cdots + \varphi_n\myvec{\delta_n})(\myvec{v_1}) + \cdots
       + x_n(\varphi_1\myvec{\delta_1} + \cdots + \varphi_n\myvec{\delta_n})(\myvec{v_n}) \\
    &= x_1(\varphi_1\myvec{\delta_1}(\myvec{v_1}) + \cdots + \varphi_n\myvec{\delta_n}(\myvec{v_1})) 
    + \cdots \\
    & \;\;\;\;\; + x_n(\varphi_1\myvec{\delta_1}(\myvec{v_n}) + \cdots + \varphi_n\myvec{\delta_n}(\myvec{v_n}))
\end{split}
\end{equation*}
Tento velmi nepřehledný zápis můžeme přepsat do jednouššího maticového tvaru:
\begin{equation*}
\myvec{\varphi}(\myvec{x}) = \left(\varphi_1 \cdots \varphi_n\right)
\left(\begin{array}{ccc}
\myvec{\delta_1}(\myvec{v_1}) & \cdots & \myvec{\delta_1}(\myvec{v_n}) \\
\vdots & \ddots & \vdots \\
\myvec{\delta_n}(\myvec{v_1}) & \cdots & \myvec{\delta_n}(\myvec{v_n})
\end{array}\right)\left(\begin{array}{c}x_1 \\ \vdots \\ x_n\end{array}\right)
\end{equation*}
Prostřední čtvercová matice je definovaná zvolenými bázemi a dosazením souřadnic
vektoru a kovektoru do vzorečku dokážeme jednoduše spočítat hodnotu kovektoru
pro daný vektor.

Pokud si vzpomenete, jak jsme ji definovali, zjistíte, že pro duální bázi bude
prostřední matice \textbf{jednotková}. Pokud se tedy omezíme pouze na duální
bázi ke zvolené bázi prostoru $\myspace{V}$ (což je běžné), můžeme prostřední
matici vynechat a dostáváme se k obvyklému součinu ``řádkového vektoru se
sloupcovým``:
\begin{equation*}
\myvec{\varphi}(\myvec{x}) = \left(\varphi_1 \cdots \varphi_n\right)
\left(\begin{array}{c}x_1 \\ \vdots \\ x_n\end{array}\right)
= \mycoord{\varphi}^T\mycoord{x}
\end{equation*}

\medskip\noindent
Omezme se tedy pouze na duální bázi a zapřemýšlejme, jak se chovají souřadnice
kovektoru, pokud přetransformujeme bázi prostoru $\myspace{V}$ pomocí lineárního
zobrazení určeného maticí $\mymatrix{T}$.

Už víme, že souřadnice vektorů se transformují \textit{kontravariantně}:
\begin{equation*}
\mycoord{x'} = \mymatrix{T}^{-1}\mycoord{x}
\end{equation*}
a tedy
\begin{equation*}
\mycoord{x} = \mymatrix{T}\mycoord{x'}
\end{equation*}
Kovektor je lineární forma, která je nezávislá na souřadnicích, tudíž musí pro
transformované souřadnice vracet stejnou hodnotu (kovektor budeme dále zapisovat
jako řádkový vektor). 
\begin{equation*}
K = \mycoord{\varphi}\mycoord{x} = \mycoord{\varphi}\mymatrix{T}\mycoord{x'}
\end{equation*}
Tím tedy získávám, že nové souřadnice kovektoru jsou:
\begin{equation*}
\mycoord{\varphi'} = \mycoord{\varphi}\mymatrix{T}
\end{equation*}
Abychom mohli snadno porovnat rozdíl chování kovektorů a vektorů, převedeme kovektory
na sloupcový vektor\footnote{
  Autor osobně považuje ztotožňování kovektoru s řádkovým vektorem za velmi matoucí.
  Kovektor je sám vektorem v duálním prostoru, tudíž dává smysl ho vyjádřit
  jako sloupcový vektor.}
\begin{equation*}
\mycoord{\varphi'}^T = \mymatrix{T}^T\mycoord{\varphi}^T
\end{equation*}
Je tedy zřejmé, že zatímco souřadnice vektoru se mění podle inverzní matice, souřadnice
kovektoru se mění podle transponované transformační matice. Tato vlastnost se nazývá
\textit{kovariance}.

Co si pod tímto názvem máme představit? Obvyklé učebnice a texty na Internetu vám v tento
okamžik obvykle tvrdí, že souřadnice se transformují shodně s bází. To opět samoukům
působí bolesti hlavy, protože to logicky nedává smysl. Pokud orotuji souřadnicovou
soustavu, tak se kovektor musí změnit přece úplně stejně, jako vektor. Aby normála
vrstevnic udržela shodný směr, musí se přece transformovat proti směru rotace.

Odvozená transponovaná matice problém vysvětluje. Inverzní matice rotace souřadnic
(viz příklad XXX) je totiž shodná s její transponovanou maticí:
\begin{equation*}
\begin{split}
\mymatrix{T}^T &= \left(\begin{array}{cc}
\cos\alpha & -\sin\alpha\\
\sin\alpha & \cos\alpha\\
\end{array}\right)^T = \left(\begin{array}{cc}
\cos\alpha & \sin\alpha\\
-\sin\alpha & \cos\alpha\\
\end{array}\right) \\
&= \left(\begin{array}{cc}
\cos-\alpha & -\sin-\alpha\\
\sin-\alpha & \cos-\alpha\\
\end{array}\right) = \mymatrix{T}^{-1}
\end{split}
\end{equation*}
A proto i kovektor na rotaci souřadnic reaguje stejně jako vektor.

Naopak matice pro změnu měřítka se transpozicí nemění:
\begin{equation*}
\begin{split}
\mymatrix{T}^T &= \left(\begin{array}{cc}
\beta_1 & 0 \\
0 & \beta_2 \\
\end{array}\right)^T = \left(\begin{array}{cc}
\beta_1 & 0 \\
0 & \beta_2 \\
\end{array}\right) = \mymatrix{T}
\end{split}
\end{equation*}
a kovektor tak změnu měřítka sleduje. Pokud se bázové vektory prodlouží,
prodlouží se i kovektor, což znamená, že se vrstevnice více zahustí:
\begin{center}
\includesvg[width=220pt]{covector_transformation}
\end{center}
Na obrázku jsme dvakrát zvětšili bázové vektory. Druhá situace je ale
nakreslená v měřítku tak, aby stejné souřadnice v první a druhé bázi odpovídaly
stejnému bodu v ploše. Tím se vektor opticky zmenšil o polovinu a vrstevnice
se dvakrát zahustily. A hustota vrstevnic je nepřímo úměrná délce kovektoru.
Pokud by se vrstevnice nezahustily, začal by vektor s polovičními souřadnicemi
dosazením do kovektoru vracet jinou hodnotu.

Kovariance tedy neznamená, že se kovektory transformují stejně jako báze, ale
že se transformují stejně na změnu měřítka. Pokud se vektor báze zvětší, zvětší
se i souřadnice kovektoru. Pokud se vektor báze zmenší, zmenší se souřadnice
také. Z pohledu rotace souřadnicového systému se ovšem kovektor v souřadnicích
chová stejně jako vektor.

\chapter{Tensory}

\noindent
Tato kapitola je hlavní důvod, proč se autor pustil do samostudia lineární algebry.
Tensorová algebra potažmo tensorový kalkul je matematický základ obecné teorie
relativity a dalších fyzikálních disciplín jako mechanika pevných těles nebo kvantová
fyzika.

Bohužel právě zde nabýváte dojmu, že učitelé zapomněli učit. Každá učebnice, která
se mi dostala do ruky, se utápí ve změti písmenek a indexů. Vtipálek Zahradník pro
jistotu kapitolu o tensorech napsal Jiráskovskou češtinou, aby se již takhle nečitelná
učebnice stala nečitelnou ještě více. Bakalářská práce Michala Řepíka z pedagogické
fakulty (sic!) neobsahuje ani jednu větu, která by se normálním jazykem pokoušela
vysvětlil, co a k čemu to je.

Zkusme tento nedostatek tedy napravit.

\section{Co to ten tensor sakra je?}

\noindent
Pokud se pokoušíte pochopit tensorovou algebru, učebnice vám obvykle poskytují informace
jako:
\begin{itemize}
  \item tensory jsou zobecnění vektorů.
  \item Tensory jsou objekty s definovaným chováním při transformaci souřadnic.
  \item Tensor je multidimenzionální pole.
  \item Tensor je prvek prostoru vzniklého tensorovým součinem vektorových prostorů.
\end{itemize}
Nevím, jak vy, ale já si pod tímto nic představit nedokázal. Zobecnění vektorů? Vektor
si jde představovat jako šipku. Zobecnění si mám představit jak? Změna souřadnic? OK,
chápu, že nějak definovaně mění, takže v principu na souřadnicích nezávisí, ale co
s takovým objektem dělat? Multidimenzionální pole? OK, ale jaký význam má který prvek?
A o tensorovém součinu snad radši ani nemluvit.

Pokud je učebnice poněkud pokrokovější, tak vám také dá příklady:
\begin{itemize}
  \item skalár je tensor řádu 0,
  \item vektor a kovektor jsou tensory řádu 1
  \item a lineární zobrazení je dokonce tensor řádu 2.
\end{itemize}

\noindent
Jasné, ne? Situaci navíc komplikuje, že existují 3 různé, různě obecné, způsoby,
jak tensory zavést. Do toho se plete Einsteinova sumační notace, která samoukům
dokáže značně zamotat hlavu. Aby toho nebylo málo, ještě je možné za určitých podmínek
nerozlišovat kovariantní a kontravariantní složky, takže samouk narazí jednou na text,
který je rozlišuje, a jindy na text, který je nerozlišuje, a jen těžko hledá, co mají
společného. Jiné texty se snaží tensor násilně namapovat do matice, čímž se dost těžko
hledá, jak se odlišují od lineárního zobrazení. A sakra, pokud je tensor multidimenzionální
pole, co má být ta kovariantní a kontravariantní složka?

Taková vstupní bariéra pro získání znalosti je škoda, protože základní idea tensorů není
vůbec nijak komplikovaná.

V následujícím příkladu budeme používat některé pojmy a operace,
které jsme dosud nevysvětlili. To je záměr - získáváte tak kotvu, ke které se můžete
vrátit, až v budoucnu danou věc budeme probírat více teoreticky. Pokud pojmu nebudete
rozumnět, nelamte si s ním zatím hlavu - podstatné je sledovat kroky, které
příklad dělá, a co z nich nakonec vyjde.

\begin{example}

Ideu tensorové algebry si zkusíme ukázat na poměrně jednoduchém příkladu
\textbf{konvoluce polynomů}:
\begin{equation*}
\myvec{f}(u) = \int_\alpha^\beta \myvec{a}(\tau)\myvec{b}(u - \tau)d\tau
\end{equation*}
Pokud jste se dosud s konvolucí nesetkali, nevěšte hlavu. Pro účely příkladu to není
potřeba, stačí věřit, že se počítá tak, jak je výše napsáno.

Pokud jste se naopak setkali, možná vás zarazilo, že neintegruji od nekonečna do
nekonečna. Integrál od nekonečna do nekonečna u polynomů není říliš zajímavý, protože
polynom roste/klesá nade všechny meze. Takže v příkladu předpokládám, že polynomy jsou
``uříznuté`` a integrál lze napsat pouze v konečném intervalu.

Už víme, že polynom lze vyjádřit jako souřadnice v prostoru polynomů. Pro jednoduchost
příkladu si vezmeme dva polynomy řádu 1:
\begin{equation*}
\begin{split}
\myvec{a}(t) = a_1 t + a_2 \\
\myvec{b}(t) = b_1 t + b_2
\end{split}
\end{equation*}
\vbox

\medskip\noindent
Výpočet konvoluce polynomů musíme zahájit substitucí\footnote{
  Pro mezivýpočty budeme používat polynom $\myvec{p}$. Substituce pak samozřejmě
  je do polynomu $\myvec{b}$.
}:
\begin{equation*}
\begin{split}
\myvec{p}(u - t) = p_1(u - t) + p_2 = -p_1t + (p_1 u + p_2)
\end{split}
\end{equation*}
Integrujeme přes $t$, $u$ v této situaci považujeme za konstantu. Ze zápisu
je poměrně zjevné, že substituce funguje jako lineární zobrazení
$\mathcal{P}_1\rightarrow\mathcal{P}_1$, které můžeme zapsat jako matici\footnote{
  Z nějakého důvodu mi blockarray environment přidává na konec jednu prázdnou řádku.
  Lépe jsem následující matici naformátovat nedokázal.
}:
\begin{equation*}
\left(\begin{blockarray}{cc}
\begin{block}{(cc)}
-1 & 0 \\
\end{block}
\begin{block}{(cc)}
 u & 1 \\
\end{block}
\end{blockarray}\right)\left(
\begin{array}{c}
p_1 \\ p_2
\end{array}\right) = \left(
\begin{array}{c}
-p_1 + 0 p_2 \\ p_1 u + p_2
\end{array}
\right)
\end{equation*}
Povšimněte si použitého závorkování v matici. Z hlediska tensorů totiž na lineární
zobrazení koukáme jako na sloupcový (kontravariantní) vektor řádkových (kovariantních)
vektorů. Tím se dostáváme k oněm kontravariantním a kovariantním složkám - linerání
zobrazení má jednu kontravariantní (sloupcový první index) a kovariantní (řádkový
druhý index) složku. Neboli se jedná o tensor (1, 1).

Pokud tedy na lineární zobrazení aplikuji sloupcový vektor, roznásobím jím každý řádkový
vektor a dostanu tak jako výsledek sloupcový vektor. V případě tensorů se již nepracuje
se standardním maticovým součinem, ale s obecnějším \textbf{Kroneckerovým součinem}
a operací \textbf{kontrakce tensoru}. V tomto konkrétním případě je však výsledek
shodný.

Zatím nic zajímavého ani nového. Třeba vás ale zaujme novinka, že na kontravariantní
složku lineárního zobrazení můžete také aplikovat kovektor. Mějme například kovektor,
který vrací hodnotu polynomu v bodě 1. Pokud ho aplikuji na lineární zobrazení
\begin{equation*}
\left(\begin{blockarray}{cc}
\begin{block}{(cc)}
-1 & 0 \\
\end{block}
\begin{block}{(cc)}
 u & 1 \\
\end{block}
\end{blockarray}\right)\left(
\begin{array}{cc}
1 & 1
\end{array}\right) = \left(
1\left(\begin{array}{cc}-1 & 0\end{array}\right) + 1\left(\begin{array}{cc}u & 1\end{array}\right)
\right) = \left(\begin{array}{cc}
u - 1 & 1
\end{array}\right)
\end{equation*}
získám kovektor, který vrací hodnotu v bodě 1 polynomu po substituci.

\medskip\noindent
Další operaci, kterou s polynomy potřebujeme provést, je jejich roznásobení:
\begin{equation*}
\begin{split}
\myvec{p}(t)\myvec{q}(t) = (p_1 t + p_2)(q_1 t + q_2) = p_1q_1t^2 + p_2q_1t + p_1q_2t + p_2q_2
\end{split}
\end{equation*}
Tato operace na vstupu přijímá dva polynomy řádu 1 a na výstup dává jeden polynom
řádu 2. I tuto operaci můžeme zapsat do trojrozměnrné ``matice``:
\begin{equation*}
\left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& 1 & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 1 & 1 & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & 0 & 1 &\\
\end{block}
\end{blockarray}\right)
\end{equation*}
Každý prvek matice má tři souřadnice: první kontravariantní určuje řádku v sloupcovém vektoru,
druhá kovariantní určuje sloupec prvního vnořeného řádkového vektoru a třetí kovariantní
opět sloupec druhého vnořeného řádkového vektoru. Matice tedy má jednu kontravariantní a dvě
kovariantní složky, jedná se tedy o tensor (1, 2).

Pokud na tento tensor aplikujeme dva sloupcové vektory reprezentující souřadnice polynomů
\begin{equation*}
\begin{split}
& \left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& 1 & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 1 & 1 & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & 0 & 1 &\\
\end{block}
\end{blockarray}\right)
\left(\begin{array}{c}p_1 \\ p_2\end{array}\right)
\left(\begin{array}{c}q_1 \\ q_2\end{array}\right)= \\ = &
\left(\begin{blockarray}{c}
\begin{block}{(c)}
p_1\left(\begin{array}{cc}1 & 0\end{array}\right) + p_2\left(\begin{array}{cc}0 & 0\end{array}\right) \\
\end{block}
\begin{block}{(c)}
p_1\left(\begin{array}{cc}0 & 1\end{array}\right) + p_2\left(\begin{array}{cc}1 & 0\end{array}\right) \\
\end{block}
\begin{block}{(c)}
p_1\left(\begin{array}{cc}0 & 0\end{array}\right) + p_2\left(\begin{array}{cc}0 & 1\end{array}\right) \\
\end{block}
\end{blockarray}\right)
\left(\begin{array}{c}q_1 \\ q_2\end{array}\right)= \\ = &
\left(\begin{blockarray}{c}
\begin{block}{(c)}
\begin{array}{cc}p_1 & 0\end{array} \\
\end{block}
\begin{block}{(c)}
\begin{array}{cc}p_2 & p_1\end{array} \\
\end{block}
\begin{block}{(c)}
\begin{array}{cc}0 & p_2\end{array} \\
\end{block}
\end{blockarray}\right)
\left(\begin{array}{c}q_1 \\ q_2\end{array}\right)= 
\begin{blockarray}{(c)}
p_1q_1 \\
p_2q_1 + p_1q_2 \\
p_2q_2 \\
\end{blockarray}
\end{split}
\end{equation*}
jako výstup získáme souřadnice polynomu vzniklého jejich vynásobením. Tensor tedy funguje
jako zobrazení $\mathcal{P}_1\times\mathcal{P}_1\rightarrow\mathcal{P}_2$.

Tím ale legrace nekončí. My totiž na tento tensor můžeme aplikovat tensor, který nám vznikl
v předchozím kroku substitucí:
\begin{equation*}
\begin{split}
&\left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& 1 & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 1 & 1 & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & 0 & 1 &\\
\end{block}
\end{blockarray}\right)
\left(\begin{blockarray}{cc}
\begin{block}{(cc)}
-1 & 0 \\
\end{block}
\begin{block}{(cc)}
 u & 1 \\
\end{block}
\end{blockarray}\right) = \\ = &
\left(\begin{blockarray}{cccc}
\begin{block}{(c(c)(c)c)}
& 1\left(\begin{array}{cc}-1 & 0\end{array}\right) + 0\left(\begin{array}{cc}u & 1\end{array}\right) &
  0\left(\begin{array}{cc}-1 & 0\end{array}\right) + 0\left(\begin{array}{cc}u & 1\end{array}\right) & \\
\end{block}
\begin{block}{(c(c)(c)c)}
& 0\left(\begin{array}{cc}-1 & 0\end{array}\right) + 1\left(\begin{array}{cc}u & 1\end{array}\right) &
  1\left(\begin{array}{cc}-1 & 0\end{array}\right) + 0\left(\begin{array}{cc}u & 1\end{array}\right) & \\
\end{block}
\begin{block}{(c(c)(c)c)}
& 0\left(\begin{array}{cc}-1 & 0\end{array}\right) + 0\left(\begin{array}{cc}u & 1\end{array}\right) &
  0\left(\begin{array}{cc}-1 & 0\end{array}\right) + 1\left(\begin{array}{cc}u & 1\end{array}\right) & \\
\end{block}
\end{blockarray}\right) = \\ = &
\left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& -1 & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& u & 1 & -1 & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & u & 1 &\\
\end{block}
\end{blockarray}\right)
\end{split}
\end{equation*}
Kombinací těchto dvou zobrazení jsme získali zobrazení, které vrací polynom vzniklý
vynásobením $\myvec{p}(t)\myvec{q}(u - t)$.

Povšimněte si, že jsme tensor aplikovali na třetí souřadnici (nejvíce vnořené řádkové vektory),
která odpovídá polynomu $\myvec{q}$, tedy jsme substituovali do $\myvec{q}$. Nic nám samozřejmě
nebrání substituovat do $\myvec{p}$, tedy aplikovat tensor na druhou souřadnici. V tomto případě
by nám dokonce vyšlo to samé, protože násobení polynomů je komutativní. Obecně ale tensorový součin
komutativní není a aplikací na různé složky dostáváme různé výsledky.

\medskip\noindent
A pokračujeme v krasojízdě. Pro výpočet konvoluce potřebujeme vynásobený polynom zintegrovat:
\begin{equation*}
\int\myvec{p}(t)dt = \int(p_1t^2 + p_2t + p_3)dt = \frac{1}{3}p_1t^3 + \frac{1}{2}p_2t^2 + p_3t
\end{equation*}
Integrační konstantu jsme nezapomněli. Protože nám jde o určitý integrál, kde se nakonec
odečte, považujeme ji za 0. Pokud by nebyla nulová, funkce by nebyla lineární zobrazení,
tedy ani tensor. S nulovou konstantou se jedná o lineární zobrazení
$\mathcal{P}_2\rightarrow\mathcal{P}_3$.

A opět lineární zobrazení můžeme vyjádřit jako matici:
\begin{equation*}
\left(\begin{blockarray}{ccc}
\begin{block}{(ccc)}
\frac{1}{3} & 0 & 0 \\
\end{block}
\begin{block}{(ccc)}
 0 & \frac{1}{2} & 0 \\
\end{block}
\begin{block}{(ccc)}
 0 & 0 & 1 \\
\end{block}
\begin{block}{(ccc)}
 0 & 0 & 0 \\
\end{block}
\end{blockarray}\right)\left(
\begin{array}{c}
p_1 \\ p_2 \\ p_3
\end{array}\right) = \left(
\begin{array}{c}
\frac{1}{3}p_1 \\ \frac{1}{2}p_2 \\ p_3 \\ 0
\end{array}
\right)
\end{equation*}
Získali jsme tak jakýsi operátor, do kterého můžeme dosadit polynom a získáme jeho integrál. Pro
výpočet konvoluce potřebujeme dosadit polynom vzniklý vynásobením $\myvec{p}(t)\myvec{q}(u - t)$.
Nikoho už by nemělo překvapit, že na tensor integrace aplikujeme tensor vzniklý v předchozím
kroku:
\begin{equation*}
\begin{split}
&\left(\begin{blockarray}{ccc}
\begin{block}{(ccc)}
\frac{1}{3} & 0 & 0 \\
\end{block}
\begin{block}{(ccc)}
 0 & \frac{1}{2} & 0 \\
\end{block}
\begin{block}{(ccc)}
 0 & 0 & 1 \\
\end{block}
\begin{block}{(ccc)}
 0 & 0 & 0 \\
\end{block}
\end{blockarray}\right)
\left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& -1 & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& u & 1 & -1 & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & u & 1 &\\
\end{block}
\end{blockarray}\right)
 = \\ = &
\left(\begin{blockarray}{c}
\begin{block}{(c)}
\frac{1}{3}\left(\left(\begin{array}{cc}-1 & 0\end{array}\right)\left(\begin{array}{cc}0 & 0\end{array}\right)\right) \\
\end{block}
\begin{block}{(c)}
\frac{1}{2}\left(\left(\begin{array}{cc}u & 1\end{array}\right)\left(\begin{array}{cc}-1 & 0\end{array}\right)\right) \\
\end{block}
\begin{block}{(c)}
1\left(\left(\begin{array}{cc}0 & 0\end{array}\right)\left(\begin{array}{cc}u & 1\end{array}\right)\right) \\
\end{block}
\begin{block}{(c)}
\left(\left(\begin{array}{cc}0 & 0\end{array}\right)\left(\begin{array}{cc}0 & 0\end{array}\right)\right) \\
\end{block}
\end{blockarray}\right)
= \\ = &
\left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& -\frac{1}{3} & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& \frac{1}{2}u & \frac{1}{2} & -\frac{1}{2} & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & u & 1 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & 0 & 0 &\\
\end{block}
\end{blockarray}\right)
\end{split}
\end{equation*}
čímž jsme získali tensor, který po dosazení polynomů $\myvec{p}$ a $\myvec{q}$ vrací primitivní funkci konvolučního
integrálu.

\medskip\noindent
A začínáme se blížit do finiše. Jako poslední krok potřebujeme dosadit integrační konstanty $\alpha$ a $\beta$
a výsledek navzájem odečíst. Hodnota polynomu v konkrétním bodě je nám již známý kovektor:
\begin{equation*}
\myvec{p}(\alpha) = p_1\alpha^3 + p_2\alpha^2 + p_3\alpha + p_4
\end{equation*}
což lze vyjádřit jako matici:
\begin{equation*}
\left(\begin{array}{cccc}\alpha^3 & \alpha^2 & \alpha & 1\end{array}\right)
\left(\begin{array}{c}p_1 \\ p_2 \\ p_3 \\ p_4 \end{array}\right)
\end{equation*}
Kovektor můžeme aplikovat na kontravariantní složku tensoru, který nám vyšel v předchozím kroku:
\begin{equation*}
\begin{split}
&\left(\begin{blockarray}{cccccc}
\begin{block}{(c(cc)(cc)c)}
& -\frac{1}{3} & 0 & 0 & 0 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& \frac{1}{2}u & \frac{1}{2} & -\frac{1}{2} & 0 & \\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & u & 1 &\\
\end{block}
\begin{block}{(c(cc)(cc)c)}
& 0 & 0 & 0 & 0 &\\
\end{block}
\end{blockarray}\right)
\left(\begin{array}{cccc}\alpha^3 & \alpha^2 & \alpha & 1\end{array}\right)
= \\ = &
\;(
\alpha^3((\begin{array}{cc}-\frac{1}{3} & 0\end{array})(\begin{array}{cc}0 & 0\end{array}))
  + \alpha^2((\begin{array}{cc}\frac{1}{2}u & \frac{1}{2}\end{array})(\begin{array}{cc}-\frac{1}{2} & 0\end{array})) \\
  + & \;\alpha((\begin{array}{cc}0 & 0\end{array})(\begin{array}{cc}u & 1\end{array}))
  + 1((\begin{array}{cc}0 & 0\end{array})(\begin{array}{cc}0 & 0\end{array}))
)
= \\ = &
((\begin{array}{cc}-\frac{1}{3}\alpha^3 + \frac{1}{2}\alpha^2u & \frac{1}{2}\alpha^2\end{array})
  (\begin{array}{cc}-\frac{1}{2}\alpha^2 + \alpha u & \alpha\end{array}))
\end{split}
\end{equation*}
Tím jsme získali tensor (0, 2) se dvěma kovariantními složkami. Pokud stejným způsobem aplikujeme
kovektor pro integrační konstantu $\beta$ a výsledné tensory po složkách navzájem odečteme, získáme
opět (0, 2) tensor:
\begin{equation*}
\begin{split}
((\begin{array}{cc}\frac{1}{3}(\alpha^3 - \beta^3) + \frac{1}{2}u(\beta^2 - \alpha^2) & \frac{1}{2}(\beta^2 - \alpha^2)\end{array}) \\
  (\begin{array}{cc}\frac{1}{2}(\alpha^2 - \beta^2) + u(\beta - \alpha) & \beta - \alpha\end{array}))
\end{split}
\end{equation*}
Tento tensor funguje jako zobrazení $\mathcal{P}_1\times\mathcal{P}_1\rightarrow\mathbb{R}$. Pokud do něj
dosadíme dva polynomy, získáme hodnotu jejich konvoluce v bodě $u$ (což si čtenář může ověřit, pokud si
konvoluci dvou polynomů spočítá přímo).

\end{example}

\noindent
Jaké ponaučení si z příkladu můžeme vzít? Na jeho začátku jsme si nadefinovali polynomy $\myvec{a}$
a $\myvec{b}$, ale v průběhu jsme je už nikdy nepoužili. Výsledkem příkladu totiž je operátor, na
který je možné aplikovat jakékoliv dva polynomy včetně těchto dvou.

Tensory a tensorová algebra jsou tedy vlastně systém operátorů a možností, jak je aplikovat a kombinovat.
V příkladu jsme viděli, že např. tensor násobení dvou polynomů lze aplikovat na dva polynomy. Ale stejně
tak jsme ho aplikovali na tensor, který substituoval do polynomu, čímž vznikl nový tensor/operátor.

Pokud jste se někdy setkali s počítačovou grafikou, znáte kombinaci transformací. Grafik vezme objekt,
aplikuje na něj několik transformací (typicky ho posune v prostoru, otočí ho a zvětší nebo zmenší).
Každá transformace má svou matici, a pokud se vynásobí, vznikne transformace, která je kombinuje
dohromady.

Tensory tedy nejsou ani tak zobecněním vektorů, jako spíše zobecněním principu skládání operací s vektory.
V příkladu jsme zkombinovali dohromady operaci substituce do polynomu, násobení dvou polynomů, integraci
polynomu a dosazení hodnoty do polynomu. A tadáá, jako výsledek jsme dostali operátor, který je schopný
pro libovolné dva polynomy spočítat jejich konvoluci.

Takže, cože to ten tensor vlastně je? Kromě dvou krajních případů (skalár a vektor), tensor je operátor.
Operátor, který může vzít vektory, kovektory, jiné tensory a stvořit nový tensor. Operátor, který se může
zkombinovat s jiným operátorem a stvořit tak nový operátor. Tensor není libovolný operátor, musí splňovat
určité podmínky (označované jako multilinearita), což ovšem v lineární algebře není příliš překvapivé.
A samozřejmě není možné aplikovat každý tensor na každý tensor. O tom však až v dalších kapitolách.

\section{Bilineární a multilineární zobrazení}

\noindent
Než se pustíme do definice tensorů, je potřeba zavést v nadpisu zmíněné pojmy. Až doteď jsme znali
\textit{linenární zobrazení}, tzn. zobrazení z vektorového prostoru do jiného vektorového prostoru,
které zachovávalo sčítání vektorů a násobení skalárem: $\varphi(\alpha\myvec{u} + \myvec{v}) =
\alpha\varphi(\myvec{u}) + \varphi(\myvec{v})$. S lineárními zobrazeními se dobře počítá, dají se
elegantně skládat a obecně jsou to pro nás již dobře známá zvířátka.

Na druhou stranu množství reálných problémů, které dokážeme popsat lineárním zobrazením, je poměrně
omezené. Zaprvé obvykle potřebujeme pracovat s více vektory než s jedním. Zadruhé linearita je celkem
vzácná vlastnost, obzvláště od doby, kdy Einstein rozbil krásně lineární newtonovskou mechaniku.
Proto je potřeba náš záběr rozšířit a bilineární zobrazení je jednou z možností.

\begin{definition}
Mějme vektorové prostory $\myspace{U}$, $\myspace{V}$ a $\myspace{W}$ nad tělesem $\myspace{T}$
a zobrazení $\mymap{B}: \myspace{U}\times\myspace{V}\rightarrow\myspace{W}$. Zobrazení $\mymap{L}$ je
\textit{bilinerání} právě tehdy pokud platí:

\begin{enumerate}
  \item $\forall\myvec{u_1},\myvec{u_2}\in\myspace{U},\forall\myvec{v}\in\myspace{V}\;
    \mymap{B}(\myvec{u_1} + \myvec{u_2}, \myvec{v}) = \mymap{B}(\myvec{u_1}, \myvec{v}) + \mymap{B}(\myvec{u_2}, \myvec{v})$
  \item $\forall\myvec{u}\in\myspace{U},\forall\myvec{v_1},\myvec{v_2}\in\myspace{V}\;
    \mymap{B}(\myvec{u}, \myvec{v_1} + \myvec{v_2}) = \mymap{B}(\myvec{u}, \myvec{v_1}) + \mymap{B}(\myvec{u}, \myvec{v_2})$
  \item $\forall\myvec{u}\in\myspace{U},\forall\myvec{v}\in\myspace{V},\forall\myscalar{\alpha}\in\myspace{T}\;
    \mymap{B}(\myscalar{\alpha}\myvec{u}, \myvec{v}) = \myscalar{\alpha}\mymap{B}(\myvec{u}, \myvec{v}) =
      \mymap{B}(\myvec{u}, \myscalar{\alpha}\myvec{v})$
\end{enumerate}

\end{definition}

\noindent
Přídavné jméno bilineární je odvozené od pozorování, že zobrazení se z pohledu jednoho parametru chová lineárně.
Pokud si vyberete jeden parametr a druhý zafixujete (tzn. dosadíte konstantu), získáte dobře známé lineární
zobrazení. Pokud např. nadefinuji $\mymap{L}(\myvec{u}) = \mymap{B}(\myvec{u}, \myvec{a})$, pak
\begin{equation*}
\mymap{L}(\myscalar{\alpha}\myvec{u_1} + \myvec{u_2}) = \mymap{B}(\myscalar{\alpha}\myvec{u_1} + \myvec{u_2}, \myvec{a})
  = \myscalar{\alpha}\mymap{B}(\myvec{u_1}, \myvec{a}) + \mymap{B}(\myvec{u_2}, \myvec{a})
  = \myscalar{\alpha}\mymap{L}(\myvec{u_1}) + \mymap{L}(\myvec{u_2})
\end{equation*}
Tedy dobrá, máme bilineární zobrazení. Nastavené podmínky na první pohled vypadají dost vycucané z prstu.
Co jsme schopni bilineárním zobrazením popsat? K čemu nám taková konstrukce je?

Ve skutečnosti bilineární zobrazení je velmi přirozené a pracujeme s ním každou chvíli už někdy od druhé třídy
základní školy. Nevěříte?

\begin{example}
Vezměme si zobrazení $\mymap{P}: \myspace{R}\times\myspace{R}\rightarrow\myspace{R}$ dané předpisem
$\mymap{P}(\myscalar{a}, \myscalar{b}) = \myscalar{a}\myscalar{b}$. Jinými slovy zobrazení, které vrátí
násobek dvou čísel. Pokud chování obyčejného násobení dvou čísel porovnáte s podmínkami, které jsme
si položili na bilineární zobrazení, získáte silný pocit deja vu:
\begin{enumerate}
  \item $(\myvec{a} + \myvec{b})\myvec{c} = \myvec{a}\myvec{c} + \myvec{b}\myvec{c}$
  \item $\myvec{a}(\myvec{b} + \myvec{c}) = \myvec{a}\myvec{b} + \myvec{a}\myvec{c}$
  \item $(\myvec{a}\myvec{b})\myvec{c} = \myvec{a}(\myvec{b}\myvec{c}) = \myvec{b}(\myvec{a}\myvec{c})$
\end{enumerate}
\end{example}

\noindent
Stručně řečeno bilineární zobrazení je zobecnění operací, které obvykle známe pod názvem \textit{součin} -
jako bilineární zobrazení lze např. popsat násobení reálných čísel, skalární součin, vektorový součin,
násobení polynomů nebo součin matic. A určitě dokážete nalézt další příklady.

\medskip\noindent
\textit{Multilineární zobrazení} je jednoduché rozšíření bilineárního na více parametrů. Příkladem může
například být zobrazení kombinující vektorový a skalární součin:
\begin{equation*}
\mymap{M}(\myvec{u}, \myvec{v}, \myvec{w}) = (\myvec{u}\times\myvec{v})\cdot\myvec{w}
\end{equation*}
Podmínky 1 a 2 se zopakují pro další parametry, v podmínce 3 skalár ``navštíví`` i další parametry.

\begin{example}
Příkladem multilineárního zobrazení je determinant. Vezměme si 3 vektory $\myvec{u}$, 
$\myvec{v}$ a $\myvec{w}\in\myspace{R}^3$. Vektory uspořádejme po řádkách do matice:
\begin{align*}
D(\myvec{u}, \myvec{v}, \myvec{w}) =
\begin{vmatrix}
u_1 & u_2 & u_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
\end{align*}
Zobrazení $D$ je multilineární, jak lze snadno nahlédnout. Pokud některý z parametrů
vynásobím skalárem, skalár se objeví jednou v každém z termů, tudíž lze vytknout.
\begin{align*}
D(\myscalar{\alpha}\myvec{u}, \myvec{v}, \myvec{w}) =
\begin{vmatrix}
\myscalar{\alpha}u_1 & \myscalar{\alpha}u_2 & \myscalar{\alpha}u_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
=
\myscalar{\alpha}
\begin{vmatrix}
u_1 & u_2 & u_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
= \myscalar{\alpha}D(\myvec{u}, \myvec{v}, \myvec{w})
\end{align*}
Pokud jeden parametr bude součet dvou vektorů, v každém z termů bude součet příslušných
souřadnic. Je možné součet v termu roznásobit, následně termy přeuzávorkovat tak,
aby daly součet dvou determinantů.
\begin{align*}
D(\myvec{u} + \myvec{z}, \myvec{v}, \myvec{w}) =
\begin{vmatrix}
u_1 + z_1 & u_2 + z_2 & u_3 + z_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
= \\
\begin{vmatrix}
u_1 & u_2 & u_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
+
\begin{vmatrix}
z_1 & z_2 & z_3 \\
v_1 & v_2 & v_3 \\
w_1 & w_2 & w_3
\end{vmatrix}
= D(\myvec{u}, \myvec{v}, \myvec{w}) + D(\myvec{z}, \myvec{v}, \myvec{w})
\end{align*}
\end{example}

\section{Vsuvka: nekonečnědimenzionální vektorový prostor}

\noindent
Zdá se, že než se pustíme do tensorů, budeme muset udělat ještě jednu odbočku. Tento můj text
začíná lineárním zobrazením a předpokládá, že čtenář zná pojem vektorový prostor. S čím se ale
v běžných kurzech lineární algebry často nesetká, je prostor, který nemá konečnou bázi. Protože
obecná definice tensorů pro takové prostory platí také, upřesníme si, co se takovým prostorem
vlastně myslí.

Báze byla definovaná jako minimální množina lineárně nezávislých vektorů, která generuje celý
vektorový prostor - každý vektor lze vyjádřit jako lineární kombinaci bázových vektorů. U konečných
bází tato definice není problematická. U nekonečných ovšem problém nastává - jak sečíst nekonečné
množství vektorů? U nekonečných součtů už není možné libovolně otáčet pořadí prvků nebo různě
závorkovat. Naše báze ale nemá žádné uspořádání, takže ani nelze nadefinovat konkrétní pořadí.

Řešení je ve skutečnosti velmi jednoduché. Sice budeme pracovat s nekonečnou bází, ale budeme
vyžadovat, aby každý vektor šel vyjádřit jako \textbf{lineární kombinace konečného počtu vektorů
báze!} A obecněji, pokud tvrdíme, že nějaká podmnožina vektorového prostoru generuje celý prostor,
vždy se jedná o konečné lineární kombinace.

I když to možná na první pohled vypadá, že jsme si omezili možnosti, existuje důkaz, že taková
báze existuje pro každý vektorový prostor - tedy za předpokladu, ža čtenář akceptuje kontroverzní
Zornovo lemma (případně ekvivalentní axiom výběru).

Bohužel důkaz nám nedává žádný návod, jak takovou bázi zkonstruovat. Takže sice víme, že báze
existuje, ale v drtivé většině případů nevíme, jak vypadá.

\begin{example}
Vezměme si příklad prostoru $\mathcal{P}(\mathbb{R})$ všech polynomů nad reálnými čísly.
Každý polynom je konečný součet násobků funkcí $x^n$, tedy báze prostoru je množina funkcí
$\{1, x, x^2, x^3, \hdots\}$. Toto je jeden ze vzácných případů, kdy bázi dokážeme popsat.
\end{example}

\begin{example}
Jiný příklad je prostor $\mathbb{R}^\infty$, tzn. rozšíření standardních aritmetických prostorů
na nekonečné n-tice, tedy vlastně prostor posloupností. Jako první myšlenka by se nabízela možnost
seskládat bázi z vektorů $(\hdots, 0, 0, 1, 0, 0, \hdots)$. Tato báze by ovšem nedokázala
vygenerovat posloupnost $(1, 1, 1, \hdots)$ - součet by nebyl konečný. Ani přidání tohoto
vektoru do báze (je lineárně nezávislý, takže ho můžeme přidat) nám příliš nepomůže - pořád
bychom nedokázali vygenerovat $(1, 2, 3, 4, \hdots)$. Máme zde prostor, jehož bázi neznáme,
pouze věříme, že existuje.

Jen pro zajímavost, všimněte si, že množina konvergentních posloupností tvoří podprostor
$\mathbb{R}^\infty$ - součet konvergentních posloupností je opět konvergentní posloupnost,
násobek konvergentní posloupnosti je opět konvergentní posloupnost.
\end{example}

\section{Jak zavést tensory, aneb univerzální vlastnost}

\noindent
Původně jsem měl záměr zavádět tensory tak, jak je běžné ve většině učebnic: nejdříve je předvést
v podobě vícerozměrných polí, a následně se propracovávat k obecnějším definicím. Jenže jak jsem se
pročítal materiály a snažil se pochopit nikoliv matematický aparát ale spíše podstatu tensorů - k čemu
jsou, co s nimi lze popsat, jak si nadefinovat vlastní - zjistil jsem, že právě ta nejobecnější definice
je paradoxně nejjednodušší a nejvíc intuitivní. Proto i v této kapitole nakonec budu pokračovat
tak, jako v předchozích - začneme s obecnou definicí nezávislou na volbě bází a teprve později budeme
rozebírat, jak se tensory chovají v souřadnicích.

\begin{definition}
\label{def:tensor_product_1}
Mějme vektorové prostory $\myspace{U}$, $\myspace{V}$ nad tělesem $\myspace{T}$ a bilineární zobrazení
$\phi$ z $\myspace{U}\times\myspace{V}$ do vektorového prostoru značeného jako
$\myspace{U}\otimes\myspace{V}$. Bilineární zobrazení $\phi$ budeme nazývat \textit{tensorový součin}
a dvojici $(\phi, \myspace{U}\otimes\myspace{V})$ jako \textit{tensorový součin prostorů} $\myspace{U}$
a $\myspace{V}$ právě tehdy pokud platí:
\begin{enumerate}	
\item Pro libovolný vektorový prostor $\myspace{W}$ nad tělesem $\myspace{T}$ a libovolné bilineární
    zobrazení $f: \myspace{U}\times\myspace{V}\rightarrow\myspace{W}$ existuje lineární zobrazení
    $F: \myspace{U}\otimes\myspace{V}\rightarrow\myspace{W}$ takové, že $f(\myvec{u}, \myvec{v})
    = F(\phi(\myvec{u}, \myvec{v}))$ (jinak značeno $f = F \circ \phi$).
\item množina obrazů $\left\lbrace\phi(u, v)|u\in\myspace{U}, v\in\myspace{V}\right\rbrace$ generuje
    \footnote{Nezapomeňte, ``generuje`` znamená konečné lineární kombinace!} celý prostor
    $\myspace{U}\otimes\myspace{V}$ (tzn. každý prvek prostoru lze vyjádřit jako lineární
    kombinaci prvků množiny obrazů).
\end{enumerate}
\end{definition}

Na začátku kapitoly jsem sliboval, že tato definice bude nejvíce intuitivní a snadná. A jsem si dost
jistý, že vám teď tak nepřipadá. Proto si ji nyní rozebereme podrobněji.

Druhý bod definice je technický a bude potřeba v následných důkazech. V tento okamžik
se jím zabývat nemusíme.

Zato ten první je podstatně zajímavější. Pro přehlednost si ukážeme obrázek, který vám umožní si během
čtení ukazovat prstem, o čem zrovna mluvíme.
\begin{center}
\includesvg[width=200pt]{universal_property}
\end{center}

Bod totiž říká, že každé bilineární zobrazení můžeme \textit{linearizovat}. Pomocí tensorového
součinu dokážeme převést libovolné bilineární zobrazení na lineární zobrazení. A to se nám vyplatí,
protože s lineárním zobrazením už umíme pracovat.

Např. pokud máme prostory s konečnou bází (tzn. vektory vyjádřené jako souřadnice) dokážeme
bilineární zobrazení vyjádřit jako ``pole čísel``, stejně jako to dokážeme pro libovolné lineární
zobrazení. Pokud si vzpomínáte na motivační příklad, pracovali jsme s poli čísel vyjádřené jako
vnořované řádkové a sloupcové vektory (tzn. vektory vektorů, ve výsledku prostě pole s $n$ indexy).

Anebo lineární zobrazení dokážeme skládat dohromady. Převedením na lineární zobrazení dokážeme
skládat i bilineární zobrazení. I to jsme v motivačním příkladu dělali - např. zobrazení integrující
polynom jsme složili dohromady s tensorem násobícím dva polynomy.

Možná vás poněkud zarazilo, že v definici není použitý pojem tensor. Pouze tensorový součin.
To není opomenutí. Definice pojmu tensoru je trochu obtížná. První možnost je považovat
za tensory prvky prostoru $\myspace{U}\otimes\myspace{V}$. To zní logicky, ale nebude to
odpovídat tomu, co za tensory považují fyzici. Ti si totiž pod pojmem tensor představují
ono zmiňované ``pole čísel``, které v naší definici odpovídá lineárnímu zobrazení $F$.
Zároveň ale pole je uspořádané tak, aby neslo informace o bilineárním zobrazení $\phi$
- sami jsme v motivačním příkladu viděli, že dokážeme na tensor aplikovat přímo vektory,
tzn. nějakým skrytým způsobem se provedla transformace do $\myspace{U}\otimes\myspace{V}$.

Zároveň existují další definice, které tensory zavádějí. A v nich by pojem tensor odkazoval
zase na jiné prvky jiného prostoru. Jak je vidět, pojem tensor není úplně jednoznačný.
V následujícím textu ho budeme používat volně ve významu $\phi$ a $F$ dohromady, tzn. jako
alternativní vyjádření nějakého bilineárního (později multilineárního) zobrazení.

První bod konečně přichází s intuitivní představou, co můžeme tensory popsat. Tensor odpovídá
bilineárnímu (a později tensory rozšíříme na multilineární) zobrazení. Tzn. cokoliv, co se
chová jako nějaká forma násobení, lze vyjádřit jako tensor. Pokud jste dosud, podobně jako já,
tápali nad obecným popisem vyjadřovací schopnosti tensorů, nyní máte odpověď.

Nutno podotknout, že v tento okamžik je naše konstrukce jenom labutí píseň. Vymysleli jsme si
podmínky, abychom mohli převést libovolné bilineární zobrazení na lineární, ale není zaručené,
že vůbec existuje něco, co je dokáže splnit. A je na nás, abychom existenci takového prostoru
dokázali. Než se však do tohoto důkazu pustíme, zkusíme nějakou dobu zkoumat, co jsme si
vlastně nadefinovali.

\begin{example}
Začněme příkladem. Vezměme si vektorový prostor $\myspace{R}^2$ se standardním skalárním součinem.
Pokud pracujeme s ortonormální bází, skalární součin je daný předpisem
\begin{equation*}
\myvec{x}\cdot\myvec{y} = x_1 y_1 + x_2 y_2
\end{equation*}
Skalární součin tak tvoří bilineární zobrazení (jak se čtenář jistě dokáže sám přesvědčit),
tedy na základě předchozích úvah bychom měli být schopní ho vyjádřit jako tensor.

První problém, který musíme vyřešit je fakt, že předpis je definovaný pro ortonormální bázi.
My ale chceme vytvořit předpis pro bilineární zobrazení fungující v jakékoliv bázi.

Pro tento účel si zavedeme dva kovektory $\epsilon_1$ a $\epsilon_2$, které pro daný vektor
spočítají jeho průmět do os ortonormální báze. Zvídavý čtenář si jistě dokáže
ověřit, že se skutečně jedná o kovektory (vynásobení vektoru vynásobí průmět, součtu vektorů
odpovídá součet průmětů). Předpis tak můžeme přepsat do podoby:
\begin{equation*}
\myvec{x}\cdot\myvec{y} = \epsilon_1(\myvec{x})\epsilon_1(\myvec{y})
    + \epsilon_2(\myvec{x})\epsilon_2(\myvec{y})
\end{equation*}

Nyní si vezměme transformační matici $\mymatrix{T}$, která převede ortonormální bázi
$\myvec{e_1}$ a $\myvec{e_2}$ do libovolné jiné báze $\myvec{\widetilde{e_1}}$
a $\myvec{\widetilde{e_2}}$. Už víme, že transformační matice obsahuje po sloupcích obrazy nové
báze vyjádřené v souřadnicích té původní
\begin{equation*}
\mymatrix{T} = \left(\begin{array}{cc}
\widetilde{e_{1_1}} & \widetilde{e_{2_1}} \\
\widetilde{e_{1_2}} & \widetilde{e_{2_2}}
\end{array}\right)
\end{equation*}
Touto maticí převedeme známou (průmět vektoru do osy ortonormální báze je v ortonorální
bázi přímo konkrétní souřadnice vektoru) podobu kovektorů $\epsilon_1$ a $\epsilon_2$
v ortonormální bázi do obecné báze.
\begin{equation*}
\begin{split}
\mycoord{\epsilon_1} &= 
\left(\begin{array}{cc}1 & 0\end{array}\right)
\left(\begin{array}{cc}
\widetilde{e_{1_1}} & \widetilde{e_{2_1}} \\
\widetilde{e_{1_2}} & \widetilde{e_{2_2}}
\end{array}\right) =
\left(\begin{array}{cc}\widetilde{e_{1_1}} & \widetilde{e_{2_1}}\end{array}\right) \\
\mycoord{\epsilon_2} &= 
\left(\begin{array}{cc}0 & 1\end{array}\right)
\left(\begin{array}{cc}
\widetilde{e_{1_1}} & \widetilde{e_{2_1}} \\
\widetilde{e_{1_2}} & \widetilde{e_{2_2}}
\end{array}\right) =
\left(\begin{array}{cc}\widetilde{e_{1_2}} & \widetilde{e_{2_2}}\end{array}\right) \\
\end{split}
\end{equation*}
Nyní můžeme předpis pro skalární součin přepsat do podoby (souřadnice vektorů $\myvec{x}$
a $\myvec{y}$ jsou vůči obecné bázi $\myvec{\widetilde{e_1}}$ a $\myvec{\widetilde{e_2}}$)
\begin{equation*}
\begin{split}
\myvec{x}\cdot\myvec{y} =
\left(
  \left(\begin{array}{cc}\widetilde{e_{1_1}} & \widetilde{e_{2_1}}\end{array}\right)
  \left(\begin{array}{c}x_1 \\ x_2\end{array}\right)
\right)
\left(
  \left(\begin{array}{cc}\widetilde{e_{1_1}} & \widetilde{e_{2_1}}\end{array}\right)
  \left(\begin{array}{c}y_1 \\ y_2\end{array}\right)
\right) + \\
\left(
  \left(\begin{array}{cc}\widetilde{e_{1_2}} & \widetilde{e_{2_2}}\end{array}\right)
  \left(\begin{array}{c}x_1 \\ x_2\end{array}\right)
\right)
\left(
  \left(\begin{array}{cc}\widetilde{e_{1_2}} & \widetilde{e_{2_2}}\end{array}\right)
  \left(\begin{array}{c}y_1 \\ y_2\end{array}\right)
\right)
\end{split}
\end{equation*}
Pokud všechno roznásobíte, sečtete, přeuzávorkujete a zpětně vrátíte do maticového
součinu, získáte následující předpis
\begin{equation*}
\myvec{x}\cdot\myvec{y} =
\left(\begin{array}{cc}x_1 & x_2\end{array}\right)
\left(\begin{array}{cc}
  \widetilde{e_{1_1}}\widetilde{e_{1_1}} + \widetilde{e_{1_2}}\widetilde{e_{1_2}} &
  \widetilde{e_{1_1}}\widetilde{e_{2_1}} + \widetilde{e_{1_2}}\widetilde{e_{2_2}} \\
  \widetilde{e_{2_1}}\widetilde{e_{1_1}} + \widetilde{e_{2_2}}\widetilde{e_{1_2}} &
  \widetilde{e_{2_1}}\widetilde{e_{2_1}} + \widetilde{e_{2_2}}\widetilde{e_{2_2}}
\end{array}\right)
\left(\begin{array}{c}y_1 \\ y_2\end{array}\right)
\end{equation*}
kde jednotlivé položky prostřední matice odpovídají
\begin{equation*}
\myvec{x}\cdot\myvec{y} =
\left(\begin{array}{cc}x_1 & x_2\end{array}\right)
\left(\begin{array}{cc}
  \myvec{\widetilde{e_1}}\cdot\myvec{\widetilde{e_1}} &
  \myvec{\widetilde{e_1}}\cdot\myvec{\widetilde{e_2}} \\
  \myvec{\widetilde{e_2}}\cdot\myvec{\widetilde{e_1}} &
  \myvec{\widetilde{e_2}}\cdot\myvec{\widetilde{e_2}}
\end{array}\right)
\left(\begin{array}{c}y_1 \\ y_2\end{array}\right)
\end{equation*}

Jak toto napasovat na naši definici? Nejdříve přetransformujme vektory skrz tensorový součin
\begin{equation*}
\begin{split}
\phi(\myvec{x}, \myvec{y}) = \phi(x_1\myvec{\widetilde{e_1}} + x_2\myvec{\widetilde{e_2}},
  y_1\myvec{\widetilde{e_1}} + y_2\myvec{\widetilde{e_2}})
\end{split}
\end{equation*}
Z vlastností bilineárního zobrazení zápis můžeme přepsat jako
\begin{equation*}
\begin{split}
\phi(\myvec{x}, \myvec{y}) =
  x_1 y_1 \phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_1}})
  + x_1 y_2 \phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_2}})
  + x_2 y_1 \phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_1}})
  + x_2 y_2 \phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_2}})
\end{split}
\end{equation*}
Teď potřebujeme nalézt lineární zobrazení $F$ takové, aby výsledek vždy byl skalární
součin $\myvec{x}\cdot\myvec{y}$. Pokud porovnáme výše uvedený součin matic, získáme
zobrazení
\begin{equation*}
\begin{split}
F(\phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_1}})) &=
    \myvec{\widetilde{e_1}}\cdot\myvec{\widetilde{e_1}} \\
F(\phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_2}})) &=
    \myvec{\widetilde{e_1}}\cdot\myvec{\widetilde{e_2}} \\
F(\phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_1}})) &=
    \myvec{\widetilde{e_2}}\cdot\myvec{\widetilde{e_1}} \\
F(\phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_2}})) &=
    \myvec{\widetilde{e_2}}\cdot\myvec{\widetilde{e_2}} \\
\end{split}
\end{equation*}
tedy
\begin{equation*}
F(\alpha_{11}\phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_1}}) + \cdots 
  + \alpha_{22}\phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_2}})) 
= \alpha_{11}\myvec{\widetilde{e_1}}\cdot\myvec{\widetilde{e_1}} + \cdots
  + \alpha_{22}\myvec{\widetilde{e_2}}\cdot\myvec{\widetilde{e_2}}
\end{equation*}
Pokud oprávněně váháte, zda $F$ je skutečně lineární zobrazení, vězte, že
$\phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_1}})$, 
$\phi(\myvec{\widetilde{e_1}}, \myvec{\widetilde{e_2}})$,
$\phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_1}})$ a
$\phi(\myvec{\widetilde{e_2}}, \myvec{\widetilde{e_2}})$ tvoří bázi prostoru
$\myspace{U}\otimes\myspace{W}$, což si časem dokážeme. A obrazy báze jednoznačně
určují celé lineární zobrazení.

\medskip\noindent
Pochopili jste příklad? Pokud ano, udělejte si pašáka. Právě jste stvořili
\textbf{metrický tensor}, který je základem obecné teorie relativity - samozřejmě
ale ve 4-rozměrném prostoročase.
\end{example}

\noindent
Podle naší definice pro každé bilineární zobrazení existuje jeho linearizace. Můžeme si ale položit
opačnou otázku: je každé lineární zobrazení $\myspace{U}\otimes\myspace{V}\rightarrow\myspace{W}$
linearizací nějakého bilineárního zobrazení $\myspace{U}\times\myspace{V}\rightarrow\myspace{W}$?

\begin{lemma}
\label{lemma:lin_to_bilin}
Mějme vektorové prostory $\myspace{U}, \myspace{V}, \myspace{W}$ nad stejným tělesem $\myspace{T}$
a tensorový součin $\phi: \myspace{U}\times\myspace{V}\rightarrow\myspace{U}\otimes\myspace{V}$.
Pak každé lineární zobrazení $F: \myspace{U}\otimes\myspace{V}\rightarrow\myspace{W}$ je linearizací
nějakého bilineárního zobrazení $f: \myspace{U}\times\myspace{V}\rightarrow\myspace{W}$,
tedy $\exists f \in \mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{W})$ takové, že
$f(\myvec{u}, \myvec{v}) = F(\phi(\myvec{u}, \myvec{v}))$.
\end{lemma}

\begin{proof}
Důkaz tvrzení je snadný. Nejdříve zkonstruujeme zobrazení $f$ očekávatelným předpisem
$f(\myvec{u}, \myvec{v}) = F(\phi(\myvec{u}, \myvec{v}))$. A pokud prokážeme, že toto zobrazení
je bilineární, pak máme vyhráno. A ono jednoduchou aplikací vlastností lineárního zobrazení
$F$ a bilineárního zobrazení $\phi$ je:
\begin{equation*}
\begin{split}
f(\myvec{u_1} + \myvec{u_2}, \myvec{v}) = F(\phi(\myvec{u_1} + \myvec{u_2}, \myvec{v}))
  = F(\phi(\myvec{u_1}, \myvec{v}) + \phi(\myvec{u_2}, \myvec{v})) \\
= F(\phi(\myvec{u_1}, \myvec{v})) + F(\phi(\myvec{u_2}, \myvec{v}))
  = f(\myvec{u_1}, \myvec{v}) + f(\myvec{u_2}, \myvec{v})
\end{split}
\end{equation*}
a násobení skalárem
\begin{equation*}
\begin{split}
f(\alpha\myvec{u}, \myvec{v}) = F(\phi(\alpha\myvec{u}, \myvec{v}))
= F(\alpha\phi(\myvec{u}, \myvec{v}))
= \alpha F(\phi(\myvec{u}, \myvec{v})) \\
= \alpha f(\myvec{u}, \myvec{v})
\end{split}
\end{equation*}
Podobný postup můžeme zopakovat i pro parametr $\myvec{v}$.
\end{proof}

\section{Vlastnosti tensorového součinu}

\noindent
V této kapitole rozebereme charakteristické vlastnosti tensorového součinu. Začneme tím, že si rozšíříme
značení. V naší definici jsme bilineární zobrazení nazvané tensorový součin značili řeckým písmenkem
$\phi$. Toto značení budeme i nadále používat v situacích, kdy budeme chtít zdůraznit bilinearitu
tensorového součinu. Běžnější značení v učebnicích a dále i v této vypadá takto: $\phi(\myvec{u}, \myvec{v}) = \myvec{u}\otimes\myvec{v}$.

Toto nové značení ovšem může působit poněkud zavádějícím způsobem. Začátečník má tendendenci předpokládat,
že každý prvek prostoru $\myspace{U}\otimes\myspace{V}$ lze vyjádřit jako $\myvec{u}\otimes\myvec{v}$.
\textbf{Tato představa, jakkoliv působí přirozeně, je mylná!} Tensorový součin, tedy zobrazení $\phi$
v naší definici, \textbf{není surjektivní} (není \textit{na}). Toto tvrzení se dá snadno nahlédnout,
pokud si vezmu dva páry lineárně nezávislých vektorů $\myvec{u_1}, \myvec{u_2}$ a $\myvec{v_1}, \myvec{v_2}$
a sečtu jejich tensorové součiny: $\myvec{u_1}\otimes\myvec{v_1} + \myvec{u_2}\otimes\myvec{v_2}$.
Z definice bilineárního zobrazení takto vzniklý prvek prostoru není obrazem žádného prvku z
$\myspace{U}\times\myspace{V}$ - nelze ho zapsat jako součet v jednom z parametrů.

\medskip\noindent
V rozboru vlastností tensorového součinu začneme jednoduchým tvrzením, které říká, že všechny možné
tensorové součiny jsou vlastně stejné - existuje mezi nimi přirozený isomorfismus. Jinými slovy
můžeme tensorový součin považovat za jednoznačný.

\begin{lemma}
Mějme dva vektorové prostory $\myspace{U}$ a $\myspace{V}$ nad tělesem $\myspace{T}$. K nim dva
různé tensorové součiny $\phi_1: \myspace{U}\times\myspace{V}\rightarrow\myspace{U}\otimes_1\myspace{V}$
a $\phi_2: \myspace{U}\times\myspace{V}\rightarrow\myspace{U}\otimes_2\myspace{V}$. Pak existuje
přirozený isomorfismus $\myspace{U}\otimes_1\myspace{V}\rightarrow\myspace{U}\otimes_2\myspace{V}$,
tedy $\myspace{U}\otimes_1\myspace{V}\cong\myspace{U}\otimes_2\myspace{V}$.
\end{lemma}

\begin{proof}
Abychom tvrzení dokázali, jednoduše aplikujeme definici (v jiných textech se to označuje jako
použití univerzální vlastnosti):
\begin{center}
\includesvg[width=200pt]{tensor_product_isomorphism}
\end{center}
Tensorový součin $\phi_1$ je z definice bilineární zobrazení, proto pro něj, také z definice, musí
existovat linearizace pomocí tensorového součinu $\phi_2$. Stejná úvaha platí i pro tensorový součin
$\phi_2$. Získáváme tak rovnice
\begin{equation*}
\begin{split}
\phi_1(\myvec{u}, \myvec{v}) &= F_1(\phi_2(\myvec{u}, \myvec{v})) \\
\phi_2(\myvec{u}, \myvec{v}) &= F_2(\phi_1(\myvec{u}, \myvec{v}))
\end{split}
\end{equation*}
Pokud druhou rovnici dosadíme do první, získáme
\begin{equation*}
\phi_1(\myvec{u}, \myvec{v}) = F_1(F_2(\phi_1(\myvec{u}, \myvec{v})))
\end{equation*}
Je tedy zřejmé, že kombinace zobrazení $F_1 \circ F_2$ musí být identita. Nejásejte, je tu samozřejmě
zádrhel. Víme, že funguje jako identita pouze na obrazech tensorového součinu $\phi_1$
(tzn. na těch prvcích $\myspace{U}\otimes_1\myspace{V}$, které lze zapsat jako
$\myvec{u}\otimes_1\myvec{v}$). Pro důkaz tvrzení potřebujeme dokázat, že funguje jako identita
pro celý prostor $\myspace{U}\otimes_1\myspace{V}$.

Zde do hry vstupuje druhá podmínka definice, kterou jsme prozatím ignorovali. Ta říká, že obrazy
tensorového součinu generují celý prostor. Tedy, každý prvek $\myvec{\chi}\in\myspace{U}\otimes_1\myspace{V}$
lze zapsat jako lineární kombinaci obrazů tensorového součinu
\begin{equation*}
\myvec{\chi} =  \sum_{i}\alpha_i\myvec{u_i}\otimes\myvec{v_i}
\end{equation*}
Pokud takto rozepsaný prvek dosadíme do kombinace zobrazení $F_1 \circ F_2$ a použijeme vlastnosti
lineárního zobrazení ($F_1$ a $F_2$ jsou z definice lineární), získáme
\begin{equation*}
\begin{split}
F_1(F_2(\myvec{\chi})) &= F_1(F_2(\sum_{i}\alpha_i\myvec{u_i}\otimes\myvec{v_i}))
  = F_1(\sum_{i}\alpha_i F_2(\myvec{u_i}\otimes\myvec{v_i})) \\
  &= \sum_{i}\alpha_i F_1(F_2(\myvec{u_i}\otimes\myvec{v_i}))
  = \sum_{i}\alpha_i\myvec{u_i}\otimes\myvec{v_i} = \myvec{\chi}
\end{split}
\end{equation*}
(Na konci jsme využili fakt, že $F_1 \circ F_2$ funguje jako identita na obrazech $\phi_1$.)
Lineární zobrazení $F_1 \circ F_2$ je tedy identitou na celém prostoru
$\myspace{U}\otimes_1\myspace{V}$.

Aby zobrazení $F_1 \circ F_2$ bylo identitou, musí být zobrazení $F_2$ prosté - pokud by nebylo,
dva různé prvky by se mapovaly na stejný obraz a zobrazení $F_1$ by je z definice zobrazení
převedlo zpět na stejný prvek.

Aby zobrazení $F_1 \circ F_2$ bylo identitou, musí být zobrazní $F_1$ \textit{na}. Pokud by nebylo,
existoval by prvek, který není jejím obrazem. Pokud bychom ho dosadili do $F_1 \circ F_2$, nedostali
bychom identický prvek.

Získali jsme tak závěr, že $F_1$ je surjektivní (\textit{na}) a $F_2$ je injektivní (\textit{prosté}).

Nyní zopakujeme celý postup znovu, akorát na začátku dosadíme první rovnici do druhé. Stejnou
úvahou získáme, že $F_2 \circ F_1$ je také identita, $F_1$ je injektivní a $F_2$ je surjektivní.
Obě lineární zobrazení tedy jsou bijektivní, a tím jsme našli námi hledaný přirozený isomorfismus
- dokonce rovnou v obou směrech.

\end{proof}

\medskip\noindent
Velmi podobným postupem dokážeme \textbf{komutativitu} tensorového součinu, jinými slovy že
prostor $\myspace{U}\otimes\myspace{V}$ je přirozeně isomorfní s prostorem
$\myspace{V}\otimes\myspace{U}$. To je praktická vlastnost, která nám v budoucnu umožní měnit
pořadí indexů pro tensory v prostorech s konečnou bází.

\begin{lemma}
Mějme dva vektorové prostory $\myspace{U}$ a $\myspace{V}$ nad tělesem $\myspace{T}$. K nim dva
tensorové součiny $\phi_1: \myspace{U}\times\myspace{V}\rightarrow\myspace{U}\otimes\myspace{V}$
a $\phi_2: \myspace{V}\times\myspace{U}\rightarrow\myspace{V}\otimes\myspace{U}$. Pak existuje
přirozený isomorfismus $\myspace{U}\otimes\myspace{V}\rightarrow\myspace{V}\otimes\myspace{U}$,
tedy $\myspace{U}\otimes\myspace{V}\cong\myspace{V}\otimes\myspace{U}$.
\end{lemma}

\begin{proof}
Schéma, podle kterého důkaz povedeme tentokrát, je na následujícím obrázku:
\begin{center}
\includesvg[width=300pt]{tensor_product_comutativity}
\end{center}
Zobrazení $f$ a $h$ nadefinujeme následovně:
\begin{equation*}
\begin{split}
f(\myvec{u}, \myvec{v}) &= \phi_2(\myvec{v}, \myvec{u}) \\
h(\myvec{v}, \myvec{u}) &= \phi_1(\myvec{u}, \myvec{v})
\end{split}
\end{equation*}
Protože zobrazení $\phi_1$ a $\phi_2$ jsou bilineární, i zobrazení $f$ a $h$ jsou zřejmě
bilineární (je triviální si ukázat, že podmínky bilineárního zobrazení stále platí, i když
jsme otočili argumenty). Tedy podle definice tensorového součinu musí pro obě zobrazení
existovat linearizace
\begin{equation*}
\begin{split}
f(\myvec{u}, \myvec{v}) &= F(\phi_1(\myvec{u}, \myvec{v})) \\
h(\myvec{v}, \myvec{u}) &= H(\phi_2(\myvec{v}, \myvec{u})
\end{split}
\end{equation*}
Pokud použiji definici funkcí $f$ a $h$ a dosadím druhou rovnici do první rovnice, získám
\begin{equation*}
\begin{split}
\phi_2(\myvec{v}, \myvec{u}) = f(\myvec{u}, \myvec{v}) = F(\phi_1(\myvec{u}, \myvec{v})) 
  = F(h(\myvec{v}, \myvec{u})) = F(H(\phi_2(\myvec{v}, \myvec{u}))
\end{split}
\end{equation*}
Pokud máte silný pocit deja vu, výjimečně to není chyba Matrixu. Dostali jsme se do stejné
situace, jako v předchozím důkazu o jednoznačnosti tensorového součinu. Stejným postupem
tedy odvodíme, že $F \circ H$ je identita, a že $H$ je injektivní (prosté) a $F$ surjektivní
(na).

A úplně stejně pak můžeme dosadit první rovnici do druhé
\begin{equation*}
\begin{split}
\phi_1(\myvec{u}, \myvec{v}) = h(\myvec{v}, \myvec{u}) = H(\phi_2(\myvec{v}, \myvec{u}))
  = H(f(\myvec{u}, \myvec{v})) = H(F(\phi_1(\myvec{u}, \myvec{v}))
\end{split}
\end{equation*}
a tím dojít k závěru, že i $H \circ F$ je identita a $H$ je surjektivní (na) a $F$ je
injektivní (prosté).

Tedy zobrazení $F$ a $H$ jsou bijekce a náš kýžený přirozený isomorfismus.
\end{proof}

\medskip\noindent
Postup se nám viditelně osvědčil, proto ho použijeme znovu. Tentokrát pro důkaz asociativity
tensorového součinu. Asociativita je podstatná vlastnost, protože nám umožní rozšířit tensory
z bilineárních zobrazení na obecně multilineární.

\begin{lemma}
Mějme vektorové prostory $\myspace{U}$, $\myspace{V}$ a $\myspace{W}$ nad tělesem $\myspace{T}$.
K nim tensorové součiny
$\phi_1: \myspace{U}\times(\myspace{V}\otimes\myspace{W})\rightarrow
  \myspace{U}\otimes(\myspace{V}\otimes\myspace{W})$
a $\phi_2: (\myspace{U}\otimes\myspace{V})\times\myspace{W}\rightarrow
  (\myspace{U}\otimes\myspace{V})\otimes\myspace{W}$. Pak existuje přirozený isomorfismus
$\myspace{U}\otimes(\myspace{V}\otimes\myspace{W})\rightarrow
  (\myspace{U}\otimes\myspace{V})\otimes\myspace{W}$,
tedy $\myspace{U}\otimes(\myspace{V}\otimes\myspace{W})\cong
  (\myspace{U}\otimes\myspace{V})\otimes\myspace{W}$.
\end{lemma}

Jinými slovy můžeme zapomenout na závorky, protože prostory jsou až na isomorfismus stejné.

\begin{proof}
I zde použijeme podobné schéma
\begin{center}
\includesvg[width=300pt]{tensor_product_asociativity}
\end{center}
Tentokrát konstrukce zobrazení $f$ a $h$ nebude tak triviální, jako v předchozím případě.
Jako první myšlenka by se nabízela možnost
\begin{equation*}
f(\myvec{u}, \myvec{v}\otimes\myvec{w}) = \phi_2(\myvec{u}\otimes\myvec{v}, \myvec{w})
\end{equation*}
která ale nemůže fungovat. Jak jsme si již řekli, ne každý prvek prostoru $\myspace{V}\otimes\myspace{W}$
lze napsat jako $\myvec{v}\otimes\myvec{w}$, proto ani není možné ho rozložit na vektory
a použít je na druhé straně rovnice. Předchozí zápis proto funguje pouze na obrazech
tensorového součinu $\myspace{V}\otimes\myspace{W}$.

Nezbývá nám než opět využít druhý bod definice tensorového součinu, který říká, že obrazy
generují celý prostor a zobrazení nadefinovat takto
\begin{equation*}
f(\myvec{u}, \myvec{\chi}) = f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
  = \sum_{i}\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i})
\end{equation*}
Abychom toto zobrazení mohli použít, potřebujeme dokázat, že je bilineární. V opačném
případě bychom nemohli použít univerzální vlastnost a nedostali bychom se k hledanému
isomorfismu. Začněme tedy dokazovat jednotlivé axiomy bilineárního zobrazení.

\begin{equation*}
\begin{split}
f(\myvec{u_1} + \myvec{u_2}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
  = \sum_{i}\alpha_i\phi_2((\myvec{u_1} + \myvec{u_2})\otimes\myvec{v_i}, \myvec{w_i})
\end{split}
\end{equation*}
S využitím vlastností bilineárních zobrazení je možné rovnici přepsat
\begin{equation*}
\begin{split}
f(\myvec{u_1} + \myvec{u_2}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
= \sum_{i}\alpha_i\phi_2(\myvec{u_1}\otimes\myvec{v_i} + \myvec{u_2}\otimes\myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\left(\alpha_i\phi_2(\myvec{u_1}\otimes\myvec{v_i}, \myvec{w_i}) 
   + \alpha_i\phi_2(\myvec{u_2}\otimes\myvec{v_i}, \myvec{w_i})\right) \\
= f(\myvec{u_1}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
   + f(\myvec{u_2}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
\end{split}
\end{equation*}

\noindent
Druhý axiom vyžaduje být lineární v druhém argumentu. Zde je výsledek jednoduše daný asociativitou
a komutativitou sčítání - součet dvou sum můžeme nahradit jednou sumou, která postupně sčítá prvky
přes indexy $i$, následně přes indexy $j$. Tuhle sumu použít v definici zobrazení $f$ a následně
ji zase rozdělit na dvě sumy\footnote{Lineární kombinace jsou konečné, proto není žádný problém
tento postup udělat.}.
\begin{equation*}
\begin{split}
f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i} + \sum_{j}\alpha_j\myvec{v_j}\otimes\myvec{w_j})
= f(\myvec{u}, \sum_{k=i,j}\alpha_k\myvec{v_k}\otimes\myvec{w_k}) \\
= \sum_{k=i,j}\alpha_k\phi_2(\myvec{u}\otimes\myvec{v_k}, \myvec{w_k})
= \sum_{i}\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i})
    + \sum_{j}\alpha_j\phi_2(\myvec{u}\otimes\myvec{v_j}, \myvec{w_j}) \\
= f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
    + f(\myvec{u}, \sum_{j}\alpha_j\myvec{v_j}\otimes\myvec{w_j})
\end{split}
\end{equation*}

\noindent
Jako poslední vyřešíme násobení skalárem. Z definice a vlastností bilineárního zobrazení dostáváme
\begin{equation*}
\begin{split}
\beta f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
= \beta \sum_{i}\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\beta\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i})
= \sum_{i}\alpha_i\phi_2(\beta\myvec{u}\otimes\myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\alpha_i\phi_2(\myvec{\beta u}\otimes\myvec{v_i}, \myvec{w_i})
= f(\beta\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
\end{split}
\end{equation*}
a podobně
\begin{equation*}
\begin{split}
\beta f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
= \beta \sum_{i}\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\beta\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i})
= f(\myvec{u}, \sum_{i}\beta\alpha_i\myvec{v_i}\otimes\myvec{w_i}) \\
= f(\myvec{u}, \beta\sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})
\end{split}
\end{equation*}
tedy námi nadefinované zobrazení $f$ je bilineární. Podobným způsobem můžeme nadefinovat
bilineární zobrazení $h$ jako
\begin{equation*}
h(\myvec{\theta}, \myvec{w}) = h(\sum_{i}\alpha_i\myvec{u_i}\otimes\myvec{v_i}, \myvec{w}) 
= \sum_{i}\alpha_i\phi_1(\myvec{u_i}, \myvec{v_i}\otimes\myvec{w})
\end{equation*}

\medskip\noindent
A nyní už nám nic nebrání použít univerzální vlastnost a zopakovat osvědčený postup.
\begin{equation*}
\begin{split}
f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
  = F(\phi_1(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})) \\
  = \sum_{i}\alpha_i F(\phi_1(\myvec{u}, \myvec{v_i}\otimes\myvec{w_i}))
\end{split}
\end{equation*}
V prvním kroku jsme použili univerzální vlastnost, ve druhém jsme sumu pomocí
vlastnostní bilineárního a lineárního zobrazení dostali ven. Nyní za $\phi_1$
dosadíme zobrazení $h$. Protože zde dosazujeme přímo obrazy tensorového součinu,
suma v zobrazení $h$ obsahuje pouze jeden sčítanec a můžeme ji zapomenout.
\begin{equation*}
\begin{split}
f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
= \sum_{i}\alpha_i F(h(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i}))
\end{split}
\end{equation*}
Po opětovném využití univerzální vlastnosti dostáváme:
\begin{equation*}
\begin{split}
f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
= \sum_{i}\alpha_i F(H(\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i})))
\end{split}
\end{equation*}
a s využitím vlastností lineárních zobrazení vrátíme sumu zpět dovnitř a dostaneme
se zpět k definici funkce $f$
\begin{equation*}
\begin{split}
f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i}) 
= F(H(\sum_{i}\alpha_i\phi_2(\myvec{u}\otimes\myvec{v_i}, \myvec{w_i}))) \\
= F(H(f(\myvec{u}, \sum_{i}\alpha_i\myvec{v_i}\otimes\myvec{w_i})))
\end{split}
\end{equation*}
Na základě stejných úvah jako v předchozích důkazech je zobrazení $F \circ H$ identita.
A stejně tak můžeme postup zopakovat v opačném směru, čímž získám, že $H \circ F$
je identita. A opět kombinací těchto dvou faktů dostáváme, že $F$ i $H$ jsou bijektivní,
tudíž námi hledaný isomorfismus.
\end{proof}

\section{Alternativní definice}

\noindent
K definici tensorového součinu \ref{def:tensor_product_1} existuje ekvivalentní alternativa:
\begin{definition}
\label{def:tensor_product_2}
Mějme vektorové prostory $\myspace{U}$, $\myspace{V}$ nad tělesem $\myspace{T}$ a bilineární zobrazení
$\phi$ z $\myspace{U}\times\myspace{V}$ do vektorového prostoru značeného jako
$\myspace{U}\otimes\myspace{V}$. Bilineární zobrazení $\phi$ budeme nazývat \textit{tensorový součin}
a dvojici $(\phi, \myspace{U}\otimes\myspace{V})$ jako \textit{tensorový součin prostorů} $\myspace{U}$
a $\myspace{V}$ právě tehdy pokud platí:
\begin{enumerate}	
\item Pro libovolný vektorový prostor $\myspace{W}$ nad tělesem $\myspace{T}$ a libovolné bilineární
	zobrazení $f: \myspace{U}\times\myspace{V}\rightarrow\myspace{W}$ existuje lineární zobrazení
	$F: \myspace{U}\otimes\myspace{V}\rightarrow\myspace{W}$ takové, že $f(\myvec{u}, \myvec{v})
	= F(\phi(\myvec{u}, \myvec{v}))$ (jinak značeno $f = F \circ \phi$).
\item Zobrazení $F$ je jednoznačné, tzn. existuje právě jedno.
\end{enumerate}
\end{definition}
Jak čtenář může vidět, definice se liší pouze v druhé podmínce - potřebu generovat celý prostor
nahradila existence právě jednoho lineárního zobrazení. Tyto dvě definice jsou ekvivalentní, což
si dokážeme v následujícím důkazu.

\begin{proof}
Důkaz povedeme tradičně, že dokážeme oba směry ekvivalence. Začněmš jednodušším směrem
``generuje $\Rightarrow$ jednoznačná``:

\medskip\noindent
Důkaz povedeme sporem. Předpokládejme tedy, že lineární zobrazení není jednoznačné,
tzn. existují $F_1 \neq F_2$ taková, že
$f(\myvec{u}, \myvec{v}) = F_1(\phi(\myvec{u}, \myvec{v})) = F_2(\phi(\myvec{u}, \myvec{v}))$.
Pak z vlastnosti, že množina obrazů generuje celý prostor, pro libovolný prvek $\myvec{\chi} = \myspace{U}\otimes\myspace{V}$ můžu psát:
\begin{equation*}
\begin{split}
F_1(\myvec{\chi}) = F_1(\sum_{i}\alpha_i\myvec{u_i}\otimes\myvec{v_i})
= \sum_{i}\alpha_i F_1(\myvec{u_i}\otimes\myvec{v_i}) \\
= \sum_{i}\alpha_i F_2(\myvec{u_i}\otimes\myvec{v_i})
= F_2(\sum_{i}\alpha_i\myvec{u_i}\otimes\myvec{v_i}) = F_2(\myvec{\chi})
\end{split}
\end{equation*}
(sumu můžeme dostat ven pomocí vlastností lineárního zobrazení, použít předpoklad, a následně
sumu vrátit zpět dovnitř). Dosáhli jsme sporu s předpokladem, že zobrazení není jednoznačné.
Tedy zobrazení jednoznačné je. Což není nic objevného - lineární zobrazení je jednoznačně
definované obrazy báze, nebo v tomto případě obrazy generující množiny.

\medskip\noindent
Směr ``jednoznačná $\Rightarrow$ generuje`` je náročnější. Opět důkaz povedeme sporem, a budeme
předpokládat, že množina obrazů tensorového součinu negeneruje celý prostor
$\myspace{U}\otimes\myspace{V}$. To znamená, že množina obrazů tensorového součinu generuje
podprostor $\myspace{G}\subset\myspace{U}\otimes\myspace{V}$. Do toho podprostoru nadefinujeme
zobrazení $\myspace{U}\times\myspace{V}\rightarrow\myspace{G}$ jako
$\phi'(\myvec{u}, \myvec{v}) = \phi(\myvec{u}, \myvec{v})$.
\begin{center}
\includesvg[width=250pt]{alternative_def}
\end{center}
Toto zobrazení je zjevně bilineární, protože tensorový součin $\phi$ je z definice bilineární.
Proto pro něj existuje linearizace $\phi'(\myvec{u}, \myvec{v}) = P(\phi(\myvec{u}, \myvec{v}))$.

Nyní vytvořme nové zobrazení $i$ zpět do prostoru $\myspace{U}\otimes\myspace{V}$, které každému
prvku v $\myspace{G}$ přiřadí stejný prvek v prostoru $\myspace{U}\otimes\myspace{V}$
$i(\myvec{\chi}) = \myvec{\chi}$ ($\myspace{G}$ je podprostor, proto takové zobrazení je možné).

A teď přichází další trik - položíme tensorový součin jako naše obecné bilineární zobrazení
$f$ v definici. Tensorový součin je sám o sobě bilineární zobrazení, proto pro něj musí existovat
linearizace $I$, pro kterou platí $\phi(\myvec{u}, \myvec{v}) = I(\phi(\myvec{u}, \myvec{v}))$.
Zcela zjevně takovým zobrazením je identita.

Nyní tedy máme dvě zobrazení, které fungují jako linearizace tensorového součinu $\phi$ - identitu
$I$ a složené zobrazení $i \circ P$:
\begin{itemize}
\item $\phi(\myvec{u}, \myvec{v}) = \phi'(\myvec{u}, \myvec{v}) = i(\phi'(\myvec{u}, \myvec{v}))
    = i(P(\phi(\myvec{u}, \myvec{v})))$
\item zobrazení $i$ a $P$ jsou obě lineární (první triviálně, druhé z definice tensorového součinu),
    proto jejich složení je také lineární.
\end{itemize}
Protože ale podle předpokladu je linearizace jednoznačná, tato dvě zobrazení musí být stejná,
tedy $I = i \circ P$. A protože identita $I$ je triviálně surjektivní (\textit{na}), musí být
surjektivní i zobrazení $i$. A protože je zároveň triviálně injektivní (\textit{prosté}), zobrazení
$i$ je bijektivní, a zároveň identita (přiřazuje prvku stejný prvek), prostory $\myspace{G}$
a $\myspace{U}\otimes\myspace{V}$ musí být stejné. Tedy množina obrazů tensorového součinu generuje
celý prostor $\myspace{U}\otimes\myspace{V}$, což je spor s naším předpokladem.
\end{proof}

\noindent
Přímým důsledkem této alternativní definice a lemma \ref{lemma:lin_to_bilin} je následující
tvrzení

\begin{lemma}
Mějme vektorové prostory $\myspace{U}, \myspace{V}, \myspace{W}$ nad stejným tělesem $\myspace{T}$.
Pak vektorový prostor bilineárních zobrazení
$\mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{W})$
je isomorfní s vektorovým prostorem lineárních zobrazení
$\mathcal{L}(\myspace{U}\otimes\myspace{V}\rightarrow\myspace{W})$, tzn.
$\mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{W}) \cong
\mathcal{L}(\myspace{U}\otimes\myspace{V}\rightarrow\myspace{W})$
\end{lemma}

\begin{proof}
Důkaz tvrzení je prostý. Zkonstruujeme zobrazení $\Psi: \mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{W}) \rightarrow
\mathcal{L}(\myspace{U}\otimes\myspace{V}\rightarrow\myspace{W})$ tak, že každému
bilineárnímu zobrazení přiřadíme jeho linearizaci $\Psi(f) = F$. Protože linearizace
je právě jedna, je konstrukce zobrazení v pořádku. Protože zobrazení $f$ i $F$ musí vracet
stejné hodnoty $f(\myvec{u}, \myvec{v}) = F(\myvec{u}\otimes\myvec{v})$, zobrazení $\Psi$
musí být injektivní (\textit{prosté}). A protože podle lemma \ref{lemma:lin_to_bilin}
je každé lineární zobrazení $F$ linearizací nějakého bilineárního zobrazení $f$, pak zobrazení
$\Psi$ musí být i surjektivní (\textit{na}). Máme tedy bijektivní zobrazení. Pokud je zároveň
lineární, pak jsme nalezli hledaný isomorfismus. A lineární je triviálně opět z faktu,
že zobrazení $f$ a $F$ vrací stejné hodnoty $f(\myvec{u}, \myvec{v}) = F(\myvec{u}\otimes\myvec{v})$.
\end{proof}

\section{Multilinearita}

\noindent
Až doteď jsme pracovali pouze s bilineárním zobrazením a jeho linearizací. Nicméně lemma dokazující, že
tensorový součin je až na isomorfismus asociativní, naznačuje, že není důvod se omezovat pouze zobrazení
o dvou parametrech.

V textech, které jsem četl, se rozšíření na multilinearitu provedlo extensí definic a technickou
úpravou vět pro více proměnných. Což je bezpochyby správný postup, a možná i správnější, než o co
se pokusím já zde - ukážu, že pomocí (bilineárního) tensorového součinu, jak jsme si ho nadefinovali,
je možné libovolné multilineární zobrazení jednoznačně zredukovat na bilineární. Tedy ve velkém závěru
kapitoly nám bude stačit dokázat, že tensorový součin existuje pro bilineární zobrazení.

Vezměme tedy multilineární zobrazení $f(\myvec{u}, \myvec{v}, \myvec{w}):
\myspace{U}\times\myspace{V}\times\myspace{W}\rightarrow\myspace{X}$, kde všechny vektorové prostory
jsou nad stejným tělesem.

Nyní si zvolme libovolný vektor $\myvec{u}$ a zobrazení pro něj ``zafixujme``:
\begin{equation*}
f(\myvec{u}, \myvec{v}, \myvec{w}) = g_{\myvec{u}}(\myvec{v}, \myvec{w})
\end{equation*}
Z multilinearity zobrazení $f$ přímo vyplývá, že zobrazení $g_{\myvec{u}}$ je bilineární (čtenář
si sám jistě dokáže ověřit, že všechny 3 axiomy platí). To znamená, že pro něj existuje jednoznačná
linearizace
\begin{equation*}
g_{\myvec{u}}(\myvec{v}, \myvec{w}) = G_{\myvec{u}}(\phi(\myvec{v}, \myvec{w}))
\end{equation*}
Nadefinujme si tedy nové zobrazení
\begin{equation*}
f'(\myvec{u}, \myvec{\chi}) = G_{\myvec{u}}(\myvec{\chi}))
\end{equation*}
kde $\myvec{\chi}\in\myspace{V}\otimes\myspace{W}$. Pokud dokážeme, že toto zobrazení je bilineární,
našli jsme hledanou redukci původního zobrazení $f$, protože z předchozích kroků platí
\begin{equation*}
f(\myvec{u}, \myvec{v}, \myvec{w}) = G_{\myvec{u}}(\phi(\myvec{v}, \myvec{w})) 
  = f'(\myvec{u}, \phi(\myvec{v}, \myvec{w})) = f'(\myvec{u}, \myvec{v}\otimes\myvec{w})
\end{equation*}

Prokázat linearitu v parametru $\myvec{\chi}$ je snadné, vyplývá přímo z vlastností lineárního
zobrazení $G_{\myvec{u}}$. Linearita v parametru $\myvec{u}$ je komplikovanější - zde se budeme
muset opřít o vlastnosti původního multilineárního zobrazení $f$ a druhou podmínku definice
tensorového součinu, která říká, že obrazy tensorového součinu generují celý prostor:
\begin{equation*}
\begin{split}
f'(\myvec{u_1}+\myvec{u_2}, \myvec{\chi}) 
  = f'(\myvec{u_1}+\myvec{u_2}, \sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i})) \\
= G_{\myvec{u_1} + \myvec{u_2}}(\sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i}))
  = \sum_{i}\alpha_i G_{\myvec{u_1} + \myvec{u_2}}(\phi(\myvec{v_i}, \myvec{w_i})) \\
= \sum_{i}\alpha_i g_{\myvec{u_1} + \myvec{u_2}}(\myvec{v_i}, \myvec{w_i})
  = \sum_{i}\alpha_i f(\myvec{u_1} + \myvec{u_2}, \myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\alpha_i (f(\myvec{u_1}, \myvec{v_i}, \myvec{w_i}) + f(\myvec{u_2}, \myvec{v_i}, \myvec{w_i})) \\
= \sum_{i}\alpha_i f(\myvec{u_1}, \myvec{v_i}, \myvec{w_i}) 
  + \sum_{i}\alpha_i f(\myvec{u_2}, \myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\alpha_i g_{\myvec{u_1}}(\myvec{v_i}, \myvec{w_i}) 
  + \sum_{i}\alpha_i g_{\myvec{u_2}}(\myvec{v_i}, \myvec{w_i}) \\
= \sum_{i}\alpha_i G_{\myvec{u_1}}(\phi(\myvec{v_i}, \myvec{w_i})) 
  + \sum_{i}\alpha_i G_{\myvec{u_2}}(\phi(\myvec{v_i}, \myvec{w_i})) \\
= G_{\myvec{u_1}}(\sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i})) 
  + G_{\myvec{u_2}}(\sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i})) \\
= f'(\myvec{u_1}, \myvec{\chi}) + f'(\myvec{u_2}, \myvec{\chi})
\end{split}
\end{equation*}
a podobný veletoč symbolů a písmenek můžeme provést i pro násobení skalárem:
\begin{equation*}
\begin{split}
f'(\beta\myvec{u}, \myvec{\chi}) 
= f'(\beta\myvec{u}, \sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i}))
= G_{\beta\myvec{u}}(\sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i})) \\
= \sum_{i}\alpha_i G_{\beta\myvec{u}}(\phi(\myvec{v_i}, \myvec{w_i}))
= \sum_{i}\alpha_i g_{\beta\myvec{u}}(\myvec{v_i}, \myvec{w_i})
= \sum_{i}\alpha_i f(\beta\myvec{u}, \myvec{v_i}, \myvec{w_i}) \\
= \beta\sum_{i}\alpha_i f(\myvec{u}, \myvec{v_i}, \myvec{w_i})
= \beta\sum_{i}\alpha_i g_{\myvec{u}}(\myvec{v_i}, \myvec{w_i})
= \beta\sum_{i}\alpha_i G_{\myvec{u}}(\phi(\myvec{v_i}, \myvec{w_i})) \\
= \beta G_{\myvec{u}}(\sum_{i}\alpha_i\phi(\myvec{v_i}, \myvec{w_i}))
= \beta f'(\myvec{u}, \myvec{\chi})
\end{split}
\end{equation*}

Ukázali jsme, že trilineární zobrazení dokážeme zredukovat na bilineární, kde jednu dvojici parametrů
nahradíme jejich tensorovým součinem. Pro toto bilineární zobrazení z definice existuje linearizace
\begin{equation*}
f'(\myvec{u}, \myvec{v}\otimes\myvec{w}) = F'(\myvec{u}\otimes(\myvec{v}\otimes\myvec{w}))
\end{equation*}
a zřejmě je tato linearizace i linearizací celého trilineárního zobrazení $f$, protože
\begin{equation*}
f(\myvec{u}, \myvec{v}, \myvec{w}) = f'(\myvec{u}, \myvec{v}\otimes\myvec{w})
\end{equation*}
Navíc z lemma o asociativitě tensorového součinu víme, že je jedno, kterou dvojici parametrů
nahradíme tensorovým součinem, protože výsledné tensorové součiny prostorů jsou přirozeně
isomorfní. Můžeme tedy zapomenout na závorky a tvrdit, že pro každé trilineární zobrazení
$f(\myvec{u}, \myvec{v}, \myvec{w})$ existuje jeho jednoznačná linearizace
$F(\myvec{u}\otimes\myvec{v}\otimes\myvec{w})$, samozřejmě za předpokladu, že naše definice
tensorového součinu je platná, což plánujeme dokázat ve velkém finále kapitoly o tensorech.

Pomocí matematické indukce dokážeme stejné tvrzení rozšířit i na více-lineární zobrazení. Zcela
stejným postupem zafixujeme jeden parametr
\begin{equation*}
f(\myvec{u}, \myvec{v_1}, \myvec{v_2}, \hdots) = g_u(\myvec{v_1}, \myvec{v_2}, \hdots)
\end{equation*}
Toto (n-1)-lineární zobrazení pomocí indukčního předpokladu zlinearizujeme
\begin{equation*}
g_u(\myvec{v_1}, \myvec{v_2}, \hdots) = G_u(\myvec{v_1}\otimes\myvec{v_2}\otimes\cdots)
\end{equation*}
a nadefinujeme zobrazení
\begin{equation*}
f'(\myvec{u}, \myvec{\chi}) = G_u(\myvec{\chi})
\end{equation*}
které je na základě stejných úvah bilineární. Pro něj najdeme linearizaci, která je zároveň
jednoznačnou linearizací i původního multilineárního zobrazení
$f(\myvec{u}, \myvec{v_1}, \myvec{v_2}, \hdots)$.

Tedy závěrem, pokud prokážeme, že existuje tensorový součin pro libovolné dva vektorové prostory
nad stejným tělesem, který dokáže linearizovat libovolné bilineární zobrazení z těchto dvou
prostorů, pak tensorový součin (jako multilineární zobrazení) existuje i pro libovolný počet
$\geq$ 2 vektorových prostorů nad stejným tělesem, který dokáže linearizovat libovolné multilineární
zobrazení z těchto prostorů.

\section{Báze tensorového součinu}

\noindent
Až doteď jsme pracovali s vektorovými prostory tak obecně, jak to šlo. Omezme se nyní pouze na
prostory s konečnou bází. Jinými slovy na prostory, kde jejich prvky lze vyjádřit jako souřadnice
oproti nějaké konečné bázi. Anebo ještě jinak na prostory, které jsou prakticky použitelné ve
fyzice.

Pokud mám dva vektorové prostory s konečnou bází, dává smysl si položit otázku, jak bude
vypadat báze jejich tensorového součinu. A odpověď pravděpodobně nebude příliš překvapivá.

\begin{lemma}
\label{lemma:tensor_basis}
Mějme dva vektorové prostory s konečnou bází $\myspace{U}$ a $\myspace{V}$ nad stejným tělesem
$\myspace{T}$, jejich tensorový součin $(\phi, \myspace{U}\otimes\myspace{V})$, bázi prostoru
$\myspace{U}$ $\{\myvec{u_1}, \dots, \myvec{u_m}\}$ a bázi prostoru $\myspace{V}$
$\{\myvec{v_1}, \dots, \myvec{v_n}\}$. Pak množina $\{\myvec{u_i}\otimes\myvec{v_j}| i \in
\{1, \dots, m\}, j \in \{1, \dots, n\}\}$ tvoří bázi prostoru $\myspace{U}\otimes\myspace{V}$.
\end{lemma}

Jinými slovy tensorový součin všech kombinací bázových vektorů tvoří bázi tensorového součinu
prostorů. Dimenze $dim(\myspace{U}\otimes\myspace{V})$ je tedy součin dimenzí jednotlivých
prostorů $dim(\myspace{U}).dim(\myspace{V})$.

\begin{proof}
Abychom prokázali, že množina $\{\myvec{u_i}\otimes\myvec{v_j}\}$ tvoří bázi, musíme dokázat
dvě věci:
\begin{enumerate}
	\item množina generuje celý prostor $\myspace{U}\otimes\myspace{V}$
	\item a je lineárně nezávislá.
\end{enumerate}

První část je jednoduchá. Vezměme dva libovolné vektory $\myvec{u}\in\myspace{U}$ a
$\myvec{v}\in\myspace{V}$ a vyjádřeme je jako lineární kombinaci vůči bázím
$\myvec{u} = \sum_{i}\alpha_i\myvec{u_i}$ a $\myvec{v} = \sum_{j}\beta_j\myvec{v_j}$.
Tyto vektory dosaďme do tensorového součinu:
\begin{equation*}
\begin{split}
\phi(\myvec{u}, \myvec{v}) = \phi(\sum_{i}\alpha_i\myvec{u_i}, \sum_{j}\beta_j\myvec{v_j})
\end{split}
\end{equation*}
Pokud aplikujeme vlastnosti bilineárního zobrazení, zápis můžeme přepsat
\begin{equation*}
\begin{split}
\phi(\myvec{u}, \myvec{v}) = \sum_{i}\sum_{j}\alpha_i\beta_j\phi(\myvec{u_i}, \myvec{v_j})
  = \sum_{i}\sum_{j}\alpha_i\beta_j\myvec{u_i}\otimes\myvec{v_j}
\end{split}
\end{equation*}
Každý obraz tensorového součinu je tedy možné zapsat jako lineární kombinaci tensorových součinů
bázových vektorů. Nyní vstoupí do hry druhá podmínka v definici tensorového součinu, která říká,
že množina obrazů generuje celý prostor $\myspace{U}\otimes\myspace{V}$. Tedy libovolný prvek
$\chi\in\myspace{U}\otimes\myspace{V}$ lze zapsat jako lineární kobinaci obrazů tensorového
součinu
\begin{equation*}
\begin{split}
\chi = \sum_{k}\gamma_k\myvec{u_k}\otimes\myvec{v_k}
\end{split}
\end{equation*}
Pozor! Vektory $\myvec{u_k}$ a $\myvec{v_k}$ zde nejsou bázové vektory, ale libovolné vektory
prostorů $\myspace{U}$ a $\myspace{V}$! Z předchozího už víme, že tensorový součin vektorů
$\myvec{u_k}\otimes\myvec{v_k}$ lze vyjádřit jako lineární kombinaci tensorových součinů
bázových vektorů:
\begin{equation*}
\begin{split}
\chi = \sum_{k}\gamma_k\sum_{i}\sum_{j}\alpha_{k_i}\beta_{k_j}\myvec{u_i}\otimes\myvec{v_j}
  = \sum_{k}\sum_{i}\sum_{j}\gamma_k\alpha_{k_i}\beta_{k_j}\myvec{u_i}\otimes\myvec{v_j}
\end{split}
\end{equation*}
a pokud součet přeuzávorkuji, získám lineární kombinaci tensorových součinů bázových vektorů
\begin{equation*}
\begin{split}
\chi = \sum_{i}\sum_{j}(\sum_{k}\gamma_k\alpha_{k_i}\beta_{k_j})\myvec{u_i}\otimes\myvec{v_j}
\end{split}
\end{equation*}

\medskip\noindent
Dokázat druhý bod je o něco složitější. Pro tento účel si zkonstruujeme pomocné bilineární
zobrazení a jeho linearizací následně vyvrátíme předpoklad, že množina tensorových
součinů bázových vektorů je lineárně závislá.

Pro konstrukci zobrazení si vezmeme duální báze prostorů $\myspace{U^*}$ a $\myspace{V^*}$
$\{\psi_1, \dots, \psi_m\}$ a $\{\varphi_1, \dots, \varphi_n\}$. Pomocí nich nadefinujeme
zobrazení
\begin{equation*}
\begin{split}
\vartheta_{ij}(\myvec{u}, \myvec{v}) = \psi_i(\myvec{u})\varphi_j(\myvec{v})
\end{split}
\end{equation*}
V tento okamžik si budete muset vzpomenout, jak jsme definovali duální bázi (\ref{def:dual_basis}).
Protože lineární forma duální báze vrací 1, pokud je do ní dosazený příslušný bázový vektor, a
vrací 0, pokud do ní dosadíme jiný bázový vektor, pro zobrazení $\vartheta_{ij}$ platí
\begin{equation*}
\begin{split}
\vartheta_{ij}(\myvec{u_k}, \myvec{v_l}) = \psi_i(\myvec{u_k})\varphi_j(\myvec{v_l}) =
\begin{cases}
  1 \: i = k \wedge j = l \\
  0 \: i \neq k \vee j \neq l
\end{cases}
\end{split}
\end{equation*}

Součin lineárních forem je zjevně bilineární zobrazení (čtenář si může sám dosadit
součet vektorů nebo násobení skalárem), proto podle definice tensorového součinu pro něj existuje
linearizace
\begin{equation*}
\begin{split}
\vartheta_{ij}(\myvec{u}, \myvec{v}) = T_{ij}(\phi(\myvec{u}, \myvec{v})) = T_{ij}(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}

Nyní předpokládejme, že množina tensorových součinů bázových vektorů není lineárně nezávislá,
tedy že existuje netriviální lineární kombinace
\begin{equation*}
\begin{split}
\sum_{i}\sum_{j}\gamma_{ij}\myvec{u_i}\otimes\myvec{v_j} = \myvec{0}
\end{split}
\end{equation*}
Zvolme si indexy $k$ a $l$ takové, aby $\gamma_{kl} \neq 0$ (musí existovat, protože kombinace
je netriviální). Sčítanec s indexy $k$ a $l$ převedeme na pravou stranu rovnice (opačný prvek
existuje z definice tělesa) a rovnici vynásobíme skalárem inverzním k $-\gamma_{kl}$ (opět
existuje z definice tělesa, protože je nenulový)
\begin{equation*}
\begin{split}
\sum_{i\in\{1, \dots, m\}\setminus\{k\}}
  \sum_{j\in\{1, \dots, n\}\setminus\{l\}}\gamma'_{ij}\myvec{u_i}\otimes\myvec{v_j} 
  = \myvec{u_k}\otimes\myvec{v_l}
\end{split}
\end{equation*}
Obě strany rovnice můžeme dosadit do lineárního zobrazení $T_{kl}$
\begin{equation*}
\begin{split}
T_{kl}(\sum_{i\in\{1, \dots, m\}\setminus\{k\}}
  \sum_{j\in\{1, \dots, n\}\setminus\{l\}}\gamma'_{ij}\myvec{u_i}\otimes\myvec{v_j})
  = T_{kl}(\myvec{u_k}\otimes\myvec{v_l})
\end{split}
\end{equation*}
a pomocí vlastností lineárního zobrazení dostaneme sumy ``ven``
\begin{equation*}
\begin{split}
\sum_{i\in\{1, \dots, m\}\setminus\{k\}}
  \sum_{j\in\{1, \dots, n\}\setminus\{l\}}\gamma'_{ij}T_{kl}(\myvec{u_i}\otimes\myvec{v_j})
  = T_{kl}(\myvec{u_k}\otimes\myvec{v_l})
\end{split}
\end{equation*}
Na levé straně rovnice je každý prvek $T_{kl}(\myvec{u_i}\otimes\myvec{v_j}) = 0$, zatímco
pravá strana je rovná $1$:
\begin{equation*}
\begin{split}
\sum_{i\in\{1, \dots, m\}\setminus\{k\}}
  \sum_{j\in\{1, \dots, n\}\setminus\{l\}}\gamma'_{ij}0 = 1
\end{split}
\end{equation*}
Nula vynásobená s čímkoliv je opět nula a součet nul je nula, tedy dostáváme rozpor
\begin{equation*}
\begin{split}
0 = 1
\end{split}
\end{equation*}
Z předpokladu lineární závislosti jsme došli k logickému rozporu, tedy množina tensorových součinů
bázových vektorů musí být lineárně nezávislá.
\end{proof}

\section{Tensory, duální prostory a bilineární formy}

\noindent
Možná to tak nevypadá, ale nyní zahajujeme klíčovou kapitolu, kterou se dostaneme k praktickému
použití tensorů. Až doteď jsme vlastně pouze ukazovali, že tensor odpovídá multilineárnímu
zobrazení. Známe vyjadřovací schopnosti a víme, kde je možné tensor použít. Prozatím jsme
se ale příliš nepříbližili k ničemu, co jsme viděli v úvodním příkladu. Co jsou složky tensorů?
Co je kontrakce tensoru? Jak vyjádřit tensor jako ``pole čísel``? A jak konečně nadefinovat
tensor namísto tensorového součinu? Abychom mohli najít příslušné odpovědi, musíme se dostat
k další alternativní definici tensorů. A cesta to nebude snadná, už jenom proto, že si možná
navodíte alergii na slovo duální...

Pokud jste pozorně studovali předchozí důkaz věty \ref{lemma:tensor_basis} dokazující, že
tensorové součiny bází tvoří bázi tensorového součinu, možná jste si všimli, jak jsme zkonstruovali
pomocné bilineární zobrazení $\vartheta_{ij}$:
\begin{equation*}
\begin{split}
\vartheta_{ij}(\myvec{u_k}, \myvec{v_l}) = \psi_i(\myvec{u_k})\varphi_j(\myvec{v_l}) =
\begin{cases}
1 \: i = k \wedge j = l \\
0 \: i \neq k \vee j \neq l
\end{cases}
\end{split}
\end{equation*}
Pokud vydolujete z paměti konstrukci duální báze (\ref{def:dual_basis}), zjistíte, že se podmínky
nápadně podobají. Bilineární formy $\vartheta_{ij}$ samozřejmě netvoří bázi duálního prostoru,
protože nejsou lineární. Ale jejich linearizace $T_{ij}$ již ano. Prostor linearizací bilineárních
forem $\mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{T})$ tvoří duální prostor
(z definice duálního prostoru) $(\myspace{U}\otimes\myspace{V})^*$ a linearizace bilineárních
forem $\vartheta_{ij}$ tvoří jeho bázi. Víme, že prostory bilineárních zobrazení a jejich linearizací
jsou isomorfní, formálně $\mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{T})\cong
\mathcal{L}(\myspace{U}\otimes\myspace{V}\rightarrow\myspace{T}) = (\myspace{U}\otimes\myspace{V})^*$.

Toto ne zas tak moc objevné zjištění nás přivádí blíže k definici pojmu tensor, kterou jsme v souvislosti
s definicí tensorového součinu odmítli provést. Důvod byl, že prvky tensorového součinu neodpovídaly
tomu, co si pod pojmem tensor představují fyzici - pro ně je tensor právě ona linearizace vyjádřená
jako ``pole čísel``. Nyní jsme ale linearizace bilineárních forem ztotožnili (až na isomorfismus)
s prvky duálního tensorového součinu. Pořád ještě to není ono, ale blížíme se.

Abychom dosáhli cíle a tensory skutečně ztotožnili s prvky tensorového součinu prostorů, budeme
potřebovat nalézt další dva isomorfismy. (Těch nikdy není dost, že?) Jako první musíme najít
vztah mezi duálním tensorovým součinem $(\myspace{U}\otimes\myspace{V})^*$ a nějakým jiným
ne-duálním tensorovým součinem.

\begin{lemma}
Mějme vektorové prostory konečné dimenze $\myspace{U}$ a $\myspace{V}$ nad stejným tělesem $\myspace{T}$.
Pak tensorový součin duálních prostorů $\myspace{U^*}\otimes\myspace{V^*}$ je přirozeně isomorfní
s duálním tensorovým součinem $(\myspace{U}\otimes\myspace{V})^*$,
tzn. $\myspace{U^*}\otimes\myspace{V^*}\cong(\myspace{U}\otimes\myspace{V})^*$.
\end{lemma}

Jinými slovy, ztotožňujeme linearizace bilineárních forem s prvky tensorového součinu duálních
prostorů. Tedy už s reálným tensorovým součinem prostorů, byť jeho ``operandy`` jsou duální
prostory.

\begin{proof}
Abychom tvrzení dokázali, musíme zkonstruovat bijektivní lineární zobrazení
$\Gamma: \myspace{U^*}\otimes\myspace{V^*}\rightarrow(\myspace{U}\otimes\myspace{V})^*$. Určitý
návod nám už poskytl předchozí důkaz, kdy jsme konstruovali bázi duálního tensorového součinu
jako násobky duální báze. Jestliže jsme bázi vytvořili jako násobení kovektorů, nemohli bychom
použít stejný trik? Zkusme následující předpis:
\begin{equation*}
\Gamma(\myvec{\varphi}\otimes\myvec{\psi})(\myvec{u}\otimes\myvec{v}) 
  = \myvec{\varphi}(\myvec{u})\myvec{\psi}(\myvec{v})
\end{equation*}
kde $\myvec{\varphi}\in\myspace{U^*}$, $\myvec{\psi}\in\myspace{V^*}$, $\myvec{u}\in\myspace{U}$
a $\myvec{v}\in\myspace{V}$.

Tato definice na první pohled není úplná. Zaprvé tensorové součiny obsahují i prvky,
které nejsou obrazy tensorového součinu vektorů, tudíž předpis definuje zobrazení z podmnožiny
prostoru $\myspace{U^*}\otimes\myspace{V^*}$. Zadruhé pravá strana je viditelně bilineární
zobrazení, proto nemůže být prvekm duálního prostoru.

Řešení druhého problému je snadné. Pro každou bilineární formu existuje její linearizace,
která pro stejné prvky vrací stejné hodnoty. Zobrazení $\Gamma$ tedy ve skutečnosti vrací
linearizaci $T_{\varphi\psi}$, která pro vektory $\myvec{u}$ a $\myvec{v}$ dává stejné
hodnoty. (Proto také v předpisu zarytě vektory vypisujeme.)

Pro řešení prvního problému potřebujeme příslušně dodefinovat zobrazení i pro ostatní prvky
prostoru $\myspace{U^*}\otimes\myspace{V^*}$. A samozřejmě tak, abychom dostali lineární
zobrazení.

Pro tento krok použijeme starou známou pravdu, že obrazy báze definují lineární zobrazení
jednoznačně. Pokud nadefinuji zobrazení $f$ tak, že libovolné lineární kombinaci báze
přiřadí stejnou lineární kombinaci jejich obrazů
\begin{equation*}
\begin{split}
f(\sum_i\alpha_i\myvec{e_i}) = \sum_i\alpha_i f(\myvec{e_i})
\end{split}
\end{equation*}
získám lineární zobrazení (čtenář si jistě dokáže do tohoto předpisu dosadit součet vektorů
a násobek skalárem, aby si to ověřil).

Předpis zobrazení $\Gamma$ nám jednoznačně určuje obrazy báze - stačí bázové vektory do předpisu
dosadit. Pomocí předchozího postupu obrazy definují lineární zobrazení. Pokud prokážeme, že
toto lineární zobrazení vrací stejnou hodnotu pro všechny další určené prvky, získáme lineární
zobrazení, které dodefinuje zobrazení $\Gamma$ na celý prostor $\myspace{U^*}\otimes\myspace{V^*}$.

Když do předpisu zobrazení $\Gamma$ dosadíme libovolný obraz tensorového součinu, kovektory,
na pravé straně můžeme vyjádřit jako lineární kombinace vůči jejich bázím:
\begin{equation*}
\begin{split}
\Gamma(\myvec{\varphi}\otimes\myvec{\psi})(\myvec{u}\otimes\myvec{v}) =
  \left(\sum_{i}\alpha_i\myvec{\varphi_i}\right)(\myvec{u})
  \left(\sum_{j}\beta_j\myvec{\psi_j}\right)(\myvec{v})
\end{split}
\end{equation*}
z definice sčítání kovektorů můžeme přepsat jako
\begin{equation*}
\begin{split}
\Gamma(\myvec{\varphi}\otimes\myvec{\psi})(\myvec{u}\otimes\myvec{v}) =
  \left(\sum_{i}\alpha_i\myvec{\varphi_i}(\myvec{u})\right)
  \left(\sum_{j}\beta_j\myvec{\psi_j}(\myvec{v})\right)
\end{split}
\end{equation*}
a nakonec roznásobíme závorky
\begin{equation*}
\begin{split}
\Gamma(\myvec{\varphi}\otimes\myvec{\psi})(\myvec{u}\otimes\myvec{v}) =
  \sum_{i}\sum_{j}\alpha_i\beta_j\myvec{\varphi_i}(\myvec{u})\myvec{\psi_j}(\myvec{v})
\end{split}
\end{equation*}
což odpovídá
\begin{equation*}
\begin{split}
\Gamma(\myvec{\varphi}\otimes\myvec{\psi})(\myvec{u}\otimes\myvec{v}) =
  \sum_{i}\sum_{j}\alpha_i\beta_j\Gamma(\myvec{\varphi_i}\otimes\myvec{\psi_j})(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Z definice tensorového součinu (tensorový součin je bilineární zobrazení) víme, že
\begin{equation*}
\begin{split}
\myvec{\varphi}\otimes\myvec{\psi} 
  = \left(\sum_i\alpha_i\myvec{\varphi_i}\right)\otimes\left(\sum_j\beta_j\myvec{\psi_j}\right)
  = \sum_i\sum_j\alpha_i\beta_j\myvec{\varphi_i}\otimes\myvec{\psi_j}
\end{split}
\end{equation*}
($\alpha_i$ a $\beta_j$ jsou skutečně stejné zde i v předchozím, protože se v obou případech jedná
o stejné lineární kombinace). Dostáváme tedy
\begin{equation*}
\begin{split}
\Gamma(\sum_i\sum_j\alpha_i\beta_j\myvec{\varphi_i}\otimes\myvec{\psi_j})
  = \sum_{i}\sum_{j}\alpha_i\beta_j\Gamma(\myvec{\varphi_i}\otimes\myvec{\psi_j})
\end{split}
\end{equation*}
Slovy: obraz lineární kombinace báze je stejná lineární kombinace obrazů báze. Máme tedy
zobrazení $\Gamma$ dobře dodefinované - je definované pro celý prostor, a zároveň je lineární.

\medskip\noindent
Teď chceme dokázat, že zobrazení $\Gamma$ je prosté. Předpokládejme, že existují dva prvky
$\Omega_1\neq\Omega_2\in\myspace{U^*}\otimes\myspace{V^*}$, které vrací stejnou hodnotu.
\begin{equation*}
\begin{split}
0 = \Gamma(\Omega_1)(\myvec{u}\otimes\myvec{v}) - \Gamma(\Omega_2)(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Oba prvky můžeme vyjádřit jako lineární kombinaci vůči bázím
\begin{equation*}
\begin{split}
0 = \Gamma(\sum_{i,j}\alpha_{ij}\myvec{\varphi_i}\otimes\myvec{\psi_j})(\myvec{u}\otimes\myvec{v}) 
  - \Gamma(\sum_{i,j}\beta_{ij}\myvec{\varphi_i}\otimes\myvec{\psi_j})(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Protože $\Gamma$ je lineární, můžeme sumy vystrčit ven
\begin{equation*}
\begin{split}
0 = \sum_{i,j}\alpha_{ij}\Gamma(\myvec{\varphi_i}\otimes\myvec{\psi_j})(\myvec{u}\otimes\myvec{v}) 
- \sum_{i,j}\beta_{ij}\Gamma(\myvec{\varphi_i}\otimes\myvec{\psi_j})(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Součty přeuzávorkovat
\begin{equation*}
\begin{split}
0 = \sum_{i,j}(\alpha_{ij} - \beta_{ij})\Gamma(\myvec{\varphi_i}\otimes\myvec{\psi_j})
  (\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Nyní můžeme dosadit podle definice zobrazení $\Gamma$
\begin{equation*}
\begin{split}
0 = \sum_{i,j}(\alpha_{ij} - \beta_{ij})\myvec{\varphi_i}(\myvec{u})\myvec{\psi_j}(\myvec{v})
\end{split}
\end{equation*}
a za bilineární formu $\myvec{\varphi_i}(\myvec{u})\myvec{\psi_j}(\myvec{v})$ dosadit její
linearizaci
\begin{equation*}
\begin{split}
0 = \sum_{i,j}(\alpha_{ij} - \beta_{ij})T_{ij}(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Jak již víme, $T_{ij}$ tvoří bázi duálního prostoru $(\myspace{U}\otimes\myspace{V})^*$, tedy
jsou lineárně nezávislé. Jediná jejich nenulová lineární kombinace je ta triviální.
Z toho vyplývá, že $\forall i,j: \alpha_{ij}-\beta_{ij} = 0$, tedy $\alpha_{ij} = \beta_{ij}$
a následně $\Omega_1 = \Omega_2$, což je spor s předpokladem. Zobrazení $\Gamma$ je prosté.

\medskip\noindent
Důkaz, že zobrazení je surjektivní (\textit{na}) si můžeme hodně zjednodušit. Víme, že dimenze
prostoru
\begin{equation*}
dim(\myspace{U^*}\otimes\myspace{V^*}) = dim(\myspace{U^*})dim(\myspace{V^*})
\end{equation*}
Pro prostory s konečnou dimenzí platí, že dimenze jejich duálního prostoru je stejná, tedy
\begin{equation*}
dim(\myspace{U^*}\otimes\myspace{V^*}) = dim(\myspace{U})dim(\myspace{V})
\end{equation*}
Dále víme, že
\begin{equation*}
dim(\myspace{U}\otimes\myspace{V}) = dim(\myspace{U})dim(\myspace{V})
\end{equation*}
a dimenze duálního tensorového součinu je opět stejná
\begin{equation*}
dim((\myspace{U}\otimes\myspace{V})^*) = dim(\myspace{U})dim(\myspace{V})
\end{equation*}
Dostáváme, že
\begin{equation*}
dim(\myspace{U^*}\otimes\myspace{V^*}) = dim((\myspace{U}\otimes\myspace{V})^*)
\end{equation*}
Pokud mám injektivní (\textit{prosté}) zobrazení mezi prostory o stejné dimenzi, zobrazení je
zároveň surjektivní (\textit{na}). Zobrazení $\Gamma$ tedy je bijektivní, nebo-li isomorfismem
prostorů $\myspace{U^*}\otimes\myspace{V^*}$ a $(\myspace{U}\otimes\myspace{V})^*$.
\end{proof}

\noindent
Každý jistě vnímá, jak je předchozí zjištění vzrušující! Ztotožnili jsme prostor linearizací
bilineárních forem s tensorovým součinem odpovídajících duálních prostorů. Vzhledem k isomorfismu
$\myspace{V^{**}}\cong\myspace{V}$ je každý tensorový součin prostorů isomorfní s nějakým prostorem
linearizací bilineárních forem, a začíná dávat smysl nazývat jeho prvky tensory:
\begin{equation*}
\def\arraystretch{2.0}
\begin{array}{|c|c|c|}
\hline
\text{Formy} & \text{Linearizace} & \text{Tensorový součin} \\
\hline
\mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{T}) &
\mathcal{L}(\myspace{U}\otimes\myspace{V}\rightarrow\myspace{T}) = (\myspace{U}\otimes\myspace{V})^* &
\myspace{U^*}\otimes\myspace{V^*} \\
\hline
\mathcal{B}(\myspace{U}\times\myspace{V^*}\rightarrow\myspace{T}) &
\mathcal{L}(\myspace{U}\otimes\myspace{V^*}\rightarrow\myspace{T}) = (\myspace{U}\otimes\myspace{V^*})^* &
\myspace{U^*}\otimes\myspace{V} \\
\hline
\mathcal{B}(\myspace{U^*}\times\myspace{V}\rightarrow\myspace{T}) &
\mathcal{L}(\myspace{U^*}\otimes\myspace{V}\rightarrow\myspace{T}) = (\myspace{U^*}\otimes\myspace{V})^* &
\myspace{U}\otimes\myspace{V^*} \\
\hline
\mathcal{B}(\myspace{U^*}\times\myspace{V^*}\rightarrow\myspace{T}) &
\mathcal{L}(\myspace{U^*}\otimes\myspace{V^*}\rightarrow\myspace{T}) = (\myspace{U^*}\otimes\myspace{V^*})^* &
\myspace{U}\otimes\myspace{V} \\
\hline
\end{array}
\end{equation*}

\noindent
Zvídavý čtenář si ale může položit otázku, jestli jsme se příliš neomezili. V obecné definici tensorového
součinu pracujeme s libovolným bilineárním zobrazením. Tady jásáme nad ztotožněním prostoru bilineárních
forem. Pokud bychom takto tensory nadefinovali, co budeme dělat s bilineárními zobrazeními, které formy
nejsou?

Zde přichází do hry druhý slibovaný isomorfismus. Pravda totiž je, že se ve skutečnosti ničím neomezujeme.

\begin{lemma}
Mějme vektorové prostory konečné dimenze $\myspace{U}$ a $\myspace{V}$ oba nad tělesem $\myspace{T}$.
Pak prostor lineárních zobrazení $\mathcal{L}(\myspace{U}\rightarrow\myspace{V})$ je 
přirozeně isomorfní s prostorem bilineárních forem
$\mathcal{B}(\myspace{U}\times\myspace{V^*}\rightarrow\myspace{T})$, formálně
$\mathcal{L}(\myspace{U}\rightarrow\myspace{V})\cong
\mathcal{B}(\myspace{U}\times\myspace{V^*}\rightarrow\myspace{T})$.
\end{lemma}

Tato věta ukazuje, že je možné libovolné lineární zobrazení jednoznačně převést na bilineární formu.
K ní dokážeme najít linearizaci, a ta už je isomorfní s prvkem nějakého tensorového součinu. Pokud
si vzpomenete, jak jsme rozšiřovali tensorový součin na multilineární zobrazení, pochopíte, že
tato věta nás ve skutečnosti neomezuje pouze na lineární zobrazení - pro libovolné bilineární a
multilineární zobrazení jsme schopni nalézt jeho linearizaci, a tu rozšířit na bilineární formu,
pro kterou opět nalezneme linearizaci, která je prvkem nějakého tensorového součinu, akorát již
více jak dvou prostorů. Omezení se na bilineární či multilineární formy nám neubírá žádnou obecnost.

\begin{proof}
Pro důkaz použijme následující zobrazení
\begin{equation*}
\Gamma(L(\myvec{u})) = B(\myvec{u}, \varphi) = \varphi(L(\myvec{u}))
\end{equation*}
Pokud prokážeme, že toto zobrazení je lineární a bijektivní, máme vyhráno.
Linearita je snadná, přímo vyplývá z faktu, že $\varphi$ je lineární forma
\begin{equation*}
\begin{split}
\Gamma((\alpha L_1 + L_2)(\myvec{u})) = \varphi((\alpha L_1 + L_2)(\myvec{u}))
  = \varphi(\alpha L_1(\myvec{u}) + L_2(\myvec{u}))) \\
  = \alpha\varphi(L_1(\myvec{u})) + \varphi(L_2(\myvec{u})))
  = \alpha\Gamma(L_1(\myvec{u})) + \Gamma(L_2(\myvec{u}))
\end{split}
\end{equation*}

Pokud lemma platí, což se právě snažíme dokázat, její důsledky jsou poměrně velké. Proto bychom
si zobrazení měli rozebrat trochu podrobněji. Jak je možné, že prostor zobrazení s jedním
parametrem může být shodný s prostorem zobrazení se dvěma parametry? Nevím jak vám, ale mě
to nedávalo smysl. Odpověď nám opět podá kapitola o duálním prostoru.

Vezměme si libovolný vektor $\myvec{u}$ a dosaďme ho do lineárního zobrazení $L(\myvec{u}) = \myvec{v}$.
Nyní ten samý vektor dosaďme do bilineární formy $B$
\begin{equation*}
g_{\myvec{u}}(\varphi) = B(\myvec{u}, \varphi) = \varphi(L(\myvec{u})) = \varphi(\myvec{v})
\end{equation*}
Pokud si vzpomenete na důkaz isomorfismu vektorového prostoru a jeho dvojitého duálu v kapitole
\ref{section:dual-dual}, zjistíte, že jsme dostali přesně ten prvek dvojitého duálu
\begin{equation*}
g_{\myvec{u}}(\varphi) = \varphi(\myvec{v}) = \mycocovec{v}(\varphi)
\end{equation*}
To znamená, že jsme vlastně dostali dvě podobná zobrazení. Pokud do lineárního zobrazení
$L$ dosadím vektor $\myvec{u}$, dostanu vektor $\myvec{v}$. Pokud do bilineárního zobrazení
$B$ dosadím vektor $\myvec{u}$, dostanu ko-kovektor $\mycocovec{v}$. V tento okamžik zobrazení
$\Gamma$ začíná dávat smysl, protože spojuje dvě zobrazení se stejnou vypovídací hodnotou.

Nyní už je poměrně snadné dokázat, že zobrazení je injektivní (\textit{prosté}). Pokud mám
dvě rozdílná zobrazení $L_1 \neq L_2$, pak musí existovat vektor $\myvec{u}$ pro které
$L_1(\myvec{u}) \neq L_2(\myvec{u})$ (jinak by zobrazení nebyla rozdílná). Pokud tento vektor
dosadíme do bilineárních forem daných zobrazením $\Gamma$, dostáváme dva rozdílné ko-kovektory 
\begin{equation*}
\Gamma(L_1)(\myvec{u}) = \mycocovec{L_1(\myvec{u}))} \neq
    \mycocovec{L_2(\myvec{u}))} = \Gamma(L_2)(\myvec{u})
\end{equation*}
a tedy bilineární formy musí být rozdílné.

Dokázat, že zobrazení je surjektivní (\textit{na}) lze opět udělat porovnáním dimenzí. Prostor
lineárních zobrazení má dimenzi $dim(\myspace{U})dim(\myspace{V})$, což je stejné jako dimenze
prostoru bilineárních forem\footnote{
    Dimenzi prostoru lineárních zobrazení jsme tu neprobírali, ale pro rychlou orientaci se
    čtenář může odrazit od rozměrů odpovídající matice. Dimenzi bilineární formy si čtenář
    dokáže snadno odvodit od dimenze příslušného tensorového součinu.
}. Pokud máme injektivní zobrazení mezi prostory stejné dimenze, zobrazení musí být i surjektivní.
\end{proof}

\begin{example}
\label{example:tensor-isomorphisms}

\noindent
Předchozí věty si rozhodně zaslouží konkrétní příklad, protože alespoň pro mne jsou dost
abstraktní.

Vezměme si jednoduché lineární zobrazení $L: \myspace{R}^2\rightarrow\myspace{R}^2$ dané předpisem
\begin{equation*}
L(\myvec{x}) = \left(
\begin{array}{c}
2 x_1 + x_2 \\
3 x_1 - 2 x_2
\end{array}
\right)
\end{equation*}

Podle druhé věty toto lineární zobrazení je přirozeně isomorfní s bilineární formou
$B: \myspace{R}^{2*}\times\myspace{R}^2\rightarrow\myspace{R}$, a to podle předpisu
\begin{equation*}
B(\myvec{\varphi}, \myvec{x}) = \myvec{\varphi}(L(\myvec{x}))
\end{equation*}
Vyjádřeme si nejdříve formu $\varphi$, pokud do ní dosadíme libovolný vektor
\begin{equation*}
\begin{split}
\myvec{\varphi}(\myvec{y}) = \varphi_1\myvec{\epsilon_1}(\myvec{y}) + \varphi_2\myvec{\epsilon_2}(\myvec{y})
\end{split}
\end{equation*}
Nyní vyjádříme vektor $\myvec{y}$ vůči jeho bázi
\begin{equation*}
\begin{split}
\myvec{\varphi}(\myvec{y}) 
  = \varphi_1\myvec{\epsilon_1}(y_1\myvec{e_1} + y_2\myvec{e_2}) + \varphi_2\myvec{\epsilon_2}(y_1\myvec{e_1} + y_2\myvec{e_2}) \\
= \varphi_1 y_1\myvec{\epsilon_1}(\myvec{e_1}) + \varphi_1 y_2\myvec{\epsilon_1}(\myvec{e_2}) 
  + \varphi_2 y_1\myvec{\epsilon_2}(\myvec{e_1}) + \varphi_2 y_2\myvec{\epsilon_2}(\myvec{e_2})
\end{split}
\end{equation*}
A pokud si vzpomenete na definici duální báze, kdy vrací $0$ a kdy $1$, dostaneme výsledek
\begin{equation*}
\begin{split}
\myvec{\varphi}(\myvec{y}) = \varphi_1 y_1 + \varphi_2 y_2
\end{split}
\end{equation*}
(Což mimochodem odpovídá maticovému součinu řádkového a sloupcového vektoru, jak jsme tvrdili v kapitole
o duálních prostorech.) Nyní dosaďme za vektor $y$ předpis lineárního zobrazení $L$
\begin{equation*}
\begin{split}
\myvec{\varphi}(L(\myvec{x})) = \varphi_1(2x_1 + x_2) + \varphi_2(3x_1 - 2x_2) \\
  = 2\varphi_1 x_1 + \varphi_1 x_2 + 3\varphi_2 x_1 - 2\varphi_2 x_2
\end{split}
\end{equation*}
Nalezli jsme tedy bilineární formu, která je isomorfním obrazem k původnímu lineárnímu zobrazení $L$
\begin{equation*}
\begin{split}
B(\myvec{\varphi}, \myvec{x}) = 2\varphi_1 x_1 + \varphi_1 x_2 + 3\varphi_2 x_1 - 2\varphi_2 x_2
\end{split}
\end{equation*}
Povšimněte si, že koeficienty v předpise jsou pořád stejné, jako byly původně v lineárním zobrazení.
To samozřejmě je očekávatelné, vzhledem k tomu, že jsme si ukázali, že bilineární forma je vlastně
stejná jako lineární zobrazení, pouze výsledek vyjadřuje jako prvek prostoru $\myspace{R}^{2**}$.

Tím že jsme nalezli isomorfní bilineární formu, dostáváme do ruky další možnost - můžeme dosadit
nějaký konkrétní kovektor a zkombinovat ho tak s lineárním zobrazením. Možná si vzpomenete, že
v motivačním příkladu na začátku kapitoly jsme něco takového udělali - dosadili jsme kovektor,
který vracel hodnotu polynomu v bodě $1$, do lineárního zobrazení, které substituovalo do
polynomu $u - t$. Tím jsme získali kovektor, který vrací hodnotu v bodě $1$ polynomu po substituci.

Máme bilineární formu, tedy dalším krokem bude nalézt její linearizaci
$H\in\mathcal{L}(\myspace{R}^{2*}\otimes\myspace{R}^2\rightarrow\myspace{R}) 
 = (\myspace{R}^{2*}\otimes\myspace{R}^2)^*$. Z bilinearity tensorového součinu víme
\begin{equation*}
\begin{split}
\myvec{\varphi}\otimes\myvec{x} = \varphi_1 x_1\myvec{\epsilon_1}\otimes\myvec{e_1}
 + \varphi_1 x_2\myvec{\epsilon_1}\otimes\myvec{e_2}
 + \varphi_2 x_1\myvec{\epsilon_2}\otimes\myvec{e_1}
 + \varphi_2 x_2\myvec{\epsilon_2}\otimes\myvec{e_2}
\end{split}
\end{equation*}
Linearizace $H$ musí splňovat
\begin{equation*}
\begin{split}
B(\myvec{\varphi}, \myvec{x}) &= H(\myvec{\varphi}\otimes\myvec{x}) \\
  &= \varphi_1 x_1 H(\myvec{\epsilon_1}\otimes\myvec{e_1})
    + \varphi_1 x_2 H(\myvec{\epsilon_1}\otimes\myvec{e_2})
    + \varphi_2 x_1 H(\myvec{\epsilon_2}\otimes\myvec{e_1}) \\
    &+ \varphi_2 x_2 H(\myvec{\epsilon_2}\otimes\myvec{e_2})
\end{split}
\end{equation*}
Pokud tento zápis porovnáme s vyjádřením bilineární formy $B$ o kousek výše, dostaneme
\begin{equation*}
\begin{split}
H(\myvec{\epsilon_1}\otimes\myvec{e_1}) &= 2 \\
H(\myvec{\epsilon_1}\otimes\myvec{e_2}) &= 1 \\
H(\myvec{\epsilon_2}\otimes\myvec{e_1}) &= 3 \\
H(\myvec{\epsilon_2}\otimes\myvec{e_2}) &= -2
\end{split}
\end{equation*}
a protože $\myvec{\epsilon_i}\otimes\myvec{e_i}$ tvoří bázi, máme kompletní definici
lineární formy $H$.

Máme linearizaci a je čas uplatnit první větu z této kapitoly: nalézt isomorfní prvek tensorového
součinu duálních prostorů $\myvec{y}\otimes\myvec{\mu}\in \myspace{R}^2\otimes\myspace{R}^{2*}$
takový, pro který platí
\begin{equation*}
\begin{split}
H(\myspace{\varphi}\otimes\myvec{x}) = \mycocovec{y}(\myvec{\varphi})\myspace{\mu}(\myvec{x})
\end{split}
\end{equation*}
Po vyjádření všech činitelů vůči jejím bázím
\begin{equation*}
\begin{split}
\mycocovec{y}(\myvec{\varphi})\myspace{\mu}(\myvec{x})
  = (\varphi_1 y_1 \mycocovec{e_1}(\myvec{\epsilon_1})
  + \varphi_1 y_2 \mycocovec{e_1}(\myvec{\epsilon_2})
  + \varphi_2 y_1 \mycocovec{e_2}(\myvec{\epsilon_1})
  + \varphi_2 y_2 \mycocovec{e_2}(\myvec{\epsilon_2})) \\
    (x_1 \mu_1 \myvec{\epsilon_1}(\myvec{e_1})
  + x_1 \mu_2 \myvec{\epsilon_1}(\myvec{e_2})
  + x_2 \mu_1 \myvec{\epsilon_2}(\myvec{e_1})
  + x_2 \mu_2 \myvec{\epsilon_2}(\myvec{e_2}))
\end{split}
\end{equation*}
Pokud si opět vzpomeneme na definici duální báze, výraz se nám značně zjednoduší
\begin{equation*}
\begin{split}
\mycocovec{y}(\myvec{\varphi})\myspace{\mu}(\myvec{x})
  = (\varphi_1 y_1 + \varphi_2 y_2)(x_1 \mu_1 + x_2 \mu_2)
\end{split}
\end{equation*}
a po roznásobení dostaneme
\begin{equation*}
\begin{split}
\mycocovec{y}(\myvec{\varphi})\myspace{\mu}(\myvec{x})
= \varphi_1 x_1 y_1 \mu_1 + \varphi_1 x_2 y_1 \mu_2 + \varphi_2 x_1 y_2 \mu_1 + \varphi_2 x_2 y_2 \mu_2
\end{split}
\end{equation*}
Porovnáním tohoto zápisu s linearizací $H$ dostáváme souřadnice prvku $\myvec{y}\otimes\myvec{\mu}$
\begin{equation*}
\begin{split}
y_1 \mu_1 &= 2 \\
y_1 \mu_2 &= 1 \\
y_2 \mu_1 &= 3 \\
y_2 \mu_2 &= -2
\end{split}
\end{equation*}
tedy hledaný prvek, zde již můžeme hrdě říci tensor, je
\begin{equation*}
\begin{split}
\myvec{y}\otimes\myvec{\mu} 
  = 2\myvec{e_1}\otimes\myvec{\epsilon_1}
  + \myvec{e_1}\otimes\myvec{\epsilon_2}
  + 3\myvec{e_2}\otimes\myvec{\epsilon_1}
  - 2\myvec{e_2}\otimes\myvec{\epsilon_2}
\end{split}
\end{equation*}
\end{example}

\noindent
Objevné důsledky dvou vět v této kapitole nás dostávají k dalším alternativním definicím tensorů
a tensorovým součinům. Například této:

\begin{definition}
Tenzorovým součinem vektorových prostorů $\myspace{U}$ a $\myspace{V}$ myslíme prostor všech
bilineárních forem $\mathcal{B}(\myspace{U}\times\myspace{V}\rightarrow\myspace{T})$ a
značíme ho $\myspace{U}\otimes\myspace{V}$. Jeho prvky nazýváme tensory.
\end{definition}

Anebo jiná definice, kterou její autor považuje za pedagogicky nejstravitelnější
\footnote{Posuďte sami.}:

\begin{definition}
Nechť $\myspace{X}$ a $\myspace{Y}$ jsou množiny a $\myspace{K}$ je těleso. Tenzorovým součinem
forem $f: \myspace{X}\rightarrow\myspace{K}$, $g: \myspace{Y}\rightarrow\myspace{K}$ nazýváme
funkci $f\otimes g: \myspace{X}\times\myspace{Y}\rightarrow\myspace{K}$ danou předpisem
$(f\otimes g)(\myvec{x}, \myvec{y}) = f(\myvec{x})g(\myvec{y})$ pro $x\in \myspace{X}$ a $y\in\myspace{Y}$.

Tenzorovým součinem lineárních podprostorů
$\myspace{U}\subseteq\mathcal{L}(\myspace{X}\rightarrow\myspace{K})$
a $\myspace{V}\subseteq\mathcal{L}(\myspace{Y}\rightarrow\myspace{K})$ nazýváme lineární podprostor 
$\myspace{U}\otimes\myspace{V}$ vektorového prostoru
$\mathcal{B}(\myspace{X}\times\myspace{Y}\rightarrow\myspace{K})$
generovaný všemi funkcemi $f\otimes g$, kde $f\in\myspace{U}$, $g\in\myspace{V}$.
\end{definition}

\noindent
Bez ohledu na alternativní definice, důsledky této kapitoly jsou důležité. Můžeme se omezit pouze
na lineární, bilineární či multilineární formy, protože víme, že každé lineární, bilineární
nebo multilineární zobrazení dokážeme jednoznačně převést na formu. A druhý důsledek je,
že konečně máme definici pojmu tensor: prvek tensorového součinu duálních prostorů.

V tomto místě je potřeba připomenout, že jsme se již omezili na prostory s konečnou bází. Pokud
autor ví, tak obě zde zveřejněná lemmata lze dokázat i pro prostory s nekonečnou dimenzí. Ale to
už je mimo rozsah této učebnice.

\section{Tensory v souřadnicích}

\noindent
V úvodním motivačním příkladu jsme s tensory pracovali jako s ``poli čísel``. To je vlastně ten
hlavní bod pro praktické počítání s tensory - fyzici tensory jinak nepoužívají. My ovšem zatím
pracovali pouze s bi/multilineárními zobrazeními, pole čísel jsme nikde nepotkali. Tento nedostatek
se pokusíme napravit v této kapitole.

Pro začátek zopakujeme trochu obecněji, co jsme udělali v příkladu předchozí kapitoly. Vezměme si
dva vektorové prostory $\myspace{U}$ a $\myspace{V}$ nad stejným tělesem $\myspace{T}$, oba s konečnou
bází $\{\myvec{u_1},\hdots\myvec{u_m}\}$ a $\{\myvec{v_1},\hdots\myvec{u_v}\}$. A mějme bilineární
formu $B: \myspace{U}\times\myspace{V}\rightarrow\myspace{T}$. K této formě nyní hledáme její linearizaci
\begin{equation*}
\begin{split}
B(\myvec{u}, \myvec{v}) = H(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Vyjádřeme si nyní tensorový součin vektorů $\myvec{u}$ a $\myvec{v}$ vyjádřené vůči jejím bázím.
\begin{equation*}
\begin{split}
\myvec{u}\otimes\myvec{v} = \left(u_1\myvec{u_1} + \cdots + u_m\myvec{u_m}\right)\otimes\left(v_1\myvec{v_1} + \cdots + v_n\myvec{v_n}\right)
\end{split}
\end{equation*}
Tensorový součin je bilineární zobrazení a jeho vlastností můžeme výraz rozepsat
\begin{equation*}
\begin{split}
\myvec{u}\otimes\myvec{v} = u_1 v_1\myvec{u_1}\otimes\myvec{v_1} + u_1 v_2\myvec{u_1}\otimes\myvec{v_2} 
  + \cdots + u_m v_n\myvec{u_m}\otimes\myvec{v_n}
\end{split}
\end{equation*}
Pokud toto dosadíme do linearizace $H$ a aplikujeme vlastnosti lineárního zobrazení, dostáváme
\begin{equation*}
\begin{split}
H(\myvec{u}\otimes\myvec{v}) &= u_1 v_1H(\myvec{u_1}\otimes\myvec{v_1}) 
  + u_1 v_2H(\myvec{u_1}\otimes\myvec{v_2}) + \cdots \\
  &\cdots + u_m v_nH(\myvec{u_m}\otimes\myvec{v_n})
\end{split}
\end{equation*}
Víme, že lineární zobrazení je jednoznačně definované obrazy báze. Tensorové součiny bází
jsou báze tensorového součinu prostorů, proto nám stačí za $H(\myvec{u_i}\otimes\myvec{v_j})$
jednoduše dosadit $B(\myvec{u_i}, \myvec{v_j})$. Tím jsme získali linearizaci.

My už ovšem také víme, že jako pojem tensor jsme si nenadefinovali tuto linearizaci, ale prvek
tensorového součinu duálních prostorů, který je s linearizací spojen isomorfním zobrazením.
Ten potřebujeme nyní nalézt. Označme ho jako $\myvec{\mu}\otimes\myvec{\vartheta}$, kde
$\myvec{\mu}\in\myspace{U}^*$ a $\myvec{\vartheta}\in\myspace{V}^*$.
Z předchozího víme, že musí splňovat následující rovnost
\begin{equation*}
\begin{split}
\myvec{\mu}(\myvec{u})\myvec{\vartheta}(\myvec{v}) = H(\myvec{u}\otimes\myvec{v})
\end{split}
\end{equation*}
Jak kovektory tak vektory vyjádříme vůči jejich bázi. Protože máme prostory s konečnou dimenzí,
velikost bází prostorů a jejich duálních prostorů jsou stejné.
\begin{equation*}
\begin{split}
\myvec{\mu}(\myvec{u})\myvec{\vartheta}(\myvec{v}) =
(\mu_1\myvec{\mu_1} + \cdots + \mu_m\myvec{\mu_m})(u_1\myvec{u_1} + \cdots + u_m\myvec{u_m})\\
(\vartheta_1\myvec{\vartheta_1} + \cdots \vartheta_n\myvec{\vartheta_n})(v_1\myvec{v_1} 
  + \cdots + v_n\myvec{v_n})
\end{split}
\end{equation*}
(Pozor na závorky! Jedny jsou skutečně závorky, druhé určují parametr zobrazení.) Pokud využiji
definici sčítání kovektorů a vlastnosti lineárního zobrazení, získám následující ošklivý rozklad
\begin{equation*}
\begin{split}
\myvec{\mu}(\myvec{u})\myvec{\vartheta}(\myvec{v}) =
\left(\mu_1 u_1 \myvec{\mu_1}(\myvec{u_1}) + \mu_1 u_2 \myvec{\mu_1}(\myvec{u_2}) + \cdots 
  + \mu_m u_m \myvec{\mu_m}(\myvec{u_m})\right) \\
\left(\vartheta_1 v_1 \myvec{\vartheta_1}(\myvec{v_1}) + \vartheta_1 v_2 \myvec{\vartheta_1}(\myvec{v_2}) 
  + \cdots + \vartheta_n v_n \myvec{\vartheta_n}(\myvec{v_n})\right)
\end{split}
\end{equation*}
Pokud si vzpomeneme na definici duální báze, výraz se výrazně zjednodušší, protože všechny výrazy
$\myvec{\mu_i}(\myvec{u_i}) = 1$ a $\myvec{\mu_i}(\myvec{u_j}) = 0, i\neq j$ (a podobně pro
$\myvec{\vartheta}$ a $\myvec{v}$). Tedy nám v závorkách zůstanou pouze koeficienty se stejným indexem.
\begin{equation*}
\begin{split}
\myvec{\mu}(\myvec{u})\myvec{\vartheta}(\myvec{v}) =
  \left(\mu_1 u_1 + \cdots + \mu_m u_m \right)
  \left(\vartheta_1 v_1 + \cdots + \vartheta_n v_n \right)
\end{split}
\end{equation*}
Nyní závorky můžeme roznásobit
\begin{equation*}
\begin{split}
\myvec{\mu}(\myvec{u})\myvec{\vartheta}(\myvec{v}) &=
  \mu_1\vartheta_1 u_1 v_1 + \mu_1\vartheta_2 u_1 v_2 + \cdots + \mu_1\vartheta_n u_1 v_n \\
  &+ \mu_2\vartheta_1 u_2 v_1 + \mu_2\vartheta_2 u_2 v_2 + \cdots + \mu_2\vartheta_n u_2 v_n\\
  &\vdots \\
  &+ \mu_m\vartheta_1 u_m v_1 + \mu_m\vartheta_2 u_m v_2 + \cdots + \mu_m\vartheta_n u_m v_n
\end{split}
\end{equation*}
Pokud tento ošklivý zápis porovnáme s vyjádřením linearizace $H$ výše (víme, že se musí rovnat),
dostaneme
\begin{equation*}
\begin{split}
\mu_i\vartheta_j = H(\myvec{u_i}\otimes\myvec{v_j}) = B(\myvec{u_i}, \myvec{v_j})
\end{split}
\end{equation*}
Teď můžeme vyjádřit tensorový součin kovektorů $\myvec{\mu}$ a $\myvec{\vartheta}$
\begin{equation*}
\begin{split}
\myvec{\mu}\otimes\myvec{\vartheta} &=
  \mu_1\vartheta_1\myvec{\mu_1}\otimes\myvec{\vartheta_1} + \cdots 
    + \mu_1\vartheta_n\myvec{\mu_1}\otimes\myvec{\vartheta_n} \\
  & + \mu_2\vartheta_1\myvec{\mu_2}\otimes\myvec{\vartheta_1} + \cdots 
    + \mu_2\vartheta_n\myvec{\mu_2}\otimes\myvec{\vartheta_n} \\
  &\vdots \\
  & + \mu_m\vartheta_1\myvec{\mu_m}\otimes\myvec{\vartheta_1} + \cdots 
    + \mu_m\vartheta_n\myvec{\mu_m}\otimes\myvec{\vartheta_n}
\end{split}
\end{equation*}
O kousek výše máme vyjádření koeficientů $\mu_i\vartheta_j$ (jedná se o stejné koeficienty, protože
jde o stejnou lineární kombinaci bázových vektorů), tedy námi hledaný tensor má následující
podobu
\begin{equation*}
\begin{split}
\myvec{\mu}\otimes\myvec{\vartheta} 
  = \sum_{i=1}^{m}\sum_{j=1}^{n}B(\myvec{u_i}, \myvec{v_j})\myvec{\mu_i}\otimes\myvec{\vartheta_j}
\end{split}
\end{equation*}
Nalezli jsme souřadnice tensoru vůči jeho bázi a zbývá nám zapsat je hezky do podoby pole. Jako první
pokus zkusíme vytvořit matici.
\begin{equation*}
\mymatrix{B} = 
\left(\begin{array}{cccc}
  B(\myvec{u_1}, \myvec{v_1}) & B(\myvec{u_1}, \myvec{v_2}) & \cdots & B(\myvec{u_1}, \myvec{v_n}) \\
  B(\myvec{u_2}, \myvec{v_1}) & B(\myvec{u_2}, \myvec{v_2}) & \cdots & B(\myvec{u_2}, \myvec{v_n}) \\
  \vdots & \vdots & \ddots & \vdots \\
  B(\myvec{u_m}, \myvec{v_1}) & B(\myvec{u_m}, \myvec{v_2}) & \cdots & B(\myvec{u_m}, \myvec{v_n})
\end{array}\right)
\end{equation*}
Toto se často dělá - typický případ ve fyzice je Cauchyho napěťový tensor. Bohužel samoukům mého
ražení to opět způsobuje bolesti hlavy - jak se, proboha, taková matice liší od lineárního zobrazení?
Z předchozí kapitoly víme, že lineární zobrazení je ekvivalentní bilineární formě s jedním vektorem
a jedním kovektorem. Zde máme bilineární formu se dvěma vektory. Ale zapsaná matice vypadá stejně.

Poučení z toho plyne, že pokud se ve fyzice setkáte s tensorem zapsaným jako matice, musíte si
nejdříve z kontextu zjistit, jakou bilineární formu popisuje. Matice sama o sobě není dostatečný
popis.

To je ovšem ve fyzice. V motivačním příkladu jsme už naznačili způsob, jak se s touto nejednoznačností
vypořádat. A budeme ho v textu nadále používat. Kontravariantní složky (souřadnice vektorů)
budeme psát do sloupců, kovariantní složky (souřadnice kovektorů) budeme psát do řádek. Náš tensor
$\myvec{\mu}\otimes\myvec{\vartheta}$ je lineární kombinací báze ve tvaru
$\myvec{\mu_i}\otimes\myvec{\vartheta_j}$, tedy dvou kovariantních složek. Výsledné ``pole čísel``
tedy bude řádkový vektor řádkových vektorů.
\begin{equation*}
\mymatrix{B} = 
\left(\begin{array}{ccc}
  \left(
    B(\myvec{u_1}, \myvec{v_1}) \cdots B(\myvec{u_1}, \myvec{v_n})
  \right) &
  \cdots &
  \left(
    B(\myvec{u_m}, \myvec{v_1}) \cdots B(\myvec{u_m}, \myvec{v_n})
  \right)
\end{array}\right)
\end{equation*}

\medskip\noindent
Zkusme nyní postup zopakovat pro bilineární formu, která jako argumenty přijímá
kovektory. Mějme tedy bilineární formu $D:\myspace{U}^*\times\myspace{V}^*\rightarrow\myspace{T}$
a k němu nalezneme linearizaci $F$
\begin{equation*}
\begin{split}
D(\myvec{\mu}, \myvec{\vartheta}) = F(\myvec{\mu}\otimes\myvec{\vartheta})
\end{split}
\end{equation*}
Stejným postupem získáme
\begin{equation*}
\begin{split}
F(\myvec{\mu}\otimes\myvec{\vartheta}) &= \mu_1\vartheta_1 D(\myvec{\mu_1}\otimes\myvec{\vartheta_1})
  + \mu_1\vartheta_2 D(\myvec{\mu_1}\otimes\myvec{\vartheta_2})
  + \cdots + \\
  & + \mu_m\vartheta_n D(\myvec{\mu_m}\otimes\myvec{\vartheta_n})
\end{split}
\end{equation*}
A pokud linearizaci proženeme isomorfismem
\begin{equation*}
\begin{split}
F(\myvec{\mu}\otimes\myvec{\vartheta}) = \mycocovec{u}(\myvec{\mu})\mycocovec{v}(\myvec{\vartheta})
\end{split}
\end{equation*}
získáme tensor
\begin{equation*}
\begin{split}
\mycocovec{u}\otimes\mycocovec{v} = \sum_{i=1}^{m}\sum_{j=1}^{n}D(\myvec{\mu_i}, \myvec{\vartheta_j})
  \mycocovec{u}_i\otimes\mycocovec{v}_j
\end{split}
\end{equation*}
a protože dvojitý duální prostor je přirozeně isomorfní s původním prostorem, můžeme převést na tensor
\begin{equation*}
\begin{split}
\myvec{u}\otimes\myvec{v} = \sum_{i=1}^{m}\sum_{j=1}^{n}D(\myvec{\mu_i}, \myvec{\vartheta_j})
  \myvec{u}_i\otimes\myvec{v}_j
\end{split}
\end{equation*}
Náš tensor tedy  má dvě kontravariantní složky, jeho zápis do pole je sloupcový vektor sloupcových
vektorů
\begin{equation*}
\begin{split}
\mymatrix{D} = 
\left(\begin{array}{c}
  \left(\begin{array}{c}
    D(\myvec{\mu_1}, \myvec{\vartheta_1}) \\
    \vdots \\
    D(\myvec{\mu_1}, \myvec{\vartheta_m})
  \end{array}\right) \\
  \vdots \\
  \left(\begin{array}{c}
    D(\myvec{\mu_m}, \myvec{\vartheta_1}) \\
    \vdots \\
    D(\myvec{\mu_m}, \myvec{\vartheta_m})
  \end{array}\right)
\end{array}\right)
\end{split}
\end{equation*}

Pokud tedy mám bilineární formu, výsledný tensor má dvě složky odpovídající argumentům formy. Ovšem
složka je obrácená - pokud je argument vektor, složka je kovektor, pokud je argument kovektor,
složka je vektor. Nebo jinými slovy, pokud je argument kontravariantní, složka je kovariantní,
a naopak.

\begin{example}

V příkladu v předchozí kapitole \ref{example:tensor-isomorphisms} jsme pro lineární zobrazení 
\begin{equation*}
L(\myvec{x}) = \left(
\begin{array}{c}
2 x_1 + x_2 \\
3x_1 - 2 x_2
\end{array}
\right)
\end{equation*}
odvodili tensor
\begin{equation*}
\begin{split}
\myvec{y}\otimes\myvec{\mu} 
= 2\myvec{e_1}\otimes\myvec{\epsilon_1}
+ \myvec{e_1}\otimes\myvec{\epsilon_2}
+ 3\myvec{e_2}\otimes\myvec{\epsilon_1}
- 2\myvec{e_2}\otimes\myvec{\epsilon_2}
\end{split}
\end{equation*}
Tensor obsahuje první kontravariantní složku a druhou kovariantní. Zápis do pole tedy bude sloupcový
vektor řádkových vektorů:
\begin{equation*}
\begin{split}
\mymatrix{L} =
\left(\begin{blockarray}{cc}
  \begin{block}{(cc)}
    2 & 1 \\
  \end{block}
  \begin{block}{(cc)}
    3 & -2 \\
  \end{block}
\end{blockarray}
\right)
\end{split}
\end{equation*}
Pokud se pořádně podíváte, dostal jsme přesně stejnou matici, jakou bychom získali přímo pro zobrazení
$L$, pouze máme navíc závorkování, abychom dokázali rozeznat jednotlivé složky tensoru. To je pochopitelně
očekávaný výsledek - tensory považujeme za zobecnění vektorů, kovektorů i lineárních zobrazení.

Narozdíl od běžného maticového násobení u tensorů můžeme prohodit indexy. Potom bychom měli
tensor
\begin{equation*}
\begin{split}
\myvec{\mu}\otimes\myvec{y} 
= 2\myvec{\epsilon_1}\otimes\myvec{e_1}
+ 3\myvec{\epsilon_1}\otimes\myvec{e_2}
+ \myvec{\epsilon_2}\otimes\myvec{e_1}
- 2\myvec{\epsilon_2}\otimes\myvec{e_2}
\end{split}
\end{equation*}
a příslušná matice by vypadala takto
\begin{equation*}
\begin{split}
\mymatrix{L} =
\left(
  \begin{array}{cc}
    \left(
      \begin{array}{c}
        2 \\ 3
      \end{array}
    \right) &
    \left(
      \begin{array}{c}
        1 \\ -2
      \end{array}
    \right)
  \end{array}
\right)
\end{split}
\end{equation*}
Opět nám vyšla stejná matice, pouze závorkování je jiné. Ani toto není neočekávaný výsledek - tensorový
součin je až na isomorfismus komutativní. A isomorfismus se nám zde projevuje jako odlišné závorky.

Je potřeba si uvědomit, že se s prohozením indexů musí změnit i bilineární forma, kterou tensor
representuje - musí se jí prohodit argumenty
\begin{equation*}
\begin{split}
\widetilde{B}: \myspace{R}^2\times\myspace{R}^{2*}\rightarrow\myspace{R}^2
\end{split}
\end{equation*}
s tím, že
\begin{equation*}
\begin{split}
\widetilde{B}(\myvec{u}, \myvec{\varphi}) = B(\myvec{\varphi}, \myvec{u})
\end{split}
\end{equation*}
\end{example}

\medskip\noindent
Zatím jsme dosazovali pouze vektory a kovektory. Co když ale argument bilineární formy bude
jiný složitější tensor? Narozdíl od vektorů a kovektorů, tensor má více složek. Jak se s nimi
bude pracovat?

\end{document}

\section{Kroneckerův součin}

\noindent
Dříve než se pustíme přímo do tensorů, rozeberme podrobněji operaci, která nás dále bude neustále provázet:
\textit{Kroneckerův součin}. Kroneckerův součin pracuje nad poli čísel, což pro nás bude vyjádření tensoru
v souřadnicích. Základní princip je jednoduchý: každý prvek levého operandu nahradíme pravým operandem a jeho
prvky vynásobíme nahrazenou hodnotou. Například:

\begin{equation*}
\begin{split}
& \left(\begin{array}{c}1 \\ 2\end{array}\right)\otimes\left(\begin{array}{c}3 \\ 4\end{array}\right) =
\left(\begin{blockarray}{c}
\begin{block}{c}
1 \left(\begin{array}{c}3 \\ 4\end{array}\right) \\
2 \left(\begin{array}{c}3 \\ 4\end{array}\right) \\
\end{block}
\end{blockarray}\right) =
\left(\begin{blockarray}{c}
\begin{block}{(c)}
1 \times 3 \\
1 \times 4 \\
\end{block}
\begin{block}{(c)}
2 \times 3 \\
2 \times 4 \\
\end{block}
\end{blockarray}\right) =
\left(\begin{blockarray}{c}
\begin{block}{(c)}
3 \\
4 \\
\end{block}
\begin{block}{(c)}
6 \\
8 \\
\end{block}
\end{blockarray}\right)
\end{split}
\end{equation*}
Výsledkem tedy je dvourozměrné pole - první index určuje vnořené pole, druhý index prvek v něm, tedy
$a_{11} = 3, a_{12} = 4, a_{21} = 6, a_{22} = 8$.

Obecně tedy, pokud mám k-rozměrné pole $a$ a n-rozměrné pole $b$, jejich Kroneckerův součin 
$s = a \otimes b$ bude "k plus n"-rozměrné pole s prvky podle předpisu:
\begin{equation*}
s_{i_1 \cdots i_k j_1 \cdots j_n} = a_{i_1 \cdots i_k} b_{j_1 \cdots j_n}
\end{equation*}
Kroneckerův součin je vlastně poměrně jednoduchá operace, která vynásobí každý prvek s každým
a uspořádá je do nového pole kopírujícího struktury původních dvou.

Kroneckerův součin demonstruje jev, se kterým se v tensorové algebře budeme potýkat: příliš rychle
narůstá složitost. Pokud vynásobím dvě pole, rozměr výsledného pole roste se součtem rozměrů polí,
počet prvků roste s násobkem počtu prvků. Velmi záhy tak zápis ve dvourozměrné ploše papírů přestane
být čitelný a budeme se muset uchylovat k zápisu po složkách s indexy, jako je popsaný součin výše.
Ale i v tomto zápisu začne počet indexů přerůstat hladinu srozumitelnosti.

Co můžeme o Kroneckerově součinu říci dále? Tak např. je \textbf{asociativní}:
\begin{equation*}
(a \otimes b) \otimes c = a \otimes (b \otimes c)
\end{equation*}
což poměrně jednoduše vyplývá z toho, že násobení prvků je asociativní:
\begin{equation*}
(a_{i_1 \cdots i_n} b_{j_1 \cdots j_o})c_{k_1 \cdots k_p} = a_{i_1 \cdots i_n} (b_{j_1 \cdots j_o} c_{k_1 \cdots k_p})
\end{equation*}
I na násobení skalárem (roznásobení každého prvku pole) se chová očekávaně:
\begin{equation*}
\alpha (a \otimes b) = (\alpha a) \otimes b = a \otimes (\alpha b)
\end{equation*}
což opět jednodušše vyplývá z asociativity a komutativity násobení jednotlivých prvků:
\begin{equation*}
\alpha (a_{i_1 \cdots i_n} b_{j_1 \cdots j_o}) = (\alpha a_{i_1 \cdots i_n}) b_{j_1 \cdots j_o}
= a_{i_1 \cdots i_n} (\alpha b_{j_1 \cdots j_o})
\end{equation*}

Kroneckerův součin ale \textbf{není obecně komutativní}: $a \otimes b \neq b \otimes a$. Ale
je v principu ``skorokomutativní``. Otočením operandů vzniknou stejné prvky, protože násobení
prvků komutativní je, ale změní se pořadí indexů. Takže vznikne pole se stejnými hodnotami,
ale jinak rozvržené, a které permutací indexů dokážeme přetransformovat do stejné podoby.


